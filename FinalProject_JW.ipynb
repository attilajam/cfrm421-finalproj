{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8908d8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Predicting closing price movements of NASDAQ stocks\n",
    "Attila Jamilov, Cooper Reynolds, Yueying Du, Brandon Leong, Josephine Welin.\n",
    "## Introduction\n",
    "In the last minutes of the market being open, many stocks see heightened volatility as well as big price fluctuation. NASDAQ stock exchange uses the NASDAQ Closing Cross auction to determine the official closing prices for various assets on their exchange. We want to evaluate the performance of multiple models that we learned in class and not, on predicting this closing price movement using the dataset provided in the [Kaggle](https://www.kaggle.com/competitions/optiver-trading-at-the-close/overview), and see what models performs best, and what features we can engineer to improve on the performance of the models. \n",
    "\n",
    "The data, posted on the Kaggle competition page, contains historical data for the daily ten minute closing auction on the NASDAQ stock exchange. Each instance in the dataset represents a specific stock at a particular timestamp within the final ten minutes of a trading day, and is identified by a unique stock_id. There are 5237980 instrances in the dataset. The target variable is the 60 second future move in the wap of the stock, called “target” in the dataset.\n",
    "\n",
    "To make this prediction, the dataset provides a variety of engineered features derived from both the auction and non-auction order books. This includes the amount unmatched at the current reference price (imbalance_size), the price of the most competitive buy/sell level in the non-auction book ([bid/ask]_price), and the weighted average price in the non-auction book (wap). \n",
    "\n",
    "For our features, we will try using only the features provided in the dataset, then creating our own original features, trying features that the Kaggle competitors had success with, and finally a compilation of all features. Then, we will select only the most helpful features, and then test our best model on the test data set through the Kaggle. \n",
    "\n",
    "For our models, we will begin with Linear Regression (Josephine), Random Forest (Brandon), LightGBM and CNN (Yueying), XGBoost (Cooper), and finally we will look into Catboost (Attila), a model developed by Yandex which the winner of the Kaggle used for his approach to this Kaggle.\n",
    "\n",
    "## Dataset explanation\n",
    "\n",
    "## Data Processing (Attila, Cooper)\n",
    "First, we need to import the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971df0b8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train.csv\", index_col=\"row_id\") # 88 out of 5 million rows have null targets, which we can't train any model on if we include this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f2ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "X = df.drop([\"target\", \"time_id\"], axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76a4de",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We drop `time_id`, from `X` as it's an identifying feature that won't help the model. We also 'remove' another identifying feature but we set it as the `index_col`, which is necessary for submitting to the Kggle. Next, we need to split the data into a training and validating subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a523dd44",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b3c11",
   "metadata": {},
   "source": [
    "`train_test_split` shuffles the data on it's own, therefore there is nothing we need to do on that part. Now we need to deal with certain features that are NaN or missing from the data. Some orders never fill, therefore it makes sense that there would be many unfilled orders with NaN target features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fede1c3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/josephine.welin/Desktop/CFRM 421/final/FinalProject.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephine.welin/Desktop/CFRM%20421/final/FinalProject.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcatboost\u001b[39;00m \u001b[39mimport\u001b[39;00m CatBoostRegressor\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephine.welin/Desktop/CFRM%20421/final/FinalProject.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cb \u001b[39m=\u001b[39m CatBoostRegressor(loss_function\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMAE\u001b[39m\u001b[39m\"\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m, thread_count\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephine.welin/Desktop/CFRM%20421/final/FinalProject.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cb\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cb = CatBoostRegressor(loss_function=\"MAE\", random_state=42, verbose=1, task_type=\"GPU\", thread_count=-1)\n",
    "\n",
    "cb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cb.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc4012",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Then, we evaluate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60adb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.346476399623894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b88d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _drop_null_rows(data):\n",
    "    \"\"\"\n",
    "    Drop rows where target, near_price, or far_price is null.\n",
    "    \"\"\"\n",
    "    return data.dropna(subset=['target', 'near_price', 'far_price'])\n",
    "\n",
    "def feature_engineering_none(data):\n",
    "    \"\"\"\n",
    "    No feature engineering, returns raw features only after dropping nulls.\n",
    "    \"\"\"\n",
    "    return _drop_null_rows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032cdc03",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We implement four variations of feature engineering strategies:\n",
    "\n",
    "- The feature_engineering_none function performs no additional transformations, returning only the cleaned data. \n",
    "- feature_engineering_non_leaderboard introduces custom, non-Kaggle leaderboard-inspired features, such as volatility and momentum indicators, log-transformed volume sizes, and various price/time interaction features. \n",
    "- feature_engineering_leaderboard replicates a simplified version of features from top Kaggle solutions, including categorical bucketing, price spreads, imbalance ratios, and time-normalized metrics. \n",
    "- Finally, feature_engineering_combined merges both approaches—combining custom and leaderboard-inspired features—offering a more comprehensive feature set. All functions include robust handling of NaN and infinite values to ensure model readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ed5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_non_leaderboard(data):\n",
    "    \"\"\"\n",
    "    Feature engineering with new features not from leaderboard posts.\n",
    "    \"\"\"\n",
    "    data = _drop_null_rows(data)\n",
    "    # Volatility features\n",
    "    data['wap_volatility'] = data.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "    data['bid_ask_spread'] = data['ask_price'] - data['bid_price']\n",
    "    data['bid_ask_volatility'] = data.groupby('stock_id')['bid_ask_spread'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # Momentum features\n",
    "    data['wap_momentum'] = data.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "    data['price_momentum'] = data.groupby('stock_id')['reference_price'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "\n",
    "    # Log transformations\n",
    "    size_cols = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size']\n",
    "    for col in size_cols:\n",
    "        data[f'log_{col}'] = np.log1p(data[col].clip(lower=0))\n",
    "\n",
    "    # Time-based interactions\n",
    "    data['bucket_price_interaction'] = data['seconds_in_bucket'] * data['reference_price']\n",
    "    data['bucket_imbalance_interaction'] = data['seconds_in_bucket'] * data['imbalance_size']\n",
    "\n",
    "    # Relative price features\n",
    "    data['wap_to_ref_price'] = data['wap'] / (data['reference_price'] + 1e-6)\n",
    "    data['bid_to_ask_price'] = data['bid_price'] / (data['ask_price'] + 1e-6)\n",
    "\n",
    "    # Handle NaN and inf\n",
    "    new_cols = [col for col in data.columns if col not in ['stock_id', 'date_id', 'target', 'time_id', 'row_id']]\n",
    "    data[new_cols] = data[new_cols].replace([np.inf, -np.inf], np.nan).fillna(data[new_cols].median())\n",
    "\n",
    "    return data\n",
    "\n",
    "def feature_engineering_leaderboard(data):\n",
    "    \"\"\"\n",
    "    Simplified feature engineering based on 1st, 9th, and 14th place Kaggle solutions.\n",
    "    \"\"\"\n",
    "    data = _drop_null_rows(data)\n",
    "    # Handle NaN and infinities in input columns\n",
    "    input_cols = ['imbalance_size', 'matched_size', 'ask_price', 'bid_price', 'wap', 'reference_price']\n",
    "    for col in input_cols:\n",
    "        data[col] = data[col].replace([np.inf, -np.inf], np.nan).fillna(data[col].median())\n",
    "\n",
    "    # 1st Place: Seconds in bucket group\n",
    "    data['seconds_in_bucket_group'] = np.where(data['seconds_in_bucket'] < 300, 0,\n",
    "                                              np.where(data['seconds_in_bucket'] < 480, 1, 2))\n",
    "\n",
    "    # 9th Place: Basic features\n",
    "    data['bid_ask_spread'] = data['ask_price'] - data['bid_price']\n",
    "    data['imbalance_ratio'] = data['imbalance_size'] / (data['matched_size'] + 1e-6)\n",
    "\n",
    "    # 14th Place: Mid price\n",
    "    data['mid_price'] = (data['ask_price'] + data['bid_price']) / 2\n",
    "\n",
    "    # Time in auction\n",
    "    data['time_in_auction'] = data['seconds_in_bucket'] / 540\n",
    "\n",
    "    # Handle NaN and inf in new features\n",
    "    new_cols = ['seconds_in_bucket_group', 'bid_ask_spread', 'imbalance_ratio', 'mid_price', 'time_in_auction']\n",
    "    data[new_cols] = data[new_cols].replace([np.inf, -np.inf], np.nan).fillna(data[new_cols].median())\n",
    "\n",
    "    return data\n",
    "\n",
    "def feature_engineering_combined(data):\n",
    "    \"\"\"\n",
    "    Combine non-leaderboard and simplified leaderboard features.\n",
    "    \"\"\"\n",
    "    # Start with non-leaderboard features\n",
    "    data = feature_engineering_non_leaderboard(data.copy())\n",
    "    # Add simplified leaderboard features\n",
    "    input_cols = ['imbalance_size', 'matched_size', 'ask_price', 'bid_price', 'wap', 'reference_price']\n",
    "    for col in input_cols:\n",
    "        data[col] = data[col].replace([np.inf, -np.inf], np.nan).fillna(data[col].median())\n",
    "\n",
    "    data['seconds_in_bucket_group'] = np.where(data['seconds_in_bucket'] < 300, 0,\n",
    "                                              np.where(data['seconds_in_bucket'] < 480, 1, 2))\n",
    "    data['imbalance_ratio'] = data['imbalance_size'] / (data['matched_size'] + 1e-6)\n",
    "    data['mid_price'] = (data['ask_price'] + data['bid_price']) / 2\n",
    "    data['time_in_auction'] = data['seconds_in_bucket'] / 540\n",
    "\n",
    "    # Handle NaN and inf in new features\n",
    "    new_cols = ['seconds_in_bucket_group', 'imbalance_ratio', 'mid_price', 'time_in_auction']\n",
    "    data[new_cols] = data[new_cols].replace([np.inf, -np.inf], np.nan).fillna(data[new_cols].median())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ce01e",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "This code is for training an XGBoost regression model, evaluating it with MAE, and inspecting feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9469a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/josephine.welin/Desktop/CFRM 421/final/FinalProject.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephine.welin/Desktop/CFRM%20421/final/FinalProject.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Written by Cooper Richmond\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephine.welin/Desktop/CFRM%20421/final/FinalProject.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mxgb\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephine.welin/Desktop/CFRM%20421/final/FinalProject.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephine.welin/Desktop/CFRM%20421/final/FinalProject.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_absolute_error\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Written by Cooper Richmond\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def run_xgboost_regression(X_train, y_train, X_val, y_val, features, target='target', random_state=42):\n",
    "    \"\"\"\n",
    "    Behavior:\n",
    "    Runs XGBoost regression\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): training features\n",
    "    - y_train (pd.Series): training target\n",
    "    - X_val (pd.DataFrame): validation features \n",
    "    - y_val (pd.Series): validation target\n",
    "    - features (list): list of features\n",
    "    - target (str): target column name \n",
    "    - random_state (int): random seed 42\n",
    "    \n",
    "    Returns:\n",
    "    - model: trained XGBoost model\n",
    "    - val_mae: validation MAE\n",
    "    - feature_importance: dataframe with feature importance\n",
    "    \"\"\"\n",
    "    # params\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 1000,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        X_train[features], y_train,\n",
    "        eval_set=eval_set,\n",
    "        \n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Predict on validation\n",
    "    y_pred = model.predict(X_val[features])\n",
    "    \n",
    "    val_mae = mean_absolute_error(y_val, y_pred)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    print(f\"Validation MAE: {val_mae:.6f}\")\n",
    "    print(\"\\nTop 5 Features:\")\n",
    "    print(feature_importance.head())\n",
    "    \n",
    "    return model, val_mae, feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b7db2",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "This method is for a Random Forests model. We randomly sample the training data and train a lightweight RandomForestRegressor.\n",
    "Performance is evaluated using MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd05e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test=None, y_test=None, sample_size=500000, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Random Forest regressor on a random subset of training data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training features\n",
    "    - y_train (pd.Series): Training target\n",
    "    - X_test (pd.DataFrame): Test features (optional)\n",
    "    - y_test (pd.Series): Test target (optional)\n",
    "    - sample_size (int): Number of rows to sample for training (default: 500,000)\n",
    "    - random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained Random Forest model\n",
    "    - mae: Mean Absolute Error on test set (if provided), else None\n",
    "    \"\"\"\n",
    "    # Sample subset of training data\n",
    "    if len(X_train) > sample_size:\n",
    "        indices = np.random.choice(X_train.index, size=sample_size, replace=False)\n",
    "        X_train_subset = X_train.loc[indices]\n",
    "        y_train_subset = y_train.loc[indices]\n",
    "    else:\n",
    "        X_train_subset = X_train\n",
    "        y_train_subset = y_train\n",
    "    \n",
    "    # Train model\n",
    "    rf = RandomForestRegressor(n_estimators=10, random_state=random_state, n_jobs=-1)\n",
    "    rf.fit(X_train_subset, y_train_subset)\n",
    "    \n",
    "    # Compute metrics if test data provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        y_pred = rf.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        return rf, mae\n",
    "    \n",
    "    return rf, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3116f71c",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Regression\n",
    "Josephine Welin.\n",
    "\n",
    "To have a basic model to compare the others to, we are using a simple linear regression.\n",
    "\n",
    "We train the model and then evalutae via MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def train_lin_reg(X_train, y_train, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Train a Linear Regression model and return model and MAE.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training features\n",
    "    - y_train (pd.Series): Training target\n",
    "    - X_test (pd.DataFrame): Test features (optional)\n",
    "    - y_test (pd.Series): Test target (optional)\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained Linear Regression model\n",
    "    - mae: Mean Absolute Error on test set (if provided), else None\n",
    "    \"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    if X_test is not None and y_test is not None:\n",
    "        y_pred = lr.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        return lr, mae\n",
    "    \n",
    "    return lr, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
