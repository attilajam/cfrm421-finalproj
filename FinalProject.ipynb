{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8908d8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Predicting closing price movements of NASDAQ stocks\n",
    "Attila Jamilov, Cooper Reynolds, Yueying Du, Brandon Leong, Josephine Welin.\n",
    "\n",
    "## Introduction\n",
    "In the last minutes of the market being open, many stocks see heightened volatility as well as big price fluctuation. NASDAQ stock exchange uses the NASDAQ Closing Cross auction to determine the official closing prices for various assets on their exchange. We want to evaluate the performance of multiple models that we learned in class and not, on predicting this closing price movement using the dataset provided in the [Kaggle](https://www.kaggle.com/competitions/optiver-trading-at-the-close/overview), and see what models performs best, and what features we can engineer to improve on the performance of the models. \n",
    "\n",
    "### Project Goal\n",
    "We evaluate how well different machine learning algorithms can predict stock price movement near the market close. Our specific target is the change in the *market clearing price* during the NASDAQ Closing Cross.\n",
    "\n",
    "### Data Overview\n",
    "Each row represents auction-related market activity for a given stock on a given day. For our features, we will try using only the features provided in the dataset, then creating our own original features, trying features that the Kaggle competitors had success with, and finally a compilation of all features. Then, we will select only the most helpful features, and then test our best model on the test data set through the Kaggle. \n",
    "\n",
    "### Models and Algorithms\n",
    "For our models, we will begin with Linear Regression (Josephine), Random Forest (Brandon), LightGBM and CNN (Yueying), XGBoost (Cooper), and finally we will look into Catboost (Attila), a model developed by Yandex which the winner of the Kaggle used for his approach to this Kaggle. We compare all of these models using mean average error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3584d6",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "First, we define some helper functions to help us process and clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d48fe7b",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"./train.csv\", index_col=\"row_id\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c7eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(df: pd.DataFrame):\n",
    "    df.dropna(subset=[\"target\"], inplace=True)\n",
    "    X = df.drop([\"target\"], axis=1)\n",
    "    y = df[\"target\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248a436",
   "metadata": {},
   "source": [
    "We also define several processing functions.\n",
    "\n",
    "The function process_data_drop_null_target removes all rows with missing target values.\n",
    "\n",
    "The function process_data_drop_prices removes rows with missing near_price or far_price, because some models (linear regression) can't handle null features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971df0b8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def process_data_drop_null_target(df: pd.DataFrame):\n",
    "    df_processed = df.dropna(subset=[\"target\"]).copy()\n",
    "    X = df_processed.drop([\"target\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n",
    "\n",
    "def process_data_drop_prices(df: pd.DataFrame):\n",
    "    df_processed = df.dropna(subset=[\"near_price\", \"far_price\"]).copy()\n",
    "    X = df_processed.drop([\"target\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb98698",
   "metadata": {},
   "source": [
    "We create a general training function to handle training multiple models with the same dataset, as well as specialized cases for LightGBM due to early stopping and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1160195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_class, X_train, y_train, X_val, y_val, params=None):\n",
    "    if params:\n",
    "        model = model_class(**params)\n",
    "    else:\n",
    "        model = model_class()\n",
    "\n",
    "    if isinstance(model, LGBMRegressor):\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='mae',\n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=150, verbose=True),\n",
    "                log_evaluation(period=150)\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    return mae, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb692fb",
   "metadata": {},
   "source": [
    "Now we import the data and call the helper function to drop any null rows in our target. We borrow this reduce memory function from [here](https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model), and modify it slightly to fit our needs. This function downcasts the cells of the dataframe to a smaller datatype, for example if for a given column, the max values and min values can be fit into int8 instead of int64, it will downcast, and the 56 bit difference is huge when combined for over 5 million cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f2ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 679.36 MB\n",
      "Memory usage after optimization is: 304.71 MB\n",
      "Decreased by 55.15%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./train.csv\", index_col=\"row_id\") \n",
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = reduce_mem_usage(df, verbose=1)\n",
    "X, y = process_data_drop_null_target(df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76a4de",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We 'remove' the identifying feature \"row_id\", but we set it as the `index_col`, which is necessary for submitting to the Kaggle. Next, we need to split the data into a training and validating subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a523dd44",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b3c11",
   "metadata": {},
   "source": [
    "`train_test_split` shuffles the data on it's own, therefore there is nothing we need to do on that part. We are done with our initial data processing, but certain models cannot handle NaN values, so individual work will need to be done to get them to work. Now we can move on to Naive model training with the most basic features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690da49d",
   "metadata": {},
   "source": [
    "## Naive features\n",
    "\n",
    "Before doing any feature selection, we will first evaluate our models on the given features, to see how well feature engineering will improve our results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5540a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (Josephine)\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "X_lr, y_lr = process_data_drop_prices(df.copy())\n",
    "X_train_lr, X_val_lr, y_train_lr, y_val_lr = train_test_split(X_lr, y_lr, test_size=0.33, random_state=42)\n",
    "mae_lr, _ = train_model(LinearRegression, X_train_lr, y_train_lr, X_val_lr, y_val_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112c42f",
   "metadata": {},
   "source": [
    "For our next model we use random forest, an ensemble learning method that utilizes decision trees to output average predictions. While easy to tune, training on the full data set was computationally intensive and inefficient due to it's size. Since Random Forest was a core part of our class' material, we included it by training it on a smaller (5%) random sample of the dataset to reduce training time even though it isn't a good fit for what we are trying to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb35a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df_rf = df.sample(frac=0.05).copy()\n",
    "X_rf, y_rf = process_data_drop_null_target(df_rf)\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(X_rf, y_rf, test_size=0.33, random_state=42)\n",
    "rf_params = {\"n_estimators\": 100, \"random_state\": 42, \"n_jobs\": -1}\n",
    "mae_rf, _ = train_model(RandomForestRegressor, X_train_rf, y_train_rf, X_val_rf, y_val_rf, params=rf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301c23a",
   "metadata": {},
   "source": [
    "LightGBM (Light Gradient Boosting Machine) is a high-performance gradient boosting framework that is well-suited for large-scale datasets and tabular data with many features. It is particularly efficient for training on millions of observations, which made it a practical choice for our dataset of over 5 million instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4fd3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3316\n",
      "[LightGBM] [Info] Number of data points in the train set: 3509387, number of used features: 15\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA TITAN RTX, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 15 dense feature groups (53.55 MB) transferred to GPU in 0.074400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.044118\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l1: 6.30096\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "lgbm_params = {\n",
    "    'metric': 'mae',\n",
    "    'device': 'gpu',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "mae_lgbm, _ = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe1216",
   "metadata": {},
   "source": [
    "This code is for training an XGBoost regression model, evaluating it with MAE, and inspecting feature importances. It accepts the training and validation features and target values, along with a list of feature names to use for training. The function configures the model with predefined hyperparameters. These include a squared error objective, mean absolute error (MAE) as the evaluation, a learning rate of 0.05, and a maximum tree depth of 6. After training, it makes predictions and calculates the MAE to determine performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c13d6471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/attilaja/cfrm421-finalproj/.venv/lib/python3.12/site-packages/xgboost/core.py:729: UserWarning: [16:07:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:absoluteerror',\n",
    "    'eval_metric': 'mae',\n",
    "    'device': \"cuda\",\n",
    "    'random_state': 42\n",
    "}\n",
    "mae_xgb, _ = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1702a2",
   "metadata": {},
   "source": [
    "Catboost is the model that was used to win first place in the kaggle. Catboost itself is an algorithm with a novel gradient-boosting scheme on decision trees. It is developed by Yandex, and it has many benefits, such as providing great quality with the default parameters, saving time from parameter tuning, and it supports categorical features, instead of having to pre-process data to numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fede1c3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% GPU memory available for training. Free: 13314.5 Total: 24212.375\n",
      "Default metric period is 5 because MAE is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 6.4094595\ttotal: 407ms\tremaining: 6m 46s\n",
      "1:\ttotal: 520ms\tremaining: 4m 19s\n",
      "2:\ttotal: 624ms\tremaining: 3m 27s\n",
      "3:\ttotal: 725ms\tremaining: 3m\n",
      "4:\ttotal: 824ms\tremaining: 2m 44s\n",
      "5:\tlearn: 6.4085711\ttotal: 927ms\tremaining: 2m 33s\n",
      "6:\ttotal: 1.03s\tremaining: 2m 25s\n",
      "7:\ttotal: 1.13s\tremaining: 2m 19s\n",
      "8:\ttotal: 1.22s\tremaining: 2m 14s\n",
      "9:\ttotal: 1.33s\tremaining: 2m 11s\n",
      "10:\tlearn: 6.4076980\ttotal: 1.43s\tremaining: 2m 8s\n",
      "11:\ttotal: 1.53s\tremaining: 2m 5s\n",
      "12:\ttotal: 1.63s\tremaining: 2m 3s\n",
      "13:\ttotal: 1.73s\tremaining: 2m 1s\n",
      "14:\ttotal: 1.83s\tremaining: 2m\n",
      "15:\tlearn: 6.4068397\ttotal: 1.93s\tremaining: 1m 58s\n",
      "16:\ttotal: 2.03s\tremaining: 1m 57s\n",
      "17:\ttotal: 2.13s\tremaining: 1m 56s\n",
      "18:\ttotal: 2.23s\tremaining: 1m 55s\n",
      "19:\ttotal: 2.33s\tremaining: 1m 54s\n",
      "20:\tlearn: 6.4059911\ttotal: 2.44s\tremaining: 1m 53s\n",
      "21:\ttotal: 2.54s\tremaining: 1m 52s\n",
      "22:\ttotal: 2.64s\tremaining: 1m 52s\n",
      "23:\ttotal: 2.74s\tremaining: 1m 51s\n",
      "24:\ttotal: 2.84s\tremaining: 1m 50s\n",
      "25:\tlearn: 6.4051710\ttotal: 2.94s\tremaining: 1m 50s\n",
      "26:\ttotal: 3.04s\tremaining: 1m 49s\n",
      "27:\ttotal: 3.14s\tremaining: 1m 49s\n",
      "28:\ttotal: 3.25s\tremaining: 1m 48s\n",
      "29:\ttotal: 3.35s\tremaining: 1m 48s\n",
      "30:\tlearn: 6.4043538\ttotal: 3.45s\tremaining: 1m 47s\n",
      "31:\ttotal: 3.55s\tremaining: 1m 47s\n",
      "32:\ttotal: 3.65s\tremaining: 1m 46s\n",
      "33:\ttotal: 3.75s\tremaining: 1m 46s\n",
      "34:\ttotal: 3.85s\tremaining: 1m 46s\n",
      "35:\tlearn: 6.4035634\ttotal: 3.95s\tremaining: 1m 45s\n",
      "36:\ttotal: 4.05s\tremaining: 1m 45s\n",
      "37:\ttotal: 4.15s\tremaining: 1m 45s\n",
      "38:\ttotal: 4.25s\tremaining: 1m 44s\n",
      "39:\ttotal: 4.36s\tremaining: 1m 44s\n",
      "40:\tlearn: 6.4027706\ttotal: 4.46s\tremaining: 1m 44s\n",
      "41:\ttotal: 4.56s\tremaining: 1m 44s\n",
      "42:\ttotal: 4.66s\tremaining: 1m 43s\n",
      "43:\ttotal: 4.76s\tremaining: 1m 43s\n",
      "44:\ttotal: 4.86s\tremaining: 1m 43s\n",
      "45:\tlearn: 6.4020030\ttotal: 4.97s\tremaining: 1m 43s\n",
      "46:\ttotal: 5.07s\tremaining: 1m 42s\n",
      "47:\ttotal: 5.17s\tremaining: 1m 42s\n",
      "48:\ttotal: 5.27s\tremaining: 1m 42s\n",
      "49:\ttotal: 5.37s\tremaining: 1m 42s\n",
      "50:\tlearn: 6.4012342\ttotal: 5.48s\tremaining: 1m 41s\n",
      "51:\ttotal: 5.58s\tremaining: 1m 41s\n",
      "52:\ttotal: 5.68s\tremaining: 1m 41s\n",
      "53:\ttotal: 5.78s\tremaining: 1m 41s\n",
      "54:\ttotal: 5.88s\tremaining: 1m 41s\n",
      "55:\tlearn: 6.4004853\ttotal: 5.98s\tremaining: 1m 40s\n",
      "56:\ttotal: 6.08s\tremaining: 1m 40s\n",
      "57:\ttotal: 6.19s\tremaining: 1m 40s\n",
      "58:\ttotal: 6.29s\tremaining: 1m 40s\n",
      "59:\ttotal: 6.39s\tremaining: 1m 40s\n",
      "60:\tlearn: 6.3997541\ttotal: 6.5s\tremaining: 1m 40s\n",
      "61:\ttotal: 6.6s\tremaining: 1m 39s\n",
      "62:\ttotal: 6.7s\tremaining: 1m 39s\n",
      "63:\ttotal: 6.8s\tremaining: 1m 39s\n",
      "64:\ttotal: 6.9s\tremaining: 1m 39s\n",
      "65:\tlearn: 6.3990247\ttotal: 7s\tremaining: 1m 39s\n",
      "66:\ttotal: 7.1s\tremaining: 1m 38s\n",
      "67:\ttotal: 7.21s\tremaining: 1m 38s\n",
      "68:\ttotal: 7.31s\tremaining: 1m 38s\n",
      "69:\ttotal: 7.41s\tremaining: 1m 38s\n",
      "70:\tlearn: 6.3983208\ttotal: 7.51s\tremaining: 1m 38s\n",
      "71:\ttotal: 7.61s\tremaining: 1m 38s\n",
      "72:\ttotal: 7.72s\tremaining: 1m 38s\n",
      "73:\ttotal: 7.82s\tremaining: 1m 37s\n",
      "74:\ttotal: 7.92s\tremaining: 1m 37s\n",
      "75:\tlearn: 6.3976187\ttotal: 8.02s\tremaining: 1m 37s\n",
      "76:\ttotal: 8.12s\tremaining: 1m 37s\n",
      "77:\ttotal: 8.22s\tremaining: 1m 37s\n",
      "78:\ttotal: 8.32s\tremaining: 1m 37s\n",
      "79:\ttotal: 8.43s\tremaining: 1m 36s\n",
      "80:\tlearn: 6.3969434\ttotal: 8.53s\tremaining: 1m 36s\n",
      "81:\ttotal: 8.63s\tremaining: 1m 36s\n",
      "82:\ttotal: 8.74s\tremaining: 1m 36s\n",
      "83:\ttotal: 8.84s\tremaining: 1m 36s\n",
      "84:\ttotal: 8.94s\tremaining: 1m 36s\n",
      "85:\tlearn: 6.3962669\ttotal: 9.04s\tremaining: 1m 36s\n",
      "86:\ttotal: 9.14s\tremaining: 1m 35s\n",
      "87:\ttotal: 9.24s\tremaining: 1m 35s\n",
      "88:\ttotal: 9.35s\tremaining: 1m 35s\n",
      "89:\ttotal: 9.45s\tremaining: 1m 35s\n",
      "90:\tlearn: 6.3956133\ttotal: 9.55s\tremaining: 1m 35s\n",
      "91:\ttotal: 9.66s\tremaining: 1m 35s\n",
      "92:\ttotal: 9.76s\tremaining: 1m 35s\n",
      "93:\ttotal: 9.86s\tremaining: 1m 35s\n",
      "94:\ttotal: 9.96s\tremaining: 1m 34s\n",
      "95:\tlearn: 6.3949658\ttotal: 10.1s\tremaining: 1m 34s\n",
      "96:\ttotal: 10.2s\tremaining: 1m 34s\n",
      "97:\ttotal: 10.3s\tremaining: 1m 34s\n",
      "98:\ttotal: 10.4s\tremaining: 1m 34s\n",
      "99:\ttotal: 10.5s\tremaining: 1m 34s\n",
      "100:\tlearn: 6.3943162\ttotal: 10.6s\tremaining: 1m 34s\n",
      "101:\ttotal: 10.7s\tremaining: 1m 34s\n",
      "102:\ttotal: 10.8s\tremaining: 1m 33s\n",
      "103:\ttotal: 10.9s\tremaining: 1m 33s\n",
      "104:\ttotal: 11s\tremaining: 1m 33s\n",
      "105:\tlearn: 6.3936927\ttotal: 11.1s\tremaining: 1m 33s\n",
      "106:\ttotal: 11.2s\tremaining: 1m 33s\n",
      "107:\ttotal: 11.3s\tremaining: 1m 33s\n",
      "108:\ttotal: 11.4s\tremaining: 1m 33s\n",
      "109:\ttotal: 11.5s\tremaining: 1m 32s\n",
      "110:\tlearn: 6.3930743\ttotal: 11.6s\tremaining: 1m 32s\n",
      "111:\ttotal: 11.7s\tremaining: 1m 32s\n",
      "112:\ttotal: 11.8s\tremaining: 1m 32s\n",
      "113:\ttotal: 11.9s\tremaining: 1m 32s\n",
      "114:\ttotal: 12s\tremaining: 1m 32s\n",
      "115:\tlearn: 6.3924549\ttotal: 12.1s\tremaining: 1m 32s\n",
      "116:\ttotal: 12.2s\tremaining: 1m 32s\n",
      "117:\ttotal: 12.3s\tremaining: 1m 31s\n",
      "118:\ttotal: 12.4s\tremaining: 1m 31s\n",
      "119:\ttotal: 12.5s\tremaining: 1m 31s\n",
      "120:\tlearn: 6.3918576\ttotal: 12.6s\tremaining: 1m 31s\n",
      "121:\ttotal: 12.7s\tremaining: 1m 31s\n",
      "122:\ttotal: 12.8s\tremaining: 1m 31s\n",
      "123:\ttotal: 12.9s\tremaining: 1m 31s\n",
      "124:\ttotal: 13s\tremaining: 1m 31s\n",
      "125:\tlearn: 6.3912729\ttotal: 13.1s\tremaining: 1m 30s\n",
      "126:\ttotal: 13.2s\tremaining: 1m 30s\n",
      "127:\ttotal: 13.3s\tremaining: 1m 30s\n",
      "128:\ttotal: 13.4s\tremaining: 1m 30s\n",
      "129:\ttotal: 13.5s\tremaining: 1m 30s\n",
      "130:\tlearn: 6.3906910\ttotal: 13.6s\tremaining: 1m 30s\n",
      "131:\ttotal: 13.7s\tremaining: 1m 30s\n",
      "132:\ttotal: 13.8s\tremaining: 1m 30s\n",
      "133:\ttotal: 13.9s\tremaining: 1m 29s\n",
      "134:\ttotal: 14s\tremaining: 1m 29s\n",
      "135:\tlearn: 6.3901251\ttotal: 14.1s\tremaining: 1m 29s\n",
      "136:\ttotal: 14.2s\tremaining: 1m 29s\n",
      "137:\ttotal: 14.3s\tremaining: 1m 29s\n",
      "138:\ttotal: 14.4s\tremaining: 1m 29s\n",
      "139:\ttotal: 14.5s\tremaining: 1m 29s\n",
      "140:\tlearn: 6.3895638\ttotal: 14.6s\tremaining: 1m 29s\n",
      "141:\ttotal: 14.7s\tremaining: 1m 29s\n",
      "142:\ttotal: 14.8s\tremaining: 1m 28s\n",
      "143:\ttotal: 14.9s\tremaining: 1m 28s\n",
      "144:\ttotal: 15s\tremaining: 1m 28s\n",
      "145:\tlearn: 6.3890201\ttotal: 15.1s\tremaining: 1m 28s\n",
      "146:\ttotal: 15.2s\tremaining: 1m 28s\n",
      "147:\ttotal: 15.3s\tremaining: 1m 28s\n",
      "148:\ttotal: 15.4s\tremaining: 1m 28s\n",
      "149:\ttotal: 15.6s\tremaining: 1m 28s\n",
      "150:\tlearn: 6.3884656\ttotal: 15.7s\tremaining: 1m 28s\n",
      "151:\ttotal: 15.8s\tremaining: 1m 27s\n",
      "152:\ttotal: 15.9s\tremaining: 1m 27s\n",
      "153:\ttotal: 16s\tremaining: 1m 27s\n",
      "154:\ttotal: 16.1s\tremaining: 1m 27s\n",
      "155:\tlearn: 6.3879413\ttotal: 16.2s\tremaining: 1m 27s\n",
      "156:\ttotal: 16.3s\tremaining: 1m 27s\n",
      "157:\ttotal: 16.4s\tremaining: 1m 27s\n",
      "158:\ttotal: 16.5s\tremaining: 1m 27s\n",
      "159:\ttotal: 16.6s\tremaining: 1m 27s\n",
      "160:\tlearn: 6.3874238\ttotal: 16.7s\tremaining: 1m 26s\n",
      "161:\ttotal: 16.8s\tremaining: 1m 26s\n",
      "162:\ttotal: 16.9s\tremaining: 1m 26s\n",
      "163:\ttotal: 17s\tremaining: 1m 26s\n",
      "164:\ttotal: 17.1s\tremaining: 1m 26s\n",
      "165:\tlearn: 6.3869023\ttotal: 17.2s\tremaining: 1m 26s\n",
      "166:\ttotal: 17.3s\tremaining: 1m 26s\n",
      "167:\ttotal: 17.4s\tremaining: 1m 26s\n",
      "168:\ttotal: 17.5s\tremaining: 1m 26s\n",
      "169:\ttotal: 17.6s\tremaining: 1m 26s\n",
      "170:\tlearn: 6.3863900\ttotal: 17.7s\tremaining: 1m 25s\n",
      "171:\ttotal: 17.8s\tremaining: 1m 25s\n",
      "172:\ttotal: 17.9s\tremaining: 1m 25s\n",
      "173:\ttotal: 18s\tremaining: 1m 25s\n",
      "174:\ttotal: 18.1s\tremaining: 1m 25s\n",
      "175:\tlearn: 6.3858896\ttotal: 18.2s\tremaining: 1m 25s\n",
      "176:\ttotal: 18.3s\tremaining: 1m 25s\n",
      "177:\ttotal: 18.4s\tremaining: 1m 25s\n",
      "178:\ttotal: 18.5s\tremaining: 1m 25s\n",
      "179:\ttotal: 18.6s\tremaining: 1m 24s\n",
      "180:\tlearn: 6.3853927\ttotal: 18.8s\tremaining: 1m 24s\n",
      "181:\ttotal: 18.9s\tremaining: 1m 24s\n",
      "182:\ttotal: 19s\tremaining: 1m 24s\n",
      "183:\ttotal: 19.1s\tremaining: 1m 24s\n",
      "184:\ttotal: 19.2s\tremaining: 1m 24s\n",
      "185:\tlearn: 6.3849065\ttotal: 19.3s\tremaining: 1m 24s\n",
      "186:\ttotal: 19.4s\tremaining: 1m 24s\n",
      "187:\ttotal: 19.5s\tremaining: 1m 24s\n",
      "188:\ttotal: 19.6s\tremaining: 1m 24s\n",
      "189:\ttotal: 19.7s\tremaining: 1m 23s\n",
      "190:\tlearn: 6.3844489\ttotal: 19.8s\tremaining: 1m 23s\n",
      "191:\ttotal: 19.9s\tremaining: 1m 23s\n",
      "192:\ttotal: 20s\tremaining: 1m 23s\n",
      "193:\ttotal: 20.1s\tremaining: 1m 23s\n",
      "194:\ttotal: 20.2s\tremaining: 1m 23s\n",
      "195:\tlearn: 6.3839702\ttotal: 20.3s\tremaining: 1m 23s\n",
      "196:\ttotal: 20.4s\tremaining: 1m 23s\n",
      "197:\ttotal: 20.5s\tremaining: 1m 23s\n",
      "198:\ttotal: 20.6s\tremaining: 1m 23s\n",
      "199:\ttotal: 20.7s\tremaining: 1m 22s\n",
      "200:\tlearn: 6.3835103\ttotal: 20.8s\tremaining: 1m 22s\n",
      "201:\ttotal: 20.9s\tremaining: 1m 22s\n",
      "202:\ttotal: 21s\tremaining: 1m 22s\n",
      "203:\ttotal: 21.1s\tremaining: 1m 22s\n",
      "204:\ttotal: 21.3s\tremaining: 1m 22s\n",
      "205:\tlearn: 6.3830498\ttotal: 21.4s\tremaining: 1m 22s\n",
      "206:\ttotal: 21.5s\tremaining: 1m 22s\n",
      "207:\ttotal: 21.6s\tremaining: 1m 22s\n",
      "208:\ttotal: 21.7s\tremaining: 1m 22s\n",
      "209:\ttotal: 21.8s\tremaining: 1m 21s\n",
      "210:\tlearn: 6.3825967\ttotal: 21.9s\tremaining: 1m 21s\n",
      "211:\ttotal: 22s\tremaining: 1m 21s\n",
      "212:\ttotal: 22.1s\tremaining: 1m 21s\n",
      "213:\ttotal: 22.2s\tremaining: 1m 21s\n",
      "214:\ttotal: 22.3s\tremaining: 1m 21s\n",
      "215:\tlearn: 6.3821602\ttotal: 22.4s\tremaining: 1m 21s\n",
      "216:\ttotal: 22.5s\tremaining: 1m 21s\n",
      "217:\ttotal: 22.6s\tremaining: 1m 21s\n",
      "218:\ttotal: 22.7s\tremaining: 1m 21s\n",
      "219:\ttotal: 22.8s\tremaining: 1m 21s\n",
      "220:\tlearn: 6.3817231\ttotal: 23s\tremaining: 1m 20s\n",
      "221:\ttotal: 23.1s\tremaining: 1m 20s\n",
      "222:\ttotal: 23.2s\tremaining: 1m 20s\n",
      "223:\ttotal: 23.3s\tremaining: 1m 20s\n",
      "224:\ttotal: 23.4s\tremaining: 1m 20s\n",
      "225:\tlearn: 6.3812922\ttotal: 23.5s\tremaining: 1m 20s\n",
      "226:\ttotal: 23.6s\tremaining: 1m 20s\n",
      "227:\ttotal: 23.7s\tremaining: 1m 20s\n",
      "228:\ttotal: 23.8s\tremaining: 1m 20s\n",
      "229:\ttotal: 23.9s\tremaining: 1m 20s\n",
      "230:\tlearn: 6.3808671\ttotal: 24s\tremaining: 1m 19s\n",
      "231:\ttotal: 24.1s\tremaining: 1m 19s\n",
      "232:\ttotal: 24.2s\tremaining: 1m 19s\n",
      "233:\ttotal: 24.3s\tremaining: 1m 19s\n",
      "234:\ttotal: 24.4s\tremaining: 1m 19s\n",
      "235:\tlearn: 6.3804756\ttotal: 24.5s\tremaining: 1m 19s\n",
      "236:\ttotal: 24.6s\tremaining: 1m 19s\n",
      "237:\ttotal: 24.8s\tremaining: 1m 19s\n",
      "238:\ttotal: 24.9s\tremaining: 1m 19s\n",
      "239:\ttotal: 25s\tremaining: 1m 19s\n",
      "240:\tlearn: 6.3800595\ttotal: 25.1s\tremaining: 1m 18s\n",
      "241:\ttotal: 25.2s\tremaining: 1m 18s\n",
      "242:\ttotal: 25.3s\tremaining: 1m 18s\n",
      "243:\ttotal: 25.4s\tremaining: 1m 18s\n",
      "244:\ttotal: 25.5s\tremaining: 1m 18s\n",
      "245:\tlearn: 6.3796600\ttotal: 25.6s\tremaining: 1m 18s\n",
      "246:\ttotal: 25.7s\tremaining: 1m 18s\n",
      "247:\ttotal: 25.8s\tremaining: 1m 18s\n",
      "248:\ttotal: 25.9s\tremaining: 1m 18s\n",
      "249:\ttotal: 26s\tremaining: 1m 18s\n",
      "250:\tlearn: 6.3792605\ttotal: 26.1s\tremaining: 1m 18s\n",
      "251:\ttotal: 26.3s\tremaining: 1m 17s\n",
      "252:\ttotal: 26.4s\tremaining: 1m 17s\n",
      "253:\ttotal: 26.5s\tremaining: 1m 17s\n",
      "254:\ttotal: 26.6s\tremaining: 1m 17s\n",
      "255:\tlearn: 6.3788730\ttotal: 26.7s\tremaining: 1m 17s\n",
      "256:\ttotal: 26.8s\tremaining: 1m 17s\n",
      "257:\ttotal: 26.9s\tremaining: 1m 17s\n",
      "258:\ttotal: 27s\tremaining: 1m 17s\n",
      "259:\ttotal: 27.1s\tremaining: 1m 17s\n",
      "260:\tlearn: 6.3784826\ttotal: 27.2s\tremaining: 1m 17s\n",
      "261:\ttotal: 27.3s\tremaining: 1m 16s\n",
      "262:\ttotal: 27.4s\tremaining: 1m 16s\n",
      "263:\ttotal: 27.5s\tremaining: 1m 16s\n",
      "264:\ttotal: 27.6s\tremaining: 1m 16s\n",
      "265:\tlearn: 6.3781088\ttotal: 27.8s\tremaining: 1m 16s\n",
      "266:\ttotal: 27.9s\tremaining: 1m 16s\n",
      "267:\ttotal: 28s\tremaining: 1m 16s\n",
      "268:\ttotal: 28.1s\tremaining: 1m 16s\n",
      "269:\ttotal: 28.2s\tremaining: 1m 16s\n",
      "270:\tlearn: 6.3777275\ttotal: 28.3s\tremaining: 1m 16s\n",
      "271:\ttotal: 28.4s\tremaining: 1m 16s\n",
      "272:\ttotal: 28.5s\tremaining: 1m 15s\n",
      "273:\ttotal: 28.6s\tremaining: 1m 15s\n",
      "274:\ttotal: 28.7s\tremaining: 1m 15s\n",
      "275:\tlearn: 6.3773696\ttotal: 28.8s\tremaining: 1m 15s\n",
      "276:\ttotal: 29s\tremaining: 1m 15s\n",
      "277:\ttotal: 29.1s\tremaining: 1m 15s\n",
      "278:\ttotal: 29.2s\tremaining: 1m 15s\n",
      "279:\ttotal: 29.3s\tremaining: 1m 15s\n",
      "280:\tlearn: 6.3770020\ttotal: 29.4s\tremaining: 1m 15s\n",
      "281:\ttotal: 29.5s\tremaining: 1m 15s\n",
      "282:\ttotal: 29.6s\tremaining: 1m 15s\n",
      "283:\ttotal: 29.7s\tremaining: 1m 14s\n",
      "284:\ttotal: 29.8s\tremaining: 1m 14s\n",
      "285:\tlearn: 6.3766350\ttotal: 30s\tremaining: 1m 14s\n",
      "286:\ttotal: 30.1s\tremaining: 1m 14s\n",
      "287:\ttotal: 30.2s\tremaining: 1m 14s\n",
      "288:\ttotal: 30.3s\tremaining: 1m 14s\n",
      "289:\ttotal: 30.4s\tremaining: 1m 14s\n",
      "290:\tlearn: 6.3762822\ttotal: 30.5s\tremaining: 1m 14s\n",
      "291:\ttotal: 30.6s\tremaining: 1m 14s\n",
      "292:\ttotal: 30.7s\tremaining: 1m 14s\n",
      "293:\ttotal: 30.8s\tremaining: 1m 14s\n",
      "294:\ttotal: 31s\tremaining: 1m 13s\n",
      "295:\tlearn: 6.3759477\ttotal: 31.1s\tremaining: 1m 13s\n",
      "296:\ttotal: 31.2s\tremaining: 1m 13s\n",
      "297:\ttotal: 31.3s\tremaining: 1m 13s\n",
      "298:\ttotal: 31.4s\tremaining: 1m 13s\n",
      "299:\ttotal: 31.5s\tremaining: 1m 13s\n",
      "300:\tlearn: 6.3756001\ttotal: 31.6s\tremaining: 1m 13s\n",
      "301:\ttotal: 31.7s\tremaining: 1m 13s\n",
      "302:\ttotal: 31.9s\tremaining: 1m 13s\n",
      "303:\ttotal: 32s\tremaining: 1m 13s\n",
      "304:\ttotal: 32.1s\tremaining: 1m 13s\n",
      "305:\tlearn: 6.3752587\ttotal: 32.2s\tremaining: 1m 12s\n",
      "306:\ttotal: 32.3s\tremaining: 1m 12s\n",
      "307:\ttotal: 32.4s\tremaining: 1m 12s\n",
      "308:\ttotal: 32.5s\tremaining: 1m 12s\n",
      "309:\ttotal: 32.6s\tremaining: 1m 12s\n",
      "310:\tlearn: 6.3749361\ttotal: 32.7s\tremaining: 1m 12s\n",
      "311:\ttotal: 32.9s\tremaining: 1m 12s\n",
      "312:\ttotal: 33s\tremaining: 1m 12s\n",
      "313:\ttotal: 33.1s\tremaining: 1m 12s\n",
      "314:\ttotal: 33.2s\tremaining: 1m 12s\n",
      "315:\tlearn: 6.3746022\ttotal: 33.3s\tremaining: 1m 12s\n",
      "316:\ttotal: 33.4s\tremaining: 1m 12s\n",
      "317:\ttotal: 33.5s\tremaining: 1m 11s\n",
      "318:\ttotal: 33.6s\tremaining: 1m 11s\n",
      "319:\ttotal: 33.7s\tremaining: 1m 11s\n",
      "320:\tlearn: 6.3742882\ttotal: 33.9s\tremaining: 1m 11s\n",
      "321:\ttotal: 34s\tremaining: 1m 11s\n",
      "322:\ttotal: 34.1s\tremaining: 1m 11s\n",
      "323:\ttotal: 34.2s\tremaining: 1m 11s\n",
      "324:\ttotal: 34.3s\tremaining: 1m 11s\n",
      "325:\tlearn: 6.3739793\ttotal: 34.4s\tremaining: 1m 11s\n",
      "326:\ttotal: 34.5s\tremaining: 1m 11s\n",
      "327:\ttotal: 34.6s\tremaining: 1m 10s\n",
      "328:\ttotal: 34.8s\tremaining: 1m 10s\n",
      "329:\ttotal: 34.9s\tremaining: 1m 10s\n",
      "330:\tlearn: 6.3736573\ttotal: 35s\tremaining: 1m 10s\n",
      "331:\ttotal: 35.1s\tremaining: 1m 10s\n",
      "332:\ttotal: 35.2s\tremaining: 1m 10s\n",
      "333:\ttotal: 35.3s\tremaining: 1m 10s\n",
      "334:\ttotal: 35.4s\tremaining: 1m 10s\n",
      "335:\tlearn: 6.3733376\ttotal: 35.5s\tremaining: 1m 10s\n",
      "336:\ttotal: 35.7s\tremaining: 1m 10s\n",
      "337:\ttotal: 35.8s\tremaining: 1m 10s\n",
      "338:\ttotal: 35.9s\tremaining: 1m 9s\n",
      "339:\ttotal: 36s\tremaining: 1m 9s\n",
      "340:\tlearn: 6.3730344\ttotal: 36.1s\tremaining: 1m 9s\n",
      "341:\ttotal: 36.2s\tremaining: 1m 9s\n",
      "342:\ttotal: 36.3s\tremaining: 1m 9s\n",
      "343:\ttotal: 36.4s\tremaining: 1m 9s\n",
      "344:\ttotal: 36.6s\tremaining: 1m 9s\n",
      "345:\tlearn: 6.3727352\ttotal: 36.7s\tremaining: 1m 9s\n",
      "346:\ttotal: 36.8s\tremaining: 1m 9s\n",
      "347:\ttotal: 36.9s\tremaining: 1m 9s\n",
      "348:\ttotal: 37s\tremaining: 1m 9s\n",
      "349:\ttotal: 37.1s\tremaining: 1m 8s\n",
      "350:\tlearn: 6.3724366\ttotal: 37.2s\tremaining: 1m 8s\n",
      "351:\ttotal: 37.4s\tremaining: 1m 8s\n",
      "352:\ttotal: 37.5s\tremaining: 1m 8s\n",
      "353:\ttotal: 37.6s\tremaining: 1m 8s\n",
      "354:\ttotal: 37.7s\tremaining: 1m 8s\n",
      "355:\tlearn: 6.3721590\ttotal: 37.8s\tremaining: 1m 8s\n",
      "356:\ttotal: 37.9s\tremaining: 1m 8s\n",
      "357:\ttotal: 38s\tremaining: 1m 8s\n",
      "358:\ttotal: 38.1s\tremaining: 1m 8s\n",
      "359:\ttotal: 38.3s\tremaining: 1m 8s\n",
      "360:\tlearn: 6.3718649\ttotal: 38.4s\tremaining: 1m 7s\n",
      "361:\ttotal: 38.5s\tremaining: 1m 7s\n",
      "362:\ttotal: 38.6s\tremaining: 1m 7s\n",
      "363:\ttotal: 38.7s\tremaining: 1m 7s\n",
      "364:\ttotal: 38.8s\tremaining: 1m 7s\n",
      "365:\tlearn: 6.3715749\ttotal: 39s\tremaining: 1m 7s\n",
      "366:\ttotal: 39.1s\tremaining: 1m 7s\n",
      "367:\ttotal: 39.2s\tremaining: 1m 7s\n",
      "368:\ttotal: 39.3s\tremaining: 1m 7s\n",
      "369:\ttotal: 39.4s\tremaining: 1m 7s\n",
      "370:\tlearn: 6.3713047\ttotal: 39.5s\tremaining: 1m 7s\n",
      "371:\ttotal: 39.6s\tremaining: 1m 6s\n",
      "372:\ttotal: 39.7s\tremaining: 1m 6s\n",
      "373:\ttotal: 39.9s\tremaining: 1m 6s\n",
      "374:\ttotal: 40s\tremaining: 1m 6s\n",
      "375:\tlearn: 6.3710204\ttotal: 40.1s\tremaining: 1m 6s\n",
      "376:\ttotal: 40.2s\tremaining: 1m 6s\n",
      "377:\ttotal: 40.3s\tremaining: 1m 6s\n",
      "378:\ttotal: 40.4s\tremaining: 1m 6s\n",
      "379:\ttotal: 40.5s\tremaining: 1m 6s\n",
      "380:\tlearn: 6.3707508\ttotal: 40.7s\tremaining: 1m 6s\n",
      "381:\ttotal: 40.8s\tremaining: 1m 5s\n",
      "382:\ttotal: 40.9s\tremaining: 1m 5s\n",
      "383:\ttotal: 41s\tremaining: 1m 5s\n",
      "384:\ttotal: 41.1s\tremaining: 1m 5s\n",
      "385:\tlearn: 6.3704915\ttotal: 41.2s\tremaining: 1m 5s\n",
      "386:\ttotal: 41.3s\tremaining: 1m 5s\n",
      "387:\ttotal: 41.5s\tremaining: 1m 5s\n",
      "388:\ttotal: 41.6s\tremaining: 1m 5s\n",
      "389:\ttotal: 41.7s\tremaining: 1m 5s\n",
      "390:\tlearn: 6.3702333\ttotal: 41.8s\tremaining: 1m 5s\n",
      "391:\ttotal: 41.9s\tremaining: 1m 5s\n",
      "392:\ttotal: 42s\tremaining: 1m 4s\n",
      "393:\ttotal: 42.1s\tremaining: 1m 4s\n",
      "394:\ttotal: 42.3s\tremaining: 1m 4s\n",
      "395:\tlearn: 6.3699734\ttotal: 42.4s\tremaining: 1m 4s\n",
      "396:\ttotal: 42.5s\tremaining: 1m 4s\n",
      "397:\ttotal: 42.6s\tremaining: 1m 4s\n",
      "398:\ttotal: 42.7s\tremaining: 1m 4s\n",
      "399:\ttotal: 42.8s\tremaining: 1m 4s\n",
      "400:\tlearn: 6.3697153\ttotal: 42.9s\tremaining: 1m 4s\n",
      "401:\ttotal: 43.1s\tremaining: 1m 4s\n",
      "402:\ttotal: 43.2s\tremaining: 1m 3s\n",
      "403:\ttotal: 43.3s\tremaining: 1m 3s\n",
      "404:\ttotal: 43.4s\tremaining: 1m 3s\n",
      "405:\tlearn: 6.3694491\ttotal: 43.5s\tremaining: 1m 3s\n",
      "406:\ttotal: 43.6s\tremaining: 1m 3s\n",
      "407:\ttotal: 43.7s\tremaining: 1m 3s\n",
      "408:\ttotal: 43.9s\tremaining: 1m 3s\n",
      "409:\ttotal: 44s\tremaining: 1m 3s\n",
      "410:\tlearn: 6.3691881\ttotal: 44.1s\tremaining: 1m 3s\n",
      "411:\ttotal: 44.2s\tremaining: 1m 3s\n",
      "412:\ttotal: 44.3s\tremaining: 1m 3s\n",
      "413:\ttotal: 44.4s\tremaining: 1m 2s\n",
      "414:\ttotal: 44.6s\tremaining: 1m 2s\n",
      "415:\tlearn: 6.3689533\ttotal: 44.7s\tremaining: 1m 2s\n",
      "416:\ttotal: 44.8s\tremaining: 1m 2s\n",
      "417:\ttotal: 44.9s\tremaining: 1m 2s\n",
      "418:\ttotal: 45s\tremaining: 1m 2s\n",
      "419:\ttotal: 45.1s\tremaining: 1m 2s\n",
      "420:\tlearn: 6.3687066\ttotal: 45.2s\tremaining: 1m 2s\n",
      "421:\ttotal: 45.4s\tremaining: 1m 2s\n",
      "422:\ttotal: 45.5s\tremaining: 1m 2s\n",
      "423:\ttotal: 45.6s\tremaining: 1m 1s\n",
      "424:\ttotal: 45.7s\tremaining: 1m 1s\n",
      "425:\tlearn: 6.3684541\ttotal: 45.8s\tremaining: 1m 1s\n",
      "426:\ttotal: 45.9s\tremaining: 1m 1s\n",
      "427:\ttotal: 46.1s\tremaining: 1m 1s\n",
      "428:\ttotal: 46.2s\tremaining: 1m 1s\n",
      "429:\ttotal: 46.3s\tremaining: 1m 1s\n",
      "430:\tlearn: 6.3682239\ttotal: 46.4s\tremaining: 1m 1s\n",
      "431:\ttotal: 46.5s\tremaining: 1m 1s\n",
      "432:\ttotal: 46.6s\tremaining: 1m 1s\n",
      "433:\ttotal: 46.8s\tremaining: 1m\n",
      "434:\ttotal: 46.9s\tremaining: 1m\n",
      "435:\tlearn: 6.3679930\ttotal: 47s\tremaining: 1m\n",
      "436:\ttotal: 47.1s\tremaining: 1m\n",
      "437:\ttotal: 47.2s\tremaining: 1m\n",
      "438:\ttotal: 47.3s\tremaining: 1m\n",
      "439:\ttotal: 47.5s\tremaining: 1m\n",
      "440:\tlearn: 6.3677520\ttotal: 47.6s\tremaining: 1m\n",
      "441:\ttotal: 47.7s\tremaining: 1m\n",
      "442:\ttotal: 47.8s\tremaining: 1m\n",
      "443:\ttotal: 47.9s\tremaining: 1m\n",
      "444:\ttotal: 48s\tremaining: 59.9s\n",
      "445:\tlearn: 6.3675360\ttotal: 48.2s\tremaining: 59.8s\n",
      "446:\ttotal: 48.3s\tremaining: 59.7s\n",
      "447:\ttotal: 48.4s\tremaining: 59.6s\n",
      "448:\ttotal: 48.5s\tremaining: 59.5s\n",
      "449:\ttotal: 48.6s\tremaining: 59.4s\n",
      "450:\tlearn: 6.3673183\ttotal: 48.7s\tremaining: 59.3s\n",
      "451:\ttotal: 48.9s\tremaining: 59.2s\n",
      "452:\ttotal: 49s\tremaining: 59.1s\n",
      "453:\ttotal: 49.1s\tremaining: 59s\n",
      "454:\ttotal: 49.2s\tremaining: 58.9s\n",
      "455:\tlearn: 6.3670898\ttotal: 49.3s\tremaining: 58.8s\n",
      "456:\ttotal: 49.4s\tremaining: 58.7s\n",
      "457:\ttotal: 49.5s\tremaining: 58.6s\n",
      "458:\ttotal: 49.7s\tremaining: 58.5s\n",
      "459:\ttotal: 49.8s\tremaining: 58.4s\n",
      "460:\tlearn: 6.3668584\ttotal: 49.9s\tremaining: 58.3s\n",
      "461:\ttotal: 50s\tremaining: 58.2s\n",
      "462:\ttotal: 50.1s\tremaining: 58.1s\n",
      "463:\ttotal: 50.2s\tremaining: 58s\n",
      "464:\ttotal: 50.4s\tremaining: 57.9s\n",
      "465:\tlearn: 6.3666498\ttotal: 50.5s\tremaining: 57.8s\n",
      "466:\ttotal: 50.6s\tremaining: 57.7s\n",
      "467:\ttotal: 50.7s\tremaining: 57.6s\n",
      "468:\ttotal: 50.8s\tremaining: 57.6s\n",
      "469:\ttotal: 50.9s\tremaining: 57.5s\n",
      "470:\tlearn: 6.3664270\ttotal: 51.1s\tremaining: 57.4s\n",
      "471:\ttotal: 51.2s\tremaining: 57.3s\n",
      "472:\ttotal: 51.3s\tremaining: 57.2s\n",
      "473:\ttotal: 51.4s\tremaining: 57.1s\n",
      "474:\ttotal: 51.5s\tremaining: 57s\n",
      "475:\tlearn: 6.3662155\ttotal: 51.6s\tremaining: 56.9s\n",
      "476:\ttotal: 51.8s\tremaining: 56.8s\n",
      "477:\ttotal: 51.9s\tremaining: 56.7s\n",
      "478:\ttotal: 52s\tremaining: 56.6s\n",
      "479:\ttotal: 52.1s\tremaining: 56.5s\n",
      "480:\tlearn: 6.3660143\ttotal: 52.2s\tremaining: 56.4s\n",
      "481:\ttotal: 52.3s\tremaining: 56.3s\n",
      "482:\ttotal: 52.5s\tremaining: 56.2s\n",
      "483:\ttotal: 52.6s\tremaining: 56.1s\n",
      "484:\ttotal: 52.7s\tremaining: 56s\n",
      "485:\tlearn: 6.3658137\ttotal: 52.8s\tremaining: 55.9s\n",
      "486:\ttotal: 52.9s\tremaining: 55.8s\n",
      "487:\ttotal: 53.1s\tremaining: 55.7s\n",
      "488:\ttotal: 53.2s\tremaining: 55.6s\n",
      "489:\ttotal: 53.3s\tremaining: 55.5s\n",
      "490:\tlearn: 6.3656160\ttotal: 53.4s\tremaining: 55.4s\n",
      "491:\ttotal: 53.5s\tremaining: 55.3s\n",
      "492:\ttotal: 53.6s\tremaining: 55.2s\n",
      "493:\ttotal: 53.8s\tremaining: 55.1s\n",
      "494:\ttotal: 53.9s\tremaining: 55s\n",
      "495:\tlearn: 6.3654245\ttotal: 54s\tremaining: 54.9s\n",
      "496:\ttotal: 54.1s\tremaining: 54.8s\n",
      "497:\ttotal: 54.2s\tremaining: 54.7s\n",
      "498:\ttotal: 54.4s\tremaining: 54.6s\n",
      "499:\ttotal: 54.5s\tremaining: 54.5s\n",
      "500:\tlearn: 6.3652302\ttotal: 54.6s\tremaining: 54.4s\n",
      "501:\ttotal: 54.7s\tremaining: 54.3s\n",
      "502:\ttotal: 54.8s\tremaining: 54.2s\n",
      "503:\ttotal: 54.9s\tremaining: 54.1s\n",
      "504:\ttotal: 55s\tremaining: 54s\n",
      "505:\tlearn: 6.3650393\ttotal: 55.2s\tremaining: 53.9s\n",
      "506:\ttotal: 55.3s\tremaining: 53.8s\n",
      "507:\ttotal: 55.4s\tremaining: 53.7s\n",
      "508:\ttotal: 55.5s\tremaining: 53.6s\n",
      "509:\ttotal: 55.6s\tremaining: 53.4s\n",
      "510:\tlearn: 6.3648421\ttotal: 55.7s\tremaining: 53.3s\n",
      "511:\ttotal: 55.9s\tremaining: 53.2s\n",
      "512:\ttotal: 56s\tremaining: 53.1s\n",
      "513:\ttotal: 56.1s\tremaining: 53s\n",
      "514:\ttotal: 56.2s\tremaining: 52.9s\n",
      "515:\tlearn: 6.3646477\ttotal: 56.3s\tremaining: 52.8s\n",
      "516:\ttotal: 56.5s\tremaining: 52.7s\n",
      "517:\ttotal: 56.6s\tremaining: 52.6s\n",
      "518:\ttotal: 56.7s\tremaining: 52.5s\n",
      "519:\ttotal: 56.8s\tremaining: 52.4s\n",
      "520:\tlearn: 6.3644625\ttotal: 56.9s\tremaining: 52.3s\n",
      "521:\ttotal: 57s\tremaining: 52.2s\n",
      "522:\ttotal: 57.2s\tremaining: 52.1s\n",
      "523:\ttotal: 57.3s\tremaining: 52s\n",
      "524:\ttotal: 57.4s\tremaining: 51.9s\n",
      "525:\tlearn: 6.3642841\ttotal: 57.5s\tremaining: 51.8s\n",
      "526:\ttotal: 57.6s\tremaining: 51.7s\n",
      "527:\ttotal: 57.7s\tremaining: 51.6s\n",
      "528:\ttotal: 57.9s\tremaining: 51.5s\n",
      "529:\ttotal: 58s\tremaining: 51.4s\n",
      "530:\tlearn: 6.3641012\ttotal: 58.1s\tremaining: 51.3s\n",
      "531:\ttotal: 58.2s\tremaining: 51.2s\n",
      "532:\ttotal: 58.3s\tremaining: 51.1s\n",
      "533:\ttotal: 58.5s\tremaining: 51s\n",
      "534:\ttotal: 58.6s\tremaining: 50.9s\n",
      "535:\tlearn: 6.3639308\ttotal: 58.7s\tremaining: 50.8s\n",
      "536:\ttotal: 58.8s\tremaining: 50.7s\n",
      "537:\ttotal: 58.9s\tremaining: 50.6s\n",
      "538:\ttotal: 59s\tremaining: 50.5s\n",
      "539:\ttotal: 59.2s\tremaining: 50.4s\n",
      "540:\tlearn: 6.3637541\ttotal: 59.3s\tremaining: 50.3s\n",
      "541:\ttotal: 59.4s\tremaining: 50.2s\n",
      "542:\ttotal: 59.5s\tremaining: 50.1s\n",
      "543:\ttotal: 59.6s\tremaining: 50s\n",
      "544:\ttotal: 59.8s\tremaining: 49.9s\n",
      "545:\tlearn: 6.3635700\ttotal: 59.9s\tremaining: 49.8s\n",
      "546:\ttotal: 60s\tremaining: 49.7s\n",
      "547:\ttotal: 1m\tremaining: 49.6s\n",
      "548:\ttotal: 1m\tremaining: 49.5s\n",
      "549:\ttotal: 1m\tremaining: 49.4s\n",
      "550:\tlearn: 6.3633962\ttotal: 1m\tremaining: 49.3s\n",
      "551:\ttotal: 1m\tremaining: 49.2s\n",
      "552:\ttotal: 1m\tremaining: 49.1s\n",
      "553:\ttotal: 1m\tremaining: 49s\n",
      "554:\ttotal: 1m\tremaining: 48.9s\n",
      "555:\tlearn: 6.3632435\ttotal: 1m 1s\tremaining: 48.8s\n",
      "556:\ttotal: 1m 1s\tremaining: 48.7s\n",
      "557:\ttotal: 1m 1s\tremaining: 48.6s\n",
      "558:\ttotal: 1m 1s\tremaining: 48.4s\n",
      "559:\ttotal: 1m 1s\tremaining: 48.3s\n",
      "560:\tlearn: 6.3630868\ttotal: 1m 1s\tremaining: 48.2s\n",
      "561:\ttotal: 1m 1s\tremaining: 48.1s\n",
      "562:\ttotal: 1m 1s\tremaining: 48s\n",
      "563:\ttotal: 1m 2s\tremaining: 47.9s\n",
      "564:\ttotal: 1m 2s\tremaining: 47.8s\n",
      "565:\tlearn: 6.3629221\ttotal: 1m 2s\tremaining: 47.7s\n",
      "566:\ttotal: 1m 2s\tremaining: 47.6s\n",
      "567:\ttotal: 1m 2s\tremaining: 47.5s\n",
      "568:\ttotal: 1m 2s\tremaining: 47.4s\n",
      "569:\ttotal: 1m 2s\tremaining: 47.3s\n",
      "570:\tlearn: 6.3627591\ttotal: 1m 2s\tremaining: 47.2s\n",
      "571:\ttotal: 1m 2s\tremaining: 47.1s\n",
      "572:\ttotal: 1m 3s\tremaining: 47s\n",
      "573:\ttotal: 1m 3s\tremaining: 46.9s\n",
      "574:\ttotal: 1m 3s\tremaining: 46.8s\n",
      "575:\tlearn: 6.3625898\ttotal: 1m 3s\tremaining: 46.7s\n",
      "576:\ttotal: 1m 3s\tremaining: 46.6s\n",
      "577:\ttotal: 1m 3s\tremaining: 46.5s\n",
      "578:\ttotal: 1m 3s\tremaining: 46.4s\n",
      "579:\ttotal: 1m 3s\tremaining: 46.3s\n",
      "580:\tlearn: 6.3624302\ttotal: 1m 4s\tremaining: 46.2s\n",
      "581:\ttotal: 1m 4s\tremaining: 46.1s\n",
      "582:\ttotal: 1m 4s\tremaining: 46s\n",
      "583:\ttotal: 1m 4s\tremaining: 45.9s\n",
      "584:\ttotal: 1m 4s\tremaining: 45.8s\n",
      "585:\tlearn: 6.3622775\ttotal: 1m 4s\tremaining: 45.6s\n",
      "586:\ttotal: 1m 4s\tremaining: 45.5s\n",
      "587:\ttotal: 1m 4s\tremaining: 45.4s\n",
      "588:\ttotal: 1m 4s\tremaining: 45.3s\n",
      "589:\ttotal: 1m 5s\tremaining: 45.2s\n",
      "590:\tlearn: 6.3621242\ttotal: 1m 5s\tremaining: 45.1s\n",
      "591:\ttotal: 1m 5s\tremaining: 45s\n",
      "592:\ttotal: 1m 5s\tremaining: 44.9s\n",
      "593:\ttotal: 1m 5s\tremaining: 44.8s\n",
      "594:\ttotal: 1m 5s\tremaining: 44.7s\n",
      "595:\tlearn: 6.3619800\ttotal: 1m 5s\tremaining: 44.6s\n",
      "596:\ttotal: 1m 5s\tremaining: 44.5s\n",
      "597:\ttotal: 1m 6s\tremaining: 44.4s\n",
      "598:\ttotal: 1m 6s\tremaining: 44.3s\n",
      "599:\ttotal: 1m 6s\tremaining: 44.2s\n",
      "600:\tlearn: 6.3618353\ttotal: 1m 6s\tremaining: 44.1s\n",
      "601:\ttotal: 1m 6s\tremaining: 44s\n",
      "602:\ttotal: 1m 6s\tremaining: 43.9s\n",
      "603:\ttotal: 1m 6s\tremaining: 43.8s\n",
      "604:\ttotal: 1m 6s\tremaining: 43.6s\n",
      "605:\tlearn: 6.3616888\ttotal: 1m 6s\tremaining: 43.5s\n",
      "606:\ttotal: 1m 7s\tremaining: 43.4s\n",
      "607:\ttotal: 1m 7s\tremaining: 43.3s\n",
      "608:\ttotal: 1m 7s\tremaining: 43.2s\n",
      "609:\ttotal: 1m 7s\tremaining: 43.1s\n",
      "610:\tlearn: 6.3615321\ttotal: 1m 7s\tremaining: 43s\n",
      "611:\ttotal: 1m 7s\tremaining: 42.9s\n",
      "612:\ttotal: 1m 7s\tremaining: 42.8s\n",
      "613:\ttotal: 1m 7s\tremaining: 42.7s\n",
      "614:\ttotal: 1m 8s\tremaining: 42.6s\n",
      "615:\tlearn: 6.3613737\ttotal: 1m 8s\tremaining: 42.5s\n",
      "616:\ttotal: 1m 8s\tremaining: 42.4s\n",
      "617:\ttotal: 1m 8s\tremaining: 42.3s\n",
      "618:\ttotal: 1m 8s\tremaining: 42.2s\n",
      "619:\ttotal: 1m 8s\tremaining: 42.1s\n",
      "620:\tlearn: 6.3612357\ttotal: 1m 8s\tremaining: 42s\n",
      "621:\ttotal: 1m 8s\tremaining: 41.9s\n",
      "622:\ttotal: 1m 9s\tremaining: 41.8s\n",
      "623:\ttotal: 1m 9s\tremaining: 41.7s\n",
      "624:\ttotal: 1m 9s\tremaining: 41.6s\n",
      "625:\tlearn: 6.3611035\ttotal: 1m 9s\tremaining: 41.4s\n",
      "626:\ttotal: 1m 9s\tremaining: 41.3s\n",
      "627:\ttotal: 1m 9s\tremaining: 41.2s\n",
      "628:\ttotal: 1m 9s\tremaining: 41.1s\n",
      "629:\ttotal: 1m 9s\tremaining: 41s\n",
      "630:\tlearn: 6.3609667\ttotal: 1m 9s\tremaining: 40.9s\n",
      "631:\ttotal: 1m 10s\tremaining: 40.8s\n",
      "632:\ttotal: 1m 10s\tremaining: 40.7s\n",
      "633:\ttotal: 1m 10s\tremaining: 40.6s\n",
      "634:\ttotal: 1m 10s\tremaining: 40.5s\n",
      "635:\tlearn: 6.3608226\ttotal: 1m 10s\tremaining: 40.4s\n",
      "636:\ttotal: 1m 10s\tremaining: 40.3s\n",
      "637:\ttotal: 1m 10s\tremaining: 40.2s\n",
      "638:\ttotal: 1m 10s\tremaining: 40.1s\n",
      "639:\ttotal: 1m 11s\tremaining: 40s\n",
      "640:\tlearn: 6.3606761\ttotal: 1m 11s\tremaining: 39.9s\n",
      "641:\ttotal: 1m 11s\tremaining: 39.8s\n",
      "642:\ttotal: 1m 11s\tremaining: 39.7s\n",
      "643:\ttotal: 1m 11s\tremaining: 39.5s\n",
      "644:\ttotal: 1m 11s\tremaining: 39.4s\n",
      "645:\tlearn: 6.3605507\ttotal: 1m 11s\tremaining: 39.3s\n",
      "646:\ttotal: 1m 11s\tremaining: 39.2s\n",
      "647:\ttotal: 1m 12s\tremaining: 39.1s\n",
      "648:\ttotal: 1m 12s\tremaining: 39s\n",
      "649:\ttotal: 1m 12s\tremaining: 38.9s\n",
      "650:\tlearn: 6.3604145\ttotal: 1m 12s\tremaining: 38.8s\n",
      "651:\ttotal: 1m 12s\tremaining: 38.7s\n",
      "652:\ttotal: 1m 12s\tremaining: 38.6s\n",
      "653:\ttotal: 1m 12s\tremaining: 38.5s\n",
      "654:\ttotal: 1m 12s\tremaining: 38.4s\n",
      "655:\tlearn: 6.3602709\ttotal: 1m 12s\tremaining: 38.3s\n",
      "656:\ttotal: 1m 13s\tremaining: 38.2s\n",
      "657:\ttotal: 1m 13s\tremaining: 38.1s\n",
      "658:\ttotal: 1m 13s\tremaining: 38s\n",
      "659:\ttotal: 1m 13s\tremaining: 37.9s\n",
      "660:\tlearn: 6.3601524\ttotal: 1m 13s\tremaining: 37.7s\n",
      "661:\ttotal: 1m 13s\tremaining: 37.6s\n",
      "662:\ttotal: 1m 13s\tremaining: 37.5s\n",
      "663:\ttotal: 1m 13s\tremaining: 37.4s\n",
      "664:\ttotal: 1m 14s\tremaining: 37.3s\n",
      "665:\tlearn: 6.3600087\ttotal: 1m 14s\tremaining: 37.2s\n",
      "666:\ttotal: 1m 14s\tremaining: 37.1s\n",
      "667:\ttotal: 1m 14s\tremaining: 37s\n",
      "668:\ttotal: 1m 14s\tremaining: 36.9s\n",
      "669:\ttotal: 1m 14s\tremaining: 36.8s\n",
      "670:\tlearn: 6.3598885\ttotal: 1m 14s\tremaining: 36.7s\n",
      "671:\ttotal: 1m 14s\tremaining: 36.6s\n",
      "672:\ttotal: 1m 15s\tremaining: 36.5s\n",
      "673:\ttotal: 1m 15s\tremaining: 36.4s\n",
      "674:\ttotal: 1m 15s\tremaining: 36.3s\n",
      "675:\tlearn: 6.3597534\ttotal: 1m 15s\tremaining: 36.2s\n",
      "676:\ttotal: 1m 15s\tremaining: 36.1s\n",
      "677:\ttotal: 1m 15s\tremaining: 35.9s\n",
      "678:\ttotal: 1m 15s\tremaining: 35.8s\n",
      "679:\ttotal: 1m 15s\tremaining: 35.7s\n",
      "680:\tlearn: 6.3596263\ttotal: 1m 16s\tremaining: 35.6s\n",
      "681:\ttotal: 1m 16s\tremaining: 35.5s\n",
      "682:\ttotal: 1m 16s\tremaining: 35.4s\n",
      "683:\ttotal: 1m 16s\tremaining: 35.3s\n",
      "684:\ttotal: 1m 16s\tremaining: 35.2s\n",
      "685:\tlearn: 6.3594970\ttotal: 1m 16s\tremaining: 35.1s\n",
      "686:\ttotal: 1m 16s\tremaining: 35s\n",
      "687:\ttotal: 1m 16s\tremaining: 34.9s\n",
      "688:\ttotal: 1m 17s\tremaining: 34.8s\n",
      "689:\ttotal: 1m 17s\tremaining: 34.7s\n",
      "690:\tlearn: 6.3593762\ttotal: 1m 17s\tremaining: 34.6s\n",
      "691:\ttotal: 1m 17s\tremaining: 34.5s\n",
      "692:\ttotal: 1m 17s\tremaining: 34.3s\n",
      "693:\ttotal: 1m 17s\tremaining: 34.2s\n",
      "694:\ttotal: 1m 17s\tremaining: 34.1s\n",
      "695:\tlearn: 6.3592576\ttotal: 1m 17s\tremaining: 34s\n",
      "696:\ttotal: 1m 18s\tremaining: 33.9s\n",
      "697:\ttotal: 1m 18s\tremaining: 33.8s\n",
      "698:\ttotal: 1m 18s\tremaining: 33.7s\n",
      "699:\ttotal: 1m 18s\tremaining: 33.6s\n",
      "700:\tlearn: 6.3591254\ttotal: 1m 18s\tremaining: 33.5s\n",
      "701:\ttotal: 1m 18s\tremaining: 33.4s\n",
      "702:\ttotal: 1m 18s\tremaining: 33.3s\n",
      "703:\ttotal: 1m 18s\tremaining: 33.2s\n",
      "704:\ttotal: 1m 18s\tremaining: 33.1s\n",
      "705:\tlearn: 6.3590126\ttotal: 1m 19s\tremaining: 32.9s\n",
      "706:\ttotal: 1m 19s\tremaining: 32.8s\n",
      "707:\ttotal: 1m 19s\tremaining: 32.7s\n",
      "708:\ttotal: 1m 19s\tremaining: 32.6s\n",
      "709:\ttotal: 1m 19s\tremaining: 32.5s\n",
      "710:\tlearn: 6.3588991\ttotal: 1m 19s\tremaining: 32.4s\n",
      "711:\ttotal: 1m 19s\tremaining: 32.3s\n",
      "712:\ttotal: 1m 19s\tremaining: 32.2s\n",
      "713:\ttotal: 1m 20s\tremaining: 32.1s\n",
      "714:\ttotal: 1m 20s\tremaining: 32s\n",
      "715:\tlearn: 6.3587829\ttotal: 1m 20s\tremaining: 31.9s\n",
      "716:\ttotal: 1m 20s\tremaining: 31.7s\n",
      "717:\ttotal: 1m 20s\tremaining: 31.6s\n",
      "718:\ttotal: 1m 20s\tremaining: 31.5s\n",
      "719:\ttotal: 1m 20s\tremaining: 31.4s\n",
      "720:\tlearn: 6.3586535\ttotal: 1m 20s\tremaining: 31.3s\n",
      "721:\ttotal: 1m 21s\tremaining: 31.2s\n",
      "722:\ttotal: 1m 21s\tremaining: 31.1s\n",
      "723:\ttotal: 1m 21s\tremaining: 31s\n",
      "724:\ttotal: 1m 21s\tremaining: 30.9s\n",
      "725:\tlearn: 6.3585242\ttotal: 1m 21s\tremaining: 30.8s\n",
      "726:\ttotal: 1m 21s\tremaining: 30.6s\n",
      "727:\ttotal: 1m 21s\tremaining: 30.5s\n",
      "728:\ttotal: 1m 21s\tremaining: 30.4s\n",
      "729:\ttotal: 1m 21s\tremaining: 30.3s\n",
      "730:\tlearn: 6.3584068\ttotal: 1m 22s\tremaining: 30.2s\n",
      "731:\ttotal: 1m 22s\tremaining: 30.1s\n",
      "732:\ttotal: 1m 22s\tremaining: 30s\n",
      "733:\ttotal: 1m 22s\tremaining: 29.9s\n",
      "734:\ttotal: 1m 22s\tremaining: 29.8s\n",
      "735:\tlearn: 6.3582916\ttotal: 1m 22s\tremaining: 29.7s\n",
      "736:\ttotal: 1m 22s\tremaining: 29.6s\n",
      "737:\ttotal: 1m 22s\tremaining: 29.4s\n",
      "738:\ttotal: 1m 23s\tremaining: 29.3s\n",
      "739:\ttotal: 1m 23s\tremaining: 29.2s\n",
      "740:\tlearn: 6.3581720\ttotal: 1m 23s\tremaining: 29.1s\n",
      "741:\ttotal: 1m 23s\tremaining: 29s\n",
      "742:\ttotal: 1m 23s\tremaining: 28.9s\n",
      "743:\ttotal: 1m 23s\tremaining: 28.8s\n",
      "744:\ttotal: 1m 23s\tremaining: 28.7s\n",
      "745:\tlearn: 6.3580665\ttotal: 1m 23s\tremaining: 28.6s\n",
      "746:\ttotal: 1m 23s\tremaining: 28.4s\n",
      "747:\ttotal: 1m 24s\tremaining: 28.3s\n",
      "748:\ttotal: 1m 24s\tremaining: 28.2s\n",
      "749:\ttotal: 1m 24s\tremaining: 28.1s\n",
      "750:\tlearn: 6.3579617\ttotal: 1m 24s\tremaining: 28s\n",
      "751:\ttotal: 1m 24s\tremaining: 27.9s\n",
      "752:\ttotal: 1m 24s\tremaining: 27.8s\n",
      "753:\ttotal: 1m 24s\tremaining: 27.7s\n",
      "754:\ttotal: 1m 24s\tremaining: 27.6s\n",
      "755:\tlearn: 6.3578414\ttotal: 1m 25s\tremaining: 27.5s\n",
      "756:\ttotal: 1m 25s\tremaining: 27.4s\n",
      "757:\ttotal: 1m 25s\tremaining: 27.2s\n",
      "758:\ttotal: 1m 25s\tremaining: 27.1s\n",
      "759:\ttotal: 1m 25s\tremaining: 27s\n",
      "760:\tlearn: 6.3577303\ttotal: 1m 25s\tremaining: 26.9s\n",
      "761:\ttotal: 1m 25s\tremaining: 26.8s\n",
      "762:\ttotal: 1m 25s\tremaining: 26.7s\n",
      "763:\ttotal: 1m 26s\tremaining: 26.6s\n",
      "764:\ttotal: 1m 26s\tremaining: 26.5s\n",
      "765:\tlearn: 6.3576254\ttotal: 1m 26s\tremaining: 26.4s\n",
      "766:\ttotal: 1m 26s\tremaining: 26.3s\n",
      "767:\ttotal: 1m 26s\tremaining: 26.1s\n",
      "768:\ttotal: 1m 26s\tremaining: 26s\n",
      "769:\ttotal: 1m 26s\tremaining: 25.9s\n",
      "770:\tlearn: 6.3575120\ttotal: 1m 26s\tremaining: 25.8s\n",
      "771:\ttotal: 1m 27s\tremaining: 25.7s\n",
      "772:\ttotal: 1m 27s\tremaining: 25.6s\n",
      "773:\ttotal: 1m 27s\tremaining: 25.5s\n",
      "774:\ttotal: 1m 27s\tremaining: 25.4s\n",
      "775:\tlearn: 6.3574117\ttotal: 1m 27s\tremaining: 25.3s\n",
      "776:\ttotal: 1m 27s\tremaining: 25.1s\n",
      "777:\ttotal: 1m 27s\tremaining: 25s\n",
      "778:\ttotal: 1m 27s\tremaining: 24.9s\n",
      "779:\ttotal: 1m 27s\tremaining: 24.8s\n",
      "780:\tlearn: 6.3572983\ttotal: 1m 28s\tremaining: 24.7s\n",
      "781:\ttotal: 1m 28s\tremaining: 24.6s\n",
      "782:\ttotal: 1m 28s\tremaining: 24.5s\n",
      "783:\ttotal: 1m 28s\tremaining: 24.4s\n",
      "784:\ttotal: 1m 28s\tremaining: 24.3s\n",
      "785:\tlearn: 6.3571758\ttotal: 1m 28s\tremaining: 24.2s\n",
      "786:\ttotal: 1m 28s\tremaining: 24s\n",
      "787:\ttotal: 1m 28s\tremaining: 23.9s\n",
      "788:\ttotal: 1m 29s\tremaining: 23.8s\n",
      "789:\ttotal: 1m 29s\tremaining: 23.7s\n",
      "790:\tlearn: 6.3570777\ttotal: 1m 29s\tremaining: 23.6s\n",
      "791:\ttotal: 1m 29s\tremaining: 23.5s\n",
      "792:\ttotal: 1m 29s\tremaining: 23.4s\n",
      "793:\ttotal: 1m 29s\tremaining: 23.3s\n",
      "794:\ttotal: 1m 29s\tremaining: 23.2s\n",
      "795:\tlearn: 6.3569740\ttotal: 1m 29s\tremaining: 23s\n",
      "796:\ttotal: 1m 30s\tremaining: 22.9s\n",
      "797:\ttotal: 1m 30s\tremaining: 22.8s\n",
      "798:\ttotal: 1m 30s\tremaining: 22.7s\n",
      "799:\ttotal: 1m 30s\tremaining: 22.6s\n",
      "800:\tlearn: 6.3568675\ttotal: 1m 30s\tremaining: 22.5s\n",
      "801:\ttotal: 1m 30s\tremaining: 22.4s\n",
      "802:\ttotal: 1m 30s\tremaining: 22.3s\n",
      "803:\ttotal: 1m 30s\tremaining: 22.2s\n",
      "804:\ttotal: 1m 31s\tremaining: 22s\n",
      "805:\tlearn: 6.3567535\ttotal: 1m 31s\tremaining: 21.9s\n",
      "806:\ttotal: 1m 31s\tremaining: 21.8s\n",
      "807:\ttotal: 1m 31s\tremaining: 21.7s\n",
      "808:\ttotal: 1m 31s\tremaining: 21.6s\n",
      "809:\ttotal: 1m 31s\tremaining: 21.5s\n",
      "810:\tlearn: 6.3566543\ttotal: 1m 31s\tremaining: 21.4s\n",
      "811:\ttotal: 1m 31s\tremaining: 21.3s\n",
      "812:\ttotal: 1m 31s\tremaining: 21.2s\n",
      "813:\ttotal: 1m 32s\tremaining: 21s\n",
      "814:\ttotal: 1m 32s\tremaining: 20.9s\n",
      "815:\tlearn: 6.3565517\ttotal: 1m 32s\tremaining: 20.8s\n",
      "816:\ttotal: 1m 32s\tremaining: 20.7s\n",
      "817:\ttotal: 1m 32s\tremaining: 20.6s\n",
      "818:\ttotal: 1m 32s\tremaining: 20.5s\n",
      "819:\ttotal: 1m 32s\tremaining: 20.4s\n",
      "820:\tlearn: 6.3564434\ttotal: 1m 32s\tremaining: 20.3s\n",
      "821:\ttotal: 1m 33s\tremaining: 20.2s\n",
      "822:\ttotal: 1m 33s\tremaining: 20s\n",
      "823:\ttotal: 1m 33s\tremaining: 19.9s\n",
      "824:\ttotal: 1m 33s\tremaining: 19.8s\n",
      "825:\tlearn: 6.3563329\ttotal: 1m 33s\tremaining: 19.7s\n",
      "826:\ttotal: 1m 33s\tremaining: 19.6s\n",
      "827:\ttotal: 1m 33s\tremaining: 19.5s\n",
      "828:\ttotal: 1m 33s\tremaining: 19.4s\n",
      "829:\ttotal: 1m 34s\tremaining: 19.3s\n",
      "830:\tlearn: 6.3562297\ttotal: 1m 34s\tremaining: 19.1s\n",
      "831:\ttotal: 1m 34s\tremaining: 19s\n",
      "832:\ttotal: 1m 34s\tremaining: 18.9s\n",
      "833:\ttotal: 1m 34s\tremaining: 18.8s\n",
      "834:\ttotal: 1m 34s\tremaining: 18.7s\n",
      "835:\tlearn: 6.3561340\ttotal: 1m 34s\tremaining: 18.6s\n",
      "836:\ttotal: 1m 34s\tremaining: 18.5s\n",
      "837:\ttotal: 1m 35s\tremaining: 18.4s\n",
      "838:\ttotal: 1m 35s\tremaining: 18.3s\n",
      "839:\ttotal: 1m 35s\tremaining: 18.1s\n",
      "840:\tlearn: 6.3560331\ttotal: 1m 35s\tremaining: 18s\n",
      "841:\ttotal: 1m 35s\tremaining: 17.9s\n",
      "842:\ttotal: 1m 35s\tremaining: 17.8s\n",
      "843:\ttotal: 1m 35s\tremaining: 17.7s\n",
      "844:\ttotal: 1m 35s\tremaining: 17.6s\n",
      "845:\tlearn: 6.3559265\ttotal: 1m 35s\tremaining: 17.5s\n",
      "846:\ttotal: 1m 36s\tremaining: 17.4s\n",
      "847:\ttotal: 1m 36s\tremaining: 17.2s\n",
      "848:\ttotal: 1m 36s\tremaining: 17.1s\n",
      "849:\ttotal: 1m 36s\tremaining: 17s\n",
      "850:\tlearn: 6.3558319\ttotal: 1m 36s\tremaining: 16.9s\n",
      "851:\ttotal: 1m 36s\tremaining: 16.8s\n",
      "852:\ttotal: 1m 36s\tremaining: 16.7s\n",
      "853:\ttotal: 1m 36s\tremaining: 16.6s\n",
      "854:\ttotal: 1m 37s\tremaining: 16.5s\n",
      "855:\tlearn: 6.3557391\ttotal: 1m 37s\tremaining: 16.4s\n",
      "856:\ttotal: 1m 37s\tremaining: 16.2s\n",
      "857:\ttotal: 1m 37s\tremaining: 16.1s\n",
      "858:\ttotal: 1m 37s\tremaining: 16s\n",
      "859:\ttotal: 1m 37s\tremaining: 15.9s\n",
      "860:\tlearn: 6.3556427\ttotal: 1m 37s\tremaining: 15.8s\n",
      "861:\ttotal: 1m 37s\tremaining: 15.7s\n",
      "862:\ttotal: 1m 38s\tremaining: 15.6s\n",
      "863:\ttotal: 1m 38s\tremaining: 15.5s\n",
      "864:\ttotal: 1m 38s\tremaining: 15.3s\n",
      "865:\tlearn: 6.3555493\ttotal: 1m 38s\tremaining: 15.2s\n",
      "866:\ttotal: 1m 38s\tremaining: 15.1s\n",
      "867:\ttotal: 1m 38s\tremaining: 15s\n",
      "868:\ttotal: 1m 38s\tremaining: 14.9s\n",
      "869:\ttotal: 1m 38s\tremaining: 14.8s\n",
      "870:\tlearn: 6.3554438\ttotal: 1m 39s\tremaining: 14.7s\n",
      "871:\ttotal: 1m 39s\tremaining: 14.6s\n",
      "872:\ttotal: 1m 39s\tremaining: 14.4s\n",
      "873:\ttotal: 1m 39s\tremaining: 14.3s\n",
      "874:\ttotal: 1m 39s\tremaining: 14.2s\n",
      "875:\tlearn: 6.3553367\ttotal: 1m 39s\tremaining: 14.1s\n",
      "876:\ttotal: 1m 39s\tremaining: 14s\n",
      "877:\ttotal: 1m 39s\tremaining: 13.9s\n",
      "878:\ttotal: 1m 40s\tremaining: 13.8s\n",
      "879:\ttotal: 1m 40s\tremaining: 13.7s\n",
      "880:\tlearn: 6.3552347\ttotal: 1m 40s\tremaining: 13.5s\n",
      "881:\ttotal: 1m 40s\tremaining: 13.4s\n",
      "882:\ttotal: 1m 40s\tremaining: 13.3s\n",
      "883:\ttotal: 1m 40s\tremaining: 13.2s\n",
      "884:\ttotal: 1m 40s\tremaining: 13.1s\n",
      "885:\tlearn: 6.3551116\ttotal: 1m 40s\tremaining: 13s\n",
      "886:\ttotal: 1m 41s\tremaining: 12.9s\n",
      "887:\ttotal: 1m 41s\tremaining: 12.8s\n",
      "888:\ttotal: 1m 41s\tremaining: 12.6s\n",
      "889:\ttotal: 1m 41s\tremaining: 12.5s\n",
      "890:\tlearn: 6.3550301\ttotal: 1m 41s\tremaining: 12.4s\n",
      "891:\ttotal: 1m 41s\tremaining: 12.3s\n",
      "892:\ttotal: 1m 41s\tremaining: 12.2s\n",
      "893:\ttotal: 1m 41s\tremaining: 12.1s\n",
      "894:\ttotal: 1m 42s\tremaining: 12s\n",
      "895:\tlearn: 6.3549315\ttotal: 1m 42s\tremaining: 11.9s\n",
      "896:\ttotal: 1m 42s\tremaining: 11.7s\n",
      "897:\ttotal: 1m 42s\tremaining: 11.6s\n",
      "898:\ttotal: 1m 42s\tremaining: 11.5s\n",
      "899:\ttotal: 1m 42s\tremaining: 11.4s\n",
      "900:\tlearn: 6.3548284\ttotal: 1m 42s\tremaining: 11.3s\n",
      "901:\ttotal: 1m 42s\tremaining: 11.2s\n",
      "902:\ttotal: 1m 42s\tremaining: 11.1s\n",
      "903:\ttotal: 1m 43s\tremaining: 10.9s\n",
      "904:\ttotal: 1m 43s\tremaining: 10.8s\n",
      "905:\tlearn: 6.3547349\ttotal: 1m 43s\tremaining: 10.7s\n",
      "906:\ttotal: 1m 43s\tremaining: 10.6s\n",
      "907:\ttotal: 1m 43s\tremaining: 10.5s\n",
      "908:\ttotal: 1m 43s\tremaining: 10.4s\n",
      "909:\ttotal: 1m 43s\tremaining: 10.3s\n",
      "910:\tlearn: 6.3546306\ttotal: 1m 43s\tremaining: 10.2s\n",
      "911:\ttotal: 1m 44s\tremaining: 10s\n",
      "912:\ttotal: 1m 44s\tremaining: 9.93s\n",
      "913:\ttotal: 1m 44s\tremaining: 9.82s\n",
      "914:\ttotal: 1m 44s\tremaining: 9.7s\n",
      "915:\tlearn: 6.3545423\ttotal: 1m 44s\tremaining: 9.59s\n",
      "916:\ttotal: 1m 44s\tremaining: 9.48s\n",
      "917:\ttotal: 1m 44s\tremaining: 9.37s\n",
      "918:\ttotal: 1m 44s\tremaining: 9.25s\n",
      "919:\ttotal: 1m 45s\tremaining: 9.14s\n",
      "920:\tlearn: 6.3544323\ttotal: 1m 45s\tremaining: 9.03s\n",
      "921:\ttotal: 1m 45s\tremaining: 8.91s\n",
      "922:\ttotal: 1m 45s\tremaining: 8.8s\n",
      "923:\ttotal: 1m 45s\tremaining: 8.68s\n",
      "924:\ttotal: 1m 45s\tremaining: 8.57s\n",
      "925:\tlearn: 6.3543548\ttotal: 1m 45s\tremaining: 8.46s\n",
      "926:\ttotal: 1m 45s\tremaining: 8.34s\n",
      "927:\ttotal: 1m 46s\tremaining: 8.23s\n",
      "928:\ttotal: 1m 46s\tremaining: 8.12s\n",
      "929:\ttotal: 1m 46s\tremaining: 8s\n",
      "930:\tlearn: 6.3542750\ttotal: 1m 46s\tremaining: 7.89s\n",
      "931:\ttotal: 1m 46s\tremaining: 7.78s\n",
      "932:\ttotal: 1m 46s\tremaining: 7.66s\n",
      "933:\ttotal: 1m 46s\tremaining: 7.55s\n",
      "934:\ttotal: 1m 46s\tremaining: 7.44s\n",
      "935:\tlearn: 6.3541872\ttotal: 1m 47s\tremaining: 7.32s\n",
      "936:\ttotal: 1m 47s\tremaining: 7.21s\n",
      "937:\ttotal: 1m 47s\tremaining: 7.09s\n",
      "938:\ttotal: 1m 47s\tremaining: 6.98s\n",
      "939:\ttotal: 1m 47s\tremaining: 6.87s\n",
      "940:\tlearn: 6.3541063\ttotal: 1m 47s\tremaining: 6.75s\n",
      "941:\ttotal: 1m 47s\tremaining: 6.64s\n",
      "942:\ttotal: 1m 47s\tremaining: 6.53s\n",
      "943:\ttotal: 1m 48s\tremaining: 6.41s\n",
      "944:\ttotal: 1m 48s\tremaining: 6.3s\n",
      "945:\tlearn: 6.3540242\ttotal: 1m 48s\tremaining: 6.18s\n",
      "946:\ttotal: 1m 48s\tremaining: 6.07s\n",
      "947:\ttotal: 1m 48s\tremaining: 5.96s\n",
      "948:\ttotal: 1m 48s\tremaining: 5.84s\n",
      "949:\ttotal: 1m 48s\tremaining: 5.73s\n",
      "950:\tlearn: 6.3539085\ttotal: 1m 48s\tremaining: 5.61s\n",
      "951:\ttotal: 1m 49s\tremaining: 5.5s\n",
      "952:\ttotal: 1m 49s\tremaining: 5.38s\n",
      "953:\ttotal: 1m 49s\tremaining: 5.27s\n",
      "954:\ttotal: 1m 49s\tremaining: 5.16s\n",
      "955:\tlearn: 6.3538310\ttotal: 1m 49s\tremaining: 5.04s\n",
      "956:\ttotal: 1m 49s\tremaining: 4.93s\n",
      "957:\ttotal: 1m 49s\tremaining: 4.81s\n",
      "958:\ttotal: 1m 49s\tremaining: 4.7s\n",
      "959:\ttotal: 1m 50s\tremaining: 4.59s\n",
      "960:\tlearn: 6.3537359\ttotal: 1m 50s\tremaining: 4.47s\n",
      "961:\ttotal: 1m 50s\tremaining: 4.36s\n",
      "962:\ttotal: 1m 50s\tremaining: 4.25s\n",
      "963:\ttotal: 1m 50s\tremaining: 4.13s\n",
      "964:\ttotal: 1m 50s\tremaining: 4.02s\n",
      "965:\tlearn: 6.3536373\ttotal: 1m 50s\tremaining: 3.9s\n",
      "966:\ttotal: 1m 51s\tremaining: 3.79s\n",
      "967:\ttotal: 1m 51s\tremaining: 3.67s\n",
      "968:\ttotal: 1m 51s\tremaining: 3.56s\n",
      "969:\ttotal: 1m 51s\tremaining: 3.44s\n",
      "970:\tlearn: 6.3535580\ttotal: 1m 51s\tremaining: 3.33s\n",
      "971:\ttotal: 1m 51s\tremaining: 3.21s\n",
      "972:\ttotal: 1m 51s\tremaining: 3.1s\n",
      "973:\ttotal: 1m 51s\tremaining: 2.99s\n",
      "974:\ttotal: 1m 51s\tremaining: 2.87s\n",
      "975:\tlearn: 6.3534589\ttotal: 1m 52s\tremaining: 2.76s\n",
      "976:\ttotal: 1m 52s\tremaining: 2.64s\n",
      "977:\ttotal: 1m 52s\tremaining: 2.53s\n",
      "978:\ttotal: 1m 52s\tremaining: 2.41s\n",
      "979:\ttotal: 1m 52s\tremaining: 2.3s\n",
      "980:\tlearn: 6.3533483\ttotal: 1m 52s\tremaining: 2.18s\n",
      "981:\ttotal: 1m 52s\tremaining: 2.07s\n",
      "982:\ttotal: 1m 53s\tremaining: 1.95s\n",
      "983:\ttotal: 1m 53s\tremaining: 1.84s\n",
      "984:\ttotal: 1m 53s\tremaining: 1.73s\n",
      "985:\tlearn: 6.3532606\ttotal: 1m 53s\tremaining: 1.61s\n",
      "986:\ttotal: 1m 53s\tremaining: 1.5s\n",
      "987:\ttotal: 1m 53s\tremaining: 1.38s\n",
      "988:\ttotal: 1m 53s\tremaining: 1.27s\n",
      "989:\ttotal: 1m 54s\tremaining: 1.15s\n",
      "990:\tlearn: 6.3531660\ttotal: 1m 54s\tremaining: 1.04s\n",
      "991:\ttotal: 1m 54s\tremaining: 921ms\n",
      "992:\ttotal: 1m 54s\tremaining: 806ms\n",
      "993:\ttotal: 1m 54s\tremaining: 691ms\n",
      "994:\ttotal: 1m 54s\tremaining: 576ms\n",
      "995:\tlearn: 6.3530731\ttotal: 1m 54s\tremaining: 461ms\n",
      "996:\ttotal: 1m 54s\tremaining: 346ms\n",
      "997:\ttotal: 1m 55s\tremaining: 231ms\n",
      "998:\ttotal: 1m 55s\tremaining: 115ms\n",
      "999:\tlearn: 6.3529961\ttotal: 1m 55s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cb_params = {\"loss_function\": \"MAE\", \"random_state\": 42, \"verbose\": 1, \"task_type\": \"GPU\", \"thread_count\": -1, \"cat_features\": ['stock_id', 'imbalance_buy_sell_flag'] }\n",
    "mae_cb, _ = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc4012",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Naive Features performance with our models\n",
    "Now that we have trained and predicted each algorithm, we can finally evaluate their performance to give us a baseline to test on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60adb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: 5.7167\n",
      "Random Forest: 6.4126\n",
      "LightGBM: 6.3010\n",
      "XGBoost: 6.2542\n",
      "CatBoost: 6.3464\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Regression: {mae_lr:.4f}\")\n",
    "print(f\"Random Forest: {mae_rf:.4f}\")\n",
    "print(f\"LightGBM: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost: {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488942ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = [\n",
    "    {'Model': 'Linear Regression', 'Feature Set': 'Baseline', 'MAE': mae_lr},\n",
    "    {'Model': 'Random Forest', 'Feature Set': 'Baseline', 'MAE': mae_rf},\n",
    "    {'Model': 'LightGBM', 'Feature Set': 'Baseline', 'MAE': mae_lgbm},\n",
    "    {'Model': 'XGBoost', 'Feature Set': 'Baseline', 'MAE': mae_xgb},\n",
    "    {'Model': 'CatBoost', 'Feature Set': 'Baseline', 'MAE': mae_cb}]\n",
    "\n",
    "# Brandon in the previous secetion its not a good thing that dropping near price and  far price gave it good results\n",
    "# This is because the df is half the size and LR cant handle NaN features. Can you add a note on this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877ae30",
   "metadata": {},
   "source": [
    "Interestingly, we found dropping near price and far price (features that are half null), gave Linear Regression the best score. Among the models that trained on the whole training set, LightGBM had the best MAE. This is a good starting point to see how well we can improve the performance of the last three models. To save time and avoid redundancy, we will not pursue Linear Regression or Random Forest further.\n",
    "\n",
    "## Original Feature Engineering\n",
    "\n",
    "In this section, we will think of and create our own features to use for training the finalist models (LightGBM, XGBoost, and CatBoost). \n",
    "\n",
    "These our are originally made features: \n",
    "\n",
    "- `wap_volatility`: Measures how much the weighted average price (WAP) swings over 5 time periods for each stock. \n",
    "\n",
    "- `bid_ask_spread`: Difference between the selling price (ask) and buying price (bid).\n",
    "\n",
    "- `bid_ask_volatility`: Tracks how much the bid-ask spread varies over 5 time periods for each stock.\n",
    "\n",
    "- `wap_momentum`: Shows the percentage change in WAP over 3 time periods for each stock.\n",
    "\n",
    "- `price_momentum`: Shows the percentage change in reference price over 3 time periods for each stock.\n",
    "\n",
    "- `log_imbalance_size`: Shrinks large imbalance size values (buy/sell order differences) using a log function.\n",
    "\n",
    "- `log_matched_size`: Shrinks large matched order size values using a log function.\n",
    "\n",
    "- `log_bid_size`: Shrinks large bid order size values using a log function.\n",
    "\n",
    "- `log_ask_size`: Shrinks large ask order size values using a log function.\n",
    "\n",
    "- `bucket_price_interaction`: Multiplies time (seconds in bucket) by reference price to capture their combined effect.\n",
    "\n",
    "- `bucket_imbalance_interaction`: Multiplies time (seconds in bucket) by imbalance size to capture their combined effect.\n",
    "\n",
    "- `wap_to_ref_price`: Divides WAP by reference price to show their relative difference.\n",
    "\n",
    "- `bid_to_ask_price`: Divides bid price by ask price to show their relative difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7602801",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def process_data_with_feature_engineering(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    # Drop rows where 'target' is null, as these cannot be used for training\n",
    "    data.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # Volatility features\n",
    "    data['wap_volatility'] = data.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "    data['bid_ask_spread'] = data['ask_price'] - data['bid_price']\n",
    "    data['bid_ask_volatility'] = data.groupby('stock_id')['bid_ask_spread'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # Momentum features\n",
    "    data['wap_momentum'] = data.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "    data['price_momentum'] = data.groupby('stock_id')['reference_price'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "\n",
    "    # Log transformations\n",
    "    size_cols = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size']\n",
    "    for col in size_cols:\n",
    "        data[f'log_{col}'] = np.log1p(data[col].clip(lower=0))\n",
    "\n",
    "    # Time-based interactions\n",
    "    data['bucket_price_interaction'] = data['seconds_in_bucket'] * data['reference_price']\n",
    "    data['bucket_imbalance_interaction'] = data['seconds_in_bucket'] * data['imbalance_size']\n",
    "\n",
    "    # Relative price features\n",
    "    data['wap_to_ref_price'] = data['wap'] / (data['reference_price'] + 1e-6)\n",
    "    data['bid_to_ask_price'] = data['bid_price'] / (data['ask_price'] + 1e-6)\n",
    "\n",
    "    # Handle NaN and inf after feature creation\n",
    "    # Identify columns that are not identifiers or the target\n",
    "    cols_to_process = [col for col in data.columns if col not in ['stock_id', 'date_id', 'target', 'time_id', 'row_id']]\n",
    "    data[cols_to_process] = data[cols_to_process].replace([np.inf, -np.inf], np.nan) # Replace inf with NaN first\n",
    "    data[cols_to_process] = data[cols_to_process].fillna(data[cols_to_process].median()) # Fill remaining NaNs with median\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = data.drop(\"target\", axis=1)\n",
    "    y = data[\"target\"]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109832e3",
   "metadata": {},
   "source": [
    "Now we can redo the naive training but with our original features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cee718f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3546189/1296277648.py:7: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
      "/tmp/ipykernel_3546189/1296277648.py:16: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n",
      "/tmp/ipykernel_3546189/1296277648.py:19: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6631\n",
      "[LightGBM] [Info] Number of data points in the train set: 3509387, number of used features: 28\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA TITAN RTX, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 28 dense feature groups (93.71 MB) transferred to GPU in 0.112237 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.044118\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l1: 6.27243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% GPU memory available for training. Free: 13312.5 Total: 24212.375\n",
      "Default metric period is 5 because MAE is/are not implemented for GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X, y = process_data_with_feature_engineering(df.copy())\n",
    "\n",
    "# Split the data into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# LightGBM\n",
    "lgbm_params = {\n",
    "    'metric': 'mae',\n",
    "    'device': 'gpu',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "mae_lgbm, lgbm_model = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)\n",
    "\n",
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'objective': 'reg:absoluteerror',\n",
    "    'eval_metric': 'mae',\n",
    "    'device': \"cuda\",\n",
    "    'random_state': 42\n",
    "}\n",
    "mae_xgb, xgb_model = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)\n",
    "\n",
    "# CatBoost\n",
    "cb_params = {\"loss_function\": \"MAE\", \"random_state\": 42, \"verbose\": 0, \"task_type\": \"GPU\", \"thread_count\": -1, \"cat_features\": ['stock_id', 'imbalance_buy_sell_flag']} \n",
    "mae_cb, cb_model = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAE: 6.2233\n",
      "XGBoost MAE: 6.2289\n",
      "CatBoost MAE: 6.3187\n"
     ]
    }
   ],
   "source": [
    "print(f\"LightGBM MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost MAE: {mae_cb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append(\n",
    "    {'Model': 'LightGBM', 'Feature Set': 'Baseline', 'MAE' : mae_lgbm},\n",
    "    {'Model': 'XGBoost', 'Feature Set': 'Baseline', 'MAE' : mae_xgb},\n",
    "    {'Model': 'CatBoost', 'Feature Set': 'Baseline', 'MAE' : mae_cb},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing some minor improvements, but not enough to justify that our features are that helpful. Let's use the post processing trick that was used by the 1st place winner, and see how those results compare:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c80bee",
   "metadata": {},
   "source": [
    "We applied the post-processing trick used by the Kaggle first-place solution, which corrects each models predictions by subtracting the weighted average prediction for each time_id. The weights are based on a predefined stock_weights dictionary reflecting the relative importance or liquidity of each stock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stock weights (Given by Kaggle) (Attila, Cooper)\n",
    "weight_df = pd.DataFrame()\n",
    "weight_df['stock_id'] = list(range(200))\n",
    "weight_df['weight'] = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,]\n",
    "stock_weights = dict(zip(weight_df['stock_id'], weight_df['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LightGBM post-processing ---\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "LightGBM Adjusted MAE: 6.2221\n",
      "\n",
      "--- XGBoost post-processing ---\n",
      "XGBoost Adjusted MAE: 6.2313\n",
      "\n",
      "--- CatBoost post-processing ---\n",
      "CatBoost Adjusted MAE: 6.3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brandon Leong - improved and cleaned code to universal helper function for post processing\n",
    "\n",
    "def apply_postprocessing(model, X_val, y_val, stock_weights, model_name=\"Model\"):\n",
    "    preds_df = pd.DataFrame({\n",
    "        'prediction': model.predict(X_val),\n",
    "        'stock_id': X_val['stock_id'],\n",
    "        'time_id': X_val['time_id']\n",
    "    })\n",
    "    preds_df['stock_weights'] = preds_df['stock_id'].map(stock_weights)\n",
    "    preds_df['weighted_sum'] = preds_df['prediction'] * preds_df['stock_weights']\n",
    "    \n",
    "    weighted_avg = preds_df.groupby('time_id')['weighted_sum'].transform('sum') / preds_df.groupby('time_id')['stock_weights'].transform('sum')\n",
    "    preds_adjusted = preds_df['prediction'] - weighted_avg\n",
    "    \n",
    "    mae_adjusted = mean_absolute_error(y_val, preds_adjusted)\n",
    "    print(f\"{model_name} Adjusted MAE: {mae_adjusted:.4f}\\n\")\n",
    "    return preds_adjusted, mae_adjusted\n",
    "\n",
    "print(\"--- LightGBM post-processing ---\")\n",
    "y_pred_lgbm_adjusted, mae_lgbm_adjusted = apply_postprocessing(lgbm_model, X_val, y_val, stock_weights, \"LightGBM\")\n",
    "\n",
    "print(\"--- XGBoost post-processing ---\")\n",
    "y_pred_xgb_adjusted, mae_xgbmae_xgb_adjustedy_postprocessing(xgb_model, X_val, y_val, stock_weights, \"XGBoost\")\n",
    "\n",
    "print(\"--- CatBoost post-processing ---\")\n",
    "y_pred_cb_adjusted, mae_cb_adjusted = apply_postprocessing(cb_model, X_val, y_val, stock_weights, \"CatBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0146cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append(\n",
    "    {'Model': 'LightGBM', 'Feature Set': 'Post Processing', 'MAE' : mae_lgbm_adjusted},\n",
    "    {'Model': 'XGBoost', 'Feature Set': 'Post Processing', 'MAE' : mae_xgb_adjusted},\n",
    "    {'Model': 'CatBoost', 'Feature Set': 'Post Processing', 'MAE' : mae_cb_adjusted},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it was helpful to post process only for LightGBM and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d34885",
   "metadata": {},
   "source": [
    "## Additional Feature Ideas Inspired by Kaggle Leaderboard\n",
    "\n",
    "This is the code for some features we created that we got the ideas from on the kaggle leaderboard. \n",
    "\n",
    "- `seconds_in_bucket_group`: Splits time into three groups: 0 (0-299 seconds), 1 (300-479 seconds), 2 (480+ seconds).  \n",
    "\n",
    "- `bid_ask_spread`: Difference between ask (sell) and bid (buy) prices.\n",
    "\n",
    "- `imbalance_ratio`: Divides imbalance size (buy/sell order difference) by matched size (executed orders).\n",
    "\n",
    "- `mid_price`: Average of ask and bid prices.\n",
    "\n",
    "- `time_in_auction`: Normalizes time (seconds in bucket) by dividing by 540 to scale between 0 and 1.\n",
    "\n",
    "We also combined these with our original features into a combined function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def _apply_non_leaderboard_features(data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Volatility features\n",
    "    data_df['wap_volatility'] = data_df.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "    data_df['bid_ask_spread'] = data_df['ask_price'] - data_df['bid_price']\n",
    "    data_df['bid_ask_volatility'] = data_df.groupby('stock_id')['bid_ask_spread'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # Momentum features\n",
    "    data_df['wap_momentum'] = data_df.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "    data_df['price_momentum'] = data_df.groupby('stock_id')['reference_price'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "\n",
    "    # Log transformations\n",
    "    size_cols = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size']\n",
    "    for col in size_cols:\n",
    "        data_df[f'log_{col}'] = np.log1p(data_df[col].clip(lower=0))\n",
    "\n",
    "    # Time-based interactions\n",
    "    data_df['bucket_price_interaction'] = data_df['seconds_in_bucket'] * data_df['reference_price']\n",
    "    data_df['bucket_imbalance_interaction'] = data_df['seconds_in_bucket'] * data_df['imbalance_size']\n",
    "\n",
    "    # Relative price features\n",
    "    data_df['wap_to_ref_price'] = data_df['wap'] / (data_df['reference_price'] + 1e-6)\n",
    "    data_df['bid_to_ask_price'] = data_df['bid_price'] / (data_df['ask_price'] + 1e-6)\n",
    "\n",
    "    # Handle NaN and inf for all newly created numerical columns\n",
    "    # Exclude identifier/target columns as they are handled in the main processing functions\n",
    "    new_cols_added = [col for col in data_df.columns if col not in ['stock_id', 'date_id', 'target', 'time_id', 'row_id']]\n",
    "    # Filter to only numerical columns that might contain NaN/inf\n",
    "    numerical_cols_to_fill = data_df[new_cols_added].select_dtypes(include=np.number).columns\n",
    "    data_df[numerical_cols_to_fill] = data_df[numerical_cols_to_fill].replace([np.inf, -np.inf], np.nan)\n",
    "    data_df[numerical_cols_to_fill] = data_df[numerical_cols_to_fill].fillna(data_df[numerical_cols_to_fill].median())\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def _apply_leaderboard_features(data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Handle NaN and infinities in specific input columns first as per original\n",
    "    input_cols = ['imbalance_size', 'matched_size', 'ask_price', 'bid_price', 'wap', 'reference_price']\n",
    "    for col in input_cols:\n",
    "        data_df[col] = data_df[col].replace([np.inf, -np.inf], np.nan).fillna(data_df[col].median())\n",
    "\n",
    "    # 1st Place: Seconds in bucket group\n",
    "    data_df['seconds_in_bucket_group'] = np.where(data_df['seconds_in_bucket'] < 300, 0,\n",
    "                                              np.where(data_df['seconds_in_bucket'] < 480, 1, 2))\n",
    "\n",
    "    # 9th Place: Basic features\n",
    "    data_df['bid_ask_spread'] = data_df['ask_price'] - data_df['bid_price']\n",
    "    data_df['imbalance_ratio'] = data_df['imbalance_size'] / (data_df['matched_size'] + 1e-6)\n",
    "\n",
    "    # 14th Place: Mid price\n",
    "    data_df['mid_price'] = (data_df['ask_price'] + data_df['bid_price']) / 2\n",
    "\n",
    "    # Time in auction\n",
    "    data_df['time_in_auction'] = data_df['seconds_in_bucket'] / 540\n",
    "\n",
    "    # Handle NaN and inf for newly created columns\n",
    "    new_cols = ['seconds_in_bucket_group', 'bid_ask_spread', 'imbalance_ratio', 'mid_price', 'time_in_auction']\n",
    "    data_df[new_cols] = data_df[new_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    data_df[new_cols] = data_df[new_cols].fillna(data_df[new_cols].median())\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# Main data processing functions\n",
    "def process_data_non_leaderboard(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df_copy = data.copy()\n",
    "    # Drop rows where 'target' is null as these cannot be used for training\n",
    "    df_copy.dropna(subset=[\"target\"], inplace=True)\n",
    "    df_processed = _apply_non_leaderboard_features(df_copy)\n",
    "\n",
    "    X = df_processed.drop([\"target\", \"time_id\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n",
    "\n",
    "def process_data_leaderboard(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df_copy = data.copy()\n",
    "    # Drop rows where 'target' is null as these cannot be used for training\n",
    "    df_copy.dropna(subset=[\"target\"], inplace=True)\n",
    "    df_processed = _apply_leaderboard_features(df_copy)\n",
    "\n",
    "    X = df_processed.drop([\"target\", \"time_id\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n",
    "\n",
    "def process_data_combined(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df_copy = data.copy()\n",
    "    # Drop rows where 'target' is null as these cannot be used for training\n",
    "    df_copy.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # Apply non-leaderboard features first\n",
    "    df_processed = _apply_non_leaderboard_features(df_copy)\n",
    "\n",
    "    # Apply additional combined features from the leaderboard approach\n",
    "    # Handle NaN and infinities in specific input columns first as per original combined\n",
    "    input_cols_combined = ['imbalance_size', 'matched_size', 'ask_price', 'bid_price', 'wap', 'reference_price']\n",
    "    for col in input_cols_combined:\n",
    "        df_processed[col] = df_processed[col].replace([np.inf, -np.inf], np.nan).fillna(df_processed[col].median())\n",
    "\n",
    "    df_processed['seconds_in_bucket_group'] = np.where(df_processed['seconds_in_bucket'] < 300, 0,\n",
    "                                              np.where(df_processed['seconds_in_bucket'] < 480, 1, 2))\n",
    "    df_processed['imbalance_ratio'] = df_processed['imbalance_size'] / (df_processed['matched_size'] + 1e-6)\n",
    "    df_processed['mid_price'] = (df_processed['ask_price'] + df_processed['bid_price']) / 2\n",
    "    df_processed['time_in_auction'] = df_processed['seconds_in_bucket'] / 540\n",
    "\n",
    "    # Handle NaN and inf for newly created combined features\n",
    "    new_cols_combined = ['seconds_in_bucket_group', 'imbalance_ratio', 'mid_price', 'time_in_auction']\n",
    "    df_processed[new_cols_combined] = df_processed[new_cols_combined].replace([np.inf, -np.inf], np.nan)\n",
    "    df_processed[new_cols_combined] = df_processed[new_cols_combined].fillna(df_processed[new_cols_combined].median())\n",
    "\n",
    "    X = df_processed.drop([\"target\", \"time_id\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these functions are defined we can evaluate our models on these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3884\n",
      "[LightGBM] [Info] Number of data points in the train set: 3509387, number of used features: 19\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA TITAN RTX, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 19 dense feature groups (66.94 MB) transferred to GPU in 0.060675 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] Start training from score -0.044118\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.29163\n",
      "[300]\tvalid_0's l1: 6.28137\n",
      "[450]\tvalid_0's l1: 6.27309\n",
      "[600]\tvalid_0's l1: 6.26716\n",
      "[750]\tvalid_0's l1: 6.26234\n",
      "[900]\tvalid_0's l1: 6.25671\n",
      "[1050]\tvalid_0's l1: 6.2523\n",
      "[1200]\tvalid_0's l1: 6.24845\n",
      "[1350]\tvalid_0's l1: 6.24466\n",
      "[1500]\tvalid_0's l1: 6.24118\n",
      "[1650]\tvalid_0's l1: 6.23821\n",
      "[1800]\tvalid_0's l1: 6.23493\n",
      "[1950]\tvalid_0's l1: 6.23193\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 6.23091\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because MAE is/are not implemented for GPU\n"
     ]
    }
   ],
   "source": [
    "X, y = process_data_leaderboard(df.copy())\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "mae_lgbm, lgbm_model = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)\n",
    "mae_xgb, xgb_model = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)\n",
    "mae_cb, cb_model = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAE: 6.2309\n",
      "XGBoost MAE: 6.2416\n",
      "CatBoost MAE: 6.3264\n"
     ]
    }
   ],
   "source": [
    "print(f\"LightGBM MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost MAE: {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append(\n",
    "    {'Model': 'LightGBM', 'Feature Set': 'Leaderboard Inspired', 'MAE' : mae_lgbm},\n",
    "    {'Model': 'XGBoost', 'Feature Set': 'Leaderboard Inspired', 'MAE' : mae_xgb},\n",
    "    {'Model': 'CatBoost', 'Feature Set': 'Leaderboard Inspired', 'MAE' : mae_cb},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate on combined features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361242/2755009002.py:7: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
      "/tmp/ipykernel_3361242/2755009002.py:16: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n",
      "/tmp/ipykernel_3361242/2755009002.py:19: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6944\n",
      "[LightGBM] [Info] Number of data points in the train set: 3509387, number of used features: 31\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA TITAN RTX, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 31 dense feature groups (107.10 MB) transferred to GPU in 0.078936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] Start training from score -0.044118\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.28222\n",
      "[300]\tvalid_0's l1: 6.27168\n",
      "[450]\tvalid_0's l1: 6.26393\n",
      "[600]\tvalid_0's l1: 6.25758\n",
      "[750]\tvalid_0's l1: 6.25216\n",
      "[900]\tvalid_0's l1: 6.24708\n",
      "[1050]\tvalid_0's l1: 6.24156\n",
      "[1200]\tvalid_0's l1: 6.23772\n",
      "[1350]\tvalid_0's l1: 6.23467\n",
      "[1500]\tvalid_0's l1: 6.23159\n",
      "[1650]\tvalid_0's l1: 6.22885\n",
      "[1800]\tvalid_0's l1: 6.22606\n",
      "[1950]\tvalid_0's l1: 6.22257\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 6.22162\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because MAE is/are not implemented for GPU\n"
     ]
    }
   ],
   "source": [
    "X, y = process_data_combined(df.copy())\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "mae_lgbm, lgbm_model = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)\n",
    "mae_xgb, xgb_model = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)\n",
    "mae_cb, cb_model = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAE: 6.2216\n",
      "XGBoost MAE: 6.2301\n",
      "CatBoost MAE: 6.3188\n"
     ]
    }
   ],
   "source": [
    "print(f\"LightGBM MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost MAE: {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append(\n",
    "    {'Model': 'LightGBM', 'Feature Set': 'Combined', 'MAE' : mae_lgbm},\n",
    "    {'Model': 'XGBoost', 'Feature Set': 'Combined', 'MAE' : mae_xgb},\n",
    "    {'Model': 'CatBoost', 'Feature Set': 'Combined', 'MAE' : mae_cb},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70517476",
   "metadata": {},
   "source": [
    "## Feature Elimination\n",
    "\n",
    "We have a lot of features, and not all of them might be necessary for our peformance. Some might even be negatively affecting performance, so we can recursively eliminate unecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import EShapCalcType, EFeaturesSelectionAlgorithm\n",
    "\n",
    "# use same params as earlier catboost\n",
    "ctb_model = CatBoostRegressor(**cb_params)\n",
    "\n",
    "# get list of features\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# trim features\n",
    "summary = ctb_model.select_features(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    features_for_select=feature_names,\n",
    "    # how many features we go down to, might need changing\n",
    "    num_features_to_select=len(feature_names) - 3,\n",
    "    steps=3,\n",
    "    algorithm=EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n",
    "    shap_calc_type=EShapCalcType.Regular,\n",
    "    train_final_model=False,\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "# get selected features after trimming\n",
    "selected_features = summary['selected_features_names']\n",
    "\n",
    "# make new model to train on with trimmed features\n",
    "ctb_model_final = CatBoostRegressor(**cb_params)\n",
    "ctb_model_final.fit(X_train[selected_features], y_train, eval_set=[(X_val[selected_features], y_val)])\n",
    "\n",
    "# predict on our y values\n",
    "y_pred = ctb_model_final.predict(X_val[selected_features])\n",
    "\n",
    "# get MAE\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f\"Final Model MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcff442",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.append({'Model': 'CatBoost', 'Feature Set': 'Feature Elimination', 'MAE' : mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that having these extra features improved the results of our model, but it is also highly likely that not all of them are helpful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data with the combined feature function...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'process_data_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Process the raw dataframe `df` using your combined feature function.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This creates the feature set (X_train) and target (y_train) for the entire process.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing training data with the combined feature function...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X_train, y_train = \u001b[43mprocess_data_combined\u001b[49m(df)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 2. Create the time-based split for offline validation.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# We align the 'date_id' from the original df with the processed data.\u001b[39;00m\n\u001b[32m      8\u001b[39m df_for_split = df.loc[y_train.index]\n",
      "\u001b[31mNameError\u001b[39m: name 'process_data_combined' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Process the raw dataframe `df` using your combined feature function.\n",
    "# This creates the feature set (X_train) and target (y_train) for the entire process.\n",
    "print(\"Processing training data with the combined feature function...\")\n",
    "X_train, y_train = process_data_combined(df)\n",
    "\n",
    "# 2. Create the time-based split for offline validation.\n",
    "# We align the 'date_id' from the original df with the processed data.\n",
    "df_for_split = df.loc[y_train.index]\n",
    "offline_split = df_for_split['date_id'] > (split_day - 45)\n",
    "\n",
    "# Create offline training and validation sets from the processed data\n",
    "X_offline_train = X_train[~offline_split]\n",
    "y_offline_train = y_train[~offline_split]\n",
    "X_offline_valid = X_train[offline_split]\n",
    "y_offline_valid = y_train[offline_split]\n",
    "\n",
    "# Get the full list of feature names from the processed data\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Clean up original dataframes to save memory\n",
    "del df, df_for_split\n",
    "gc.collect()\n",
    "\n",
    "ctb_params = dict(iterations=1200,\n",
    "                    learning_rate=1.0,\n",
    "                    depth=8,\n",
    "                    l2_leaf_reg=30,\n",
    "                    bootstrap_type='Bernoulli',\n",
    "                    subsample=0.66,\n",
    "                    loss_function='MAE',\n",
    "                    eval_metric = 'MAE',\n",
    "                    metric_period=100,\n",
    "                    od_type='Iter',\n",
    "                    od_wait=30,\n",
    "                    task_type='GPU',\n",
    "                    allow_writing_files=False,\n",
    "                    )\n",
    "\n",
    "# 3. Perform feature elimination using the new processed and split data\n",
    "print(\"Feature Elimination Performing.\")\n",
    "ctb_model_selector = CatBoostRegressor(**ctb_params)\n",
    "summary = ctb_model_selector.select_features(\n",
    "    X_offline_train, y_offline_train,\n",
    "    eval_set=[(X_offline_valid, y_offline_valid)],\n",
    "    features_for_select=feature_names,\n",
    "    num_features_to_select=len(feature_names) - 24, # Adjust drop count as needed\n",
    "    steps=3,\n",
    "    algorithm=EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n",
    "    shap_calc_type=EShapCalcType.Regular,\n",
    "    train_final_model=False,\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "selected_features = summary['selected_features_names']\n",
    "print(f\"Selected {len(selected_features)} features from the processed data.\")\n",
    "\n",
    "# 4. Train a validation model on the selected feature subset\n",
    "print(\"Valid Model Training on Selected Features Subset.\")\n",
    "ctb_model = CatBoostRegressor(**ctb_params)\n",
    "ctb_model.fit(\n",
    "    X_offline_train[selected_features], y_offline_train,\n",
    "    eval_set=[(X_offline_valid[selected_features], y_offline_valid)],\n",
    "    use_best_model=True,\n",
    ")\n",
    "\n",
    "del X_offline_train, y_offline_train, X_offline_valid, y_offline_valid\n",
    "gc.collect()\n",
    "\n",
    "# 5. Train the final inference model on the FULL processed training data\n",
    "print(\"Infer Model Training on Selected Features Subset.\")\n",
    "infer_params = ctb_params.copy()\n",
    "# Use the best number of iterations found during validation training\n",
    "infer_params[\"iterations\"] = ctb_model.best_iteration_\n",
    "infer_ctb_model = CatBoostRegressor(**infer_params)\n",
    "infer_ctb_model.fit(X_train[selected_features], y_train)\n",
    "print(\"Infer Model Training on Selected Features Subset Complete.\")\n",
    "\n",
    "del X_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "if is_offline:\n",
    "    # 6. For offline score, process the validation data (`df_valid`) the same way\n",
    "    print(\"Processing validation data for offline score...\")\n",
    "    X_valid_feats, y_valid_target = process_data_combined(df_valid)\n",
    "\n",
    "    # Predict using the selected features\n",
    "    offline_predictions = infer_ctb_model.predict(X_valid_feats[selected_features])\n",
    "    offline_score = mean_absolute_error(offline_predictions, y_valid_target)\n",
    "    print(f\"Offline Score: {np.round(offline_score, 4)}\")\n",
    "    \n",
    "    del df_valid, X_valid_feats, y_valid_target\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca80ba",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "\n",
    "With this, we can see that CatBoost performs the best out of our finalist models when paired with all of the features. Now to further improve our model's performance, we can begin hypertuning the parameters of the model using RandomizedSearchCV. This allows us to explore a wide range of parameter combinations compared to GridSearch, which is exhaustive but computationally heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ac33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code by Brandon Leong\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# our initial catboost parameters\n",
    "cb_params = {\n",
    "    \"loss_function\": \"MAE\", \n",
    "    \"random_state\": 42, \n",
    "    \"verbose\": 0, \n",
    "    \"task_type\": \"CUDA\", \n",
    "    \"thread_count\": -1, \n",
    "    \"cat_features\": ['stock_id', 'imbalance_buy_sell_flag']\n",
    "} \n",
    "\n",
    "# distribution of parameters to try different values for, our what we are tuning\n",
    "parameter_distributions = {\n",
    "    'iterations': randint(1000, 3000), # number of trees\n",
    "    'learning_rate': uniform(0.01, 0.09), # reduces the gradient step, smaller value = more iterations needed\n",
    "    'depth': randint(4, 10), # tree depth\n",
    "    'l2_leaf_reg': uniform(1, 10), # regularizer values\n",
    "    'bagging_temperature': uniform(0, 1), # settings of the Bayesian bootstrap\n",
    "    'random_strength': uniform(0.1, 1), # amount of randomness to use for scoring splits\n",
    "    'border_count': randint(32, 255), # number of splits for numerical features\n",
    "    'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide'] # how trees are grown\n",
    "}\n",
    "\n",
    "# catboost model\n",
    "cat = CatBoostRegressor(\n",
    "    loss_function=\"MAE\",\n",
    "    task_type=\"CUDA\",\n",
    "    thread_count=-1,\n",
    "    random_state=42,\n",
    "    cat_features=['stock_id', 'imbalance_buy_sell_flag'],\n",
    "    verbose=0,\n",
    "    # early stopping\n",
    "    od_type='Iter',\n",
    "    od_wait=30,\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "# use randomized search to find best parameters\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=cat,\n",
    "    param_distributions=parameter_distributions,\n",
    "    n_iter=40,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train[selected_features], y_train)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters:{best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a504edd",
   "metadata": {},
   "source": [
    "Compared to RandomizedSearchCV, we also have Optuna, a hyperpameter optimization framework suggested to use on the Catboost documentation page. Optuna is generally more efficient and finds better parameters faster. Optuna tries hyperparameters based on past results to focus on promising areas, and supports early stopping allowing us to stop bad trials early. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba63e5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# code by brandon leong\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobjective\u001b[39m(trial):\n\u001b[32m      5\u001b[39m     params = {\n\u001b[32m      6\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_int(\u001b[33m'\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m10\u001b[39m),\n\u001b[32m      7\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.3\u001b[39m, log=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcat_features\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mstock_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mimbalance_buy_sell_flag\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m     }\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "# code by Brandon Leong\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'iterations': 1000,\n",
    "        'loss_function': 'MAE',\n",
    "        'task_type': 'CUDA',\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0,\n",
    "        'cat_features': ['stock_id', 'imbalance_buy_sell_flag']\n",
    "    }\n",
    "\n",
    "    # train / test splits\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    # model\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_train[selected_features], y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "    # predict\n",
    "    preds = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    return mae\n",
    "\n",
    "# we want to minimize the mae\n",
    "study = optuna.create_study(direction='minimize')\n",
    "# 50 trials, can be more for better results\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print('Best trial:')\n",
    "best_params = study.best_trial.params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff37f5",
   "metadata": {},
   "source": [
    "Now that we have our optimal parameters, we can retrain the model with the best parameters to get out final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3479ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code by Brandon Leong\n",
    "\n",
    "best_params.update({\n",
    "    'loss_function': 'MAE',\n",
    "    'task_type': 'CUDA',\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'cat_features': ['stock_id', 'imbalance_buy_sell_flag']\n",
    "})\n",
    "\n",
    "final_model = CatBoostRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# predict on validation set\n",
    "final_preds = final_model.predict(X_val)\n",
    "\n",
    "# get MAE\n",
    "final_mae = mean_absolute_error(y_val, final_preds)\n",
    "\n",
    "# results\n",
    "print(f\"Final MAE with best trial params: {final_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c8ff8",
   "metadata": {},
   "source": [
    "## Exploring Our Models and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401436d",
   "metadata": {},
   "source": [
    "To make it visually clear how each model performed across feature engineering strategies, we plot the MAE for each model with each feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce36fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot bar chart of MAE (Josephine)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mae_df = pd.DataFrame(results_dict)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=mae_df,\n",
    "    x='Feature Set',\n",
    "    y='MAE',\n",
    "    hue='Model',\n",
    "    palette='Set2'\n",
    ")\n",
    "plt.title('Model Performance by Feature Set (MAE)', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c854cf",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- Random Forest has a much higher MAE on the Baseline feature set compared to the other models.\n",
    "- For all other feature sets (Original, Post-Processing, Leaderboard Inspired, Combined), the gradient boosting models (LightGBM, XGBoost, CatBoost) have similar and better performance (lower MAE).\n",
    "- Linear Regression is missing for all but the Baseline set, possibly due to poor performance or irrelevance.\n",
    "- The Combined and Leaderboard Inspired feature sets slightly outperform the others across the board, suggesting that they offer the most predictive value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af73c80",
   "metadata": {},
   "source": [
    "This code calculates the training error for the final LightGBM, XGBoost, and CatBoost models by using the already-trained model objects to make predictions on the training data.It makes a line plot that visually compares each model's training error against its validation error, allowing you to easily check for signs of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code by Cooper Richmond\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "overfitting_data = []\n",
    "final_models_for_plot = {\n",
    "    \"LightGBM\": (lgbm_model, mae_lgbm),\n",
    "    \"XGBoost\": (xgb_model, mae_xgb),\n",
    "    \"CatBoost\": (cb_model, mae_cb)\n",
    "}\n",
    "\n",
    "for model_name, (model_obj, validation_mae) in final_models_for_plot.items():\n",
    "    y_pred_train = model_obj.predict(X_train)\n",
    "    training_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    overfitting_data.append({'Model': model_name, 'Data Set': 'Training', 'MAE': training_mae})\n",
    "    overfitting_data.append({'Model': model_name, 'Data Set': 'Validation', 'MAE': validation_mae})\n",
    "\n",
    "overfitting_df = pd.DataFrame(overfitting_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.lineplot(data=overfitting_df, x='Model', y='MAE', hue='Data Set', style='Data Set', markers=True, dashes=False, markersize=8)\n",
    "\n",
    "ax.set_title('Training vs. Validation MAE (Overfitting Check)', fontsize=16)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Mean Absolute Error (MAE)', fontsize=12)\n",
    "ax.legend(title='Data Set')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65974758",
   "metadata": {},
   "source": [
    "Overfitting is what happens when a machine learning model learns the training data too well, including its noise and outliers, which causes the model to perform poorly on new, unseen data.\n",
    "\n",
    "This chart plots the Mean Absolute Error (MAE) for both the training and validation datasets across three different models: LightGBM, XGBoost, and CatBoost.\n",
    "\n",
    "Heres a breakdown of what the chart shows:\n",
    "\n",
    "X-axis: The machine learning models being compared.\n",
    "\n",
    "Y-axis: The Mean Absolute Error (MAE). A lower MAE indicates a better performance.\n",
    "\n",
    "Blue Line (Training MAE): This shows the error of each model on the data it was trained on.\n",
    "\n",
    "Orange Line (Validation MAE): This shows the error of each model on a separate set of data it has not seen before. This is a better representation of how the model would perform in the real world.\n",
    "\n",
    "Analysis of the Chart:\n",
    "Gap between training and validation error: There is a noticeable gap between the training and validation MAE for all three models. The validation error is consistently higher than the training error, which is expected. However, the size of this gap can indicate overfitting.\n",
    "Performance of the models:\n",
    "LightGBM and XGBoost: These two models show a larger gap between training and validation error compared to CatBoost. This suggests they generalize worse to new data and are potentially more overfit.\n",
    "\n",
    "CatBoost: This model has the smallest gap between its training and validation performance. This indicates it is less prone to overfitting on this dataset compared to the other two models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "\n",
    "Our experiments have shown that the additional features definetely help improve the Mean Absolute Error of the models as features like seconds_in_bucket_group, imbalance_ratio, and mid_price help the models understand different auction phases, supply-demand imbalances, and price centrality which heavily impact how we model price movements. \n",
    "\n",
    "Despite having far fewer features and less development time than top leaderboard competitors as well as the top placers on the leaderboard being able to construct over 300 features for this dataset combined with the computing power to quickly test through those data sets, we were able to get relatively good enough results (the difference of MAE from number 1 to us is <1). This showcases the importance of selecting the right model combined with proper feature engineering. If we continued to develop more features and pick which subsets of features worked best we would be able to get good results. Unfortunately we were hit with a time limitation, as the Kaggle ran for over 6 months, while we only had realistically one week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Areas of Improvement\n",
    "\n",
    "While our results with our current feature engineering and models have promising results, there a clear areas that we can optimize and refine upon to enhance performance. \n",
    "\n",
    "### 1. Handling Missing Data\n",
    "We could improve upon how we handle the NaN rows of far_price and near_price. As a human, we know that this means that the orders did not fill for the auction, but the models have no way to tell. If we were to find some way to put this in a feature, it might improve the results of the model. Rather than simply dropping the values, creating an indicator to flag for missing migth help with providing better signals over masking patterns in the data.\n",
    "\n",
    "### 2. Expanded feature Engineering\n",
    "Another area of improvement would be like previously mentioned, picking the best features and forgetting about other ones that worsen or elongate training time. The winning solution had hundreds of features, and going through each feature to determine the most important and relevant ones could increase accuracy.  \n",
    "\n",
    "### 3. Model Hyperparameter Tuning\n",
    "We could also try hyper parameter tuning the other models, as fine tuning hyperparameters such as learning rate, tree depth, number of estimators, regularization terms, and subsampling ratios can lead to significant gains and maybe a different model would rise to have the best results.\n",
    "\n",
    "### 4. Outside Kaggle Competition\n",
    "While in regards to the competition, we were supposed to only use the single dataset we were given if we were to continue to refine the model outside of it for real world applicatino, linking external data might help add additional context such as interest rates, CPI, or even news sentiment. Additionally, using other scoring methods such as time series cross validaiton could better reflect everchanging and sequential stock data. \n",
    "\n",
    "All of these would be achievable given more time, and we would definitely see some improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9876865",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
