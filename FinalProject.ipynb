{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8908d8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Predicting closing price movements of NASDAQ stocks\n",
    "Code and text done by Attila Jamilov, Cooper Richmond, some code by Yueying Du, Brandon Leong, Josephine Welin.\n",
    "Text written and improved by Brandon\n",
    "\n",
    "## Introduction\n",
    "In the last minutes of the market being open, many stocks see heightened volatility as well as big price fluctuation. NASDAQ stock exchange uses the NASDAQ Closing Cross auction to determine the official closing prices for various assets on their exchange. We want to evaluate the performance of multiple models that we learned in class and not, on predicting this closing price movement using the dataset provided in the [Kaggle](https://www.kaggle.com/competitions/optiver-trading-at-the-close/overview), and see what models performs best, and what features we can engineer to improve on the performance of the models. \n",
    "\n",
    "### Project Goal\n",
    "We evaluate how well different machine learning algorithms can predict stock price movement near the market close. Our specific target is the change in the *market clearing price* during the NASDAQ Closing Cross.\n",
    "\n",
    "### Data Overview\n",
    "Each row represents auction-related market activity for a given stock on a given day. For our features, we will try using only the features provided in the dataset, then creating our own original features, trying features that the Kaggle competitors had success with, and finally a compilation of all features. Then, we will select only the most helpful features, and then test our best model on the test data set through the Kaggle. \n",
    "\n",
    "### Models and Algorithms\n",
    "For our models, we will begin with Linear Regression (Josephine), Random Forest (Brandon), LightGBM and CNN (Yueying), XGBoost (Cooper), and finally we will look into Catboost (Attila), a model developed by Yandex which the winner of the Kaggle used for his approach to this Kaggle. We compare all of these models using mean average error. \n",
    "\n",
    "## Data Processing\n",
    "First, we need to import the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fadd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing done by Attila and Cooper\n",
    "import pandas as pd\n",
    "\n",
    "def process_data(df: pd.DataFrame):\n",
    "    df.dropna(subset=[\"target\"], inplace=True)\n",
    "    X = df.drop([\"target\"], axis=1)\n",
    "    y = df[\"target\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0bf40",
   "metadata": {},
   "source": [
    "We also define several processing functions.\n",
    "\n",
    "The function process_data_drop_null_target removes all rows with missing target values.\n",
    "\n",
    "The function process_data_drop_prices removes rows with missing near_price or far_price, because some models (linear regression) can't handle null features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac478317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_drop_null_target(df: pd.DataFrame):\n",
    "    df_processed = df.dropna(subset=[\"target\"]).copy()\n",
    "    X = df_processed.drop([\"target\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n",
    "\n",
    "def process_data_drop_prices(df: pd.DataFrame):\n",
    "    df_processed = df.dropna(subset=[\"near_price\", \"far_price\"]).copy()\n",
    "    X = df_processed.drop([\"target\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef193c66",
   "metadata": {},
   "source": [
    "We create a general training function to handle training multiple models with the same dataset, as well as specialized cases for LightGBM due to early stopping and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b460557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_class, X_train, y_train, X_val, y_val, params=None):\n",
    "    if params:\n",
    "        model = model_class(**params)\n",
    "    else:\n",
    "        model = model_class()\n",
    "\n",
    "    if isinstance(model, LGBMRegressor):\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='mae',\n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=150, verbose=True),\n",
    "                log_evaluation(period=150)\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    return mae, model\n",
    "\n",
    "df = pd.read_csv(\"./train.csv\", index_col=\"row_id\") \n",
    "\n",
    "df = df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f2ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = process_data_drop_null_target(df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922a156",
   "metadata": {},
   "source": [
    "We 'remove' the identifying feature \"row_id\", but we set it as the `index_col`, which is necessary for submitting to the Kaggle but will not be used as a feature when training the models. \n",
    "\n",
    "Next, we need to split the data into a training and validating subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a523dd44",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b3c11",
   "metadata": {},
   "source": [
    "`train_test_split` shuffles the data on it's own, therefore there is nothing we need to do on that part. We are done with our initial data processing, but certain models cannot handle NaN values, so individual work will need to be done to get them to work. Now we can move on to Naive model training with the most basic features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690da49d",
   "metadata": {},
   "source": [
    "## Naive feature selection\n",
    "\n",
    "Before doing any feature selection, we will first evaluate our models on the given features, to see how well feature engineering will improve our results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef5540a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperrichmond/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/cooperrichmond/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/cooperrichmond/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "# Original code by Josephine, appended and corrected to work with data to Notebook by Attila, Cooper\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "X_lr, y_lr = process_data_drop_prices(df.copy())\n",
    "X_train_lr, X_val_lr, y_train_lr, y_val_lr = train_test_split(X_lr, y_lr, test_size=0.33, random_state=42)\n",
    "mae_lr, _ = train_model(LinearRegression, X_train_lr, y_train_lr, X_val_lr, y_val_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112c42f",
   "metadata": {},
   "source": [
    "Random forest is an ensemble learning method that utilizes decision trees to output average predictions. While easy to tune, training on the full data set was computationally intensive and inefficient due to it's size. Since Random Forest was a core part of our class' material, we included it by training it on a smaller (5%) random sample of the dataset to reduce training time even though it isn't a good fit for what we are trying to achieve.\n",
    "\n",
    "*Contributed by [Brandon]*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb35a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Original code by Brandon, appended and corrected to work with data to Notebook by Attila, Cooper\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df_rf = df.sample(frac=0.05).copy()\n",
    "X_rf, y_rf = process_data_drop_null_target(df_rf)\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(X_rf, y_rf, test_size=0.33, random_state=42)\n",
    "rf_params = {\"n_estimators\": 100, \"random_state\": 42, \"n_jobs\": -1}\n",
    "mae_rf, _ = train_model(RandomForestRegressor, X_train_rf, y_train_rf, X_val_rf, y_val_rf, params=rf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301c23a",
   "metadata": {},
   "source": [
    "LightGBM (Light Gradient Boosting Machine) is a high-performance gradient boosting framework that is well-suited for large-scale datasets and tabular data with many features. It is particularly efficient for training on millions of observations, which made it a practical choice for our dataset of over 5 million instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3316\n",
      "[LightGBM] [Info] Number of data points in the train set: 350939, number of used features: 15\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Start training from score -0.026625\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.32681\n",
      "[300]\tvalid_0's l1: 6.31888\n",
      "[450]\tvalid_0's l1: 6.3149\n",
      "[600]\tvalid_0's l1: 6.31322\n",
      "[750]\tvalid_0's l1: 6.31105\n",
      "[900]\tvalid_0's l1: 6.30933\n",
      "[1050]\tvalid_0's l1: 6.30834\n",
      "[1200]\tvalid_0's l1: 6.30757\n",
      "[1350]\tvalid_0's l1: 6.30704\n",
      "[1500]\tvalid_0's l1: 6.30636\n",
      "[1650]\tvalid_0's l1: 6.30576\n",
      "[1800]\tvalid_0's l1: 6.30483\n",
      "[1950]\tvalid_0's l1: 6.30448\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 6.30436\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "# Original code by Yueying, appended to notebook by Attila, Cooper\n",
    "\n",
    "lgbm_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 31,\n",
    "    'n_estimators': 2000,\n",
    "    'min_child_samples': 50,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 0.5,\n",
    "    'device': 'gpu',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "mae_lgbm, _ = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe1216",
   "metadata": {},
   "source": [
    "This code is for training an XGBoost regression model, evaluating it with MAE, and inspecting feature importances. It accepts the training and validation features and target values, along with a list of feature names to use for training. The function configures the model with predefined hyperparameters. These include a squared error objective, mean absolute error (MAE) as the evaluation, a learning rate of 0.05, and a maximum tree depth of 6. After training, it makes predictions and calculates the MAE to determine performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c13d6471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperrichmond/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:18:15] WARNING: /Users/runner/work/xgboost/xgboost/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:absoluteerror',\n",
    "    'eval_metric': 'mae',\n",
    "    'device': \"cuda\",\n",
    "    'random_state': 42\n",
    "}\n",
    "mae_xgb, _ = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1702a2",
   "metadata": {},
   "source": [
    "Catboost is the model that was used to win first place in the kaggle. Catboost itself is an algorithm with a novel gradient-boosting scheme on decision trees. It is developed by Yandex, and it has many benefits, such as providing great quality with the default parameters, saving time from parameter tuning, and it supports categorical features, instead of having to pre-process data to numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fede1c3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "catboost/libs/train_lib/trainer_env.cpp:9: Environment for task type [GPU] not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor\n\u001b[1;32m      3\u001b[0m cb_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimbalance_buy_sell_flag\u001b[39m\u001b[38;5;124m'\u001b[39m] }\n\u001b[0;32m----> 4\u001b[0m mae_cb, _ \u001b[38;5;241m=\u001b[39m train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params\u001b[38;5;241m=\u001b[39mcb_params)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_class, X_train, y_train, X_val, y_val, params)\u001b[0m\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      9\u001b[0m         X_train, y_train,\n\u001b[1;32m     10\u001b[0m         eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         ]\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     20\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     21\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_val, y_pred)\n",
      "File \u001b[0;32m~/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/catboost/core.py:5873\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   5872\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m-> 5873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, graph, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline,\n\u001b[1;32m   5874\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[1;32m   5875\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[1;32m   5876\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[0;32m~/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/catboost/core.py:2410\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2407\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[1;32m   2411\u001b[0m         train_pool,\n\u001b[1;32m   2412\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_sets\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   2413\u001b[0m         params,\n\u001b[1;32m   2414\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2415\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2416\u001b[0m     )\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/catboost/core.py:1790\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[38;5;241m.\u001b[39m_object \u001b[38;5;28;01mif\u001b[39;00m init_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:5023\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:5072\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: catboost/libs/train_lib/trainer_env.cpp:9: Environment for task type [GPU] not found"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cb_params = {\"loss_function\": \"MAE\", \"random_state\": 42, \"verbose\": 1, \"task_type\": \"GPU\", \"thread_count\": -1, \"cat_features\": ['stock_id', 'imbalance_buy_sell_flag'] }\n",
    "mae_cb, _ = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc4012",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Naive Feature selection model performance\n",
    "Now that we have trained and predicted each algorithm, we can finally evaluate their performance to give us a baseline to test on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60adb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: 5.7134\n",
      "Random Forest: 6.3858\n",
      "LightGBM: 6.2487\n",
      "XGBoost: 6.2542\n",
      "CatBoost: 6.3464\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Regression: {mae_lr:.4f}\")\n",
    "print(f\"Random Forest: {mae_rf:.4f}\")\n",
    "print(f\"LightGBM: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost: {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877ae30",
   "metadata": {},
   "source": [
    "Interestingly, we found dropping near price and far price (features that are half null), gave Linear Regression the best score. Among the models that trained on the whole training set, LightGBM had the best MAE. This is a good starting point to see how well we can improve the performance of the last three models. To save time and avoid redundancy, we will not pursue Linear Regression or Random Forest further.\n",
    "\n",
    "## Original Feature Engineering\n",
    "Contributed by Cooper Richmond\n",
    "\n",
    "In this section, we will think of and create our own features to use for training the finalist models (LightGBM, XGBoost, and CatBoost). \n",
    "\n",
    "These our are originally made features: \n",
    "\n",
    "- `wap_volatility`: Measures how much the weighted average price (WAP) swings over 5 time periods for each stock. \n",
    "\n",
    "- `bid_ask_spread`: Difference between the selling price (ask) and buying price (bid).\n",
    "\n",
    "- `bid_ask_volatility`: Tracks how much the bid-ask spread varies over 5 time periods for each stock.\n",
    "\n",
    "- `wap_momentum`: Shows the percentage change in WAP over 3 time periods for each stock.\n",
    "\n",
    "- `price_momentum`: Shows the percentage change in reference price over 3 time periods for each stock.\n",
    "\n",
    "- `log_imbalance_size`: Shrinks large imbalance size values (buy/sell order differences) using a log function.\n",
    "\n",
    "- `log_matched_size`: Shrinks large matched order size values using a log function.\n",
    "\n",
    "- `log_bid_size`: Shrinks large bid order size values using a log function.\n",
    "\n",
    "- `log_ask_size`: Shrinks large ask order size values using a log function.\n",
    "\n",
    "- `bucket_price_interaction`: Multiplies time (seconds in bucket) by reference price to capture their combined effect.\n",
    "\n",
    "- `bucket_imbalance_interaction`: Multiplies time (seconds in bucket) by imbalance size to capture their combined effect.\n",
    "\n",
    "- `wap_to_ref_price`: Divides WAP by reference price to show their relative difference.\n",
    "\n",
    "- `bid_to_ask_price`: Divides bid price by ask price to show their relative difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7602801",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Written by Cooper and Attila \n",
    "def process_data_with_feature_engineering(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    # Drop rows where 'target' is null, as these cannot be used for training\n",
    "    data.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # Volatility features\n",
    "    data['wap_volatility'] = data.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "    data['bid_ask_spread'] = data['ask_price'] - data['bid_price']\n",
    "    data['bid_ask_volatility'] = data.groupby('stock_id')['bid_ask_spread'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # Momentum features\n",
    "    data['wap_momentum'] = data.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "    data['price_momentum'] = data.groupby('stock_id')['reference_price'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "\n",
    "    # Log transformations\n",
    "    size_cols = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size']\n",
    "    for col in size_cols:\n",
    "        data[f'log_{col}'] = np.log1p(data[col].clip(lower=0))\n",
    "\n",
    "    # Time-based interactions\n",
    "    data['bucket_price_interaction'] = data['seconds_in_bucket'] * data['reference_price']\n",
    "    data['bucket_imbalance_interaction'] = data['seconds_in_bucket'] * data['imbalance_size']\n",
    "\n",
    "    # Relative price features\n",
    "    data['wap_to_ref_price'] = data['wap'] / (data['reference_price'] + 1e-6)\n",
    "    data['bid_to_ask_price'] = data['bid_price'] / (data['ask_price'] + 1e-6)\n",
    "\n",
    "    # Handle NaN and inf after feature creation\n",
    "    # Identify columns that are not identifiers or the target\n",
    "    cols_to_process = [col for col in data.columns if col not in ['stock_id', 'date_id', 'target', 'time_id', 'row_id']]\n",
    "    data[cols_to_process] = data[cols_to_process].replace([np.inf, -np.inf], np.nan) # Replace inf with NaN first\n",
    "    data[cols_to_process] = data[cols_to_process].fillna(data[cols_to_process].median()) # Fill remaining NaNs with median\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = data.drop(\"target\", axis=1)\n",
    "    y = data[\"target\"]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109832e3",
   "metadata": {},
   "source": [
    "Now we can redo the naive training but with our original features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee718f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361242/2978197467.py:8: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
      "/tmp/ipykernel_3361242/2978197467.py:17: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n",
      "/tmp/ipykernel_3361242/2978197467.py:20: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6631\n",
      "[LightGBM] [Info] Number of data points in the train set: 3509387, number of used features: 28\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA TITAN RTX, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 28 dense feature groups (93.71 MB) transferred to GPU in 0.072622 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] Start training from score -0.044118\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.28241\n",
      "[300]\tvalid_0's l1: 6.27216\n",
      "[450]\tvalid_0's l1: 6.26487\n",
      "[600]\tvalid_0's l1: 6.25915\n",
      "[750]\tvalid_0's l1: 6.25455\n",
      "[900]\tvalid_0's l1: 6.24964\n",
      "[1050]\tvalid_0's l1: 6.24462\n",
      "[1200]\tvalid_0's l1: 6.24064\n",
      "[1350]\tvalid_0's l1: 6.23612\n",
      "[1500]\tvalid_0's l1: 6.23274\n",
      "[1650]\tvalid_0's l1: 6.22993\n",
      "[1800]\tvalid_0's l1: 6.22697\n",
      "[1950]\tvalid_0's l1: 6.22417\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 6.2233\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because MAE is/are not implemented for GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X, y = process_data_with_feature_engineering(df.copy())\n",
    "\n",
    "# Split the data into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# LightGBM\n",
    "lgbm_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 31,\n",
    "    'n_estimators': 2000,\n",
    "    'min_child_samples': 50,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 0.5,\n",
    "    'device': 'gpu',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "mae_lgbm, lgbm_model = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)\n",
    "\n",
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'objective': 'reg:absoluteerror',\n",
    "    'eval_metric': 'mae',\n",
    "    'device': \"cuda\",\n",
    "    'random_state': 42\n",
    "}\n",
    "mae_xgb, xgb_model = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)\n",
    "\n",
    "# CatBoost\n",
    "cb_params = {\"loss_function\": \"MAE\", \"random_state\": 42, \"verbose\": 0, \"task_type\": \"GPU\", \"thread_count\": -1, \"cat_features\": ['stock_id', 'imbalance_buy_sell_flag']} \n",
    "mae_cb, cb_model = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e8ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAE: 6.2233\n",
      "XGBoost MAE: 6.2289\n",
      "CatBoost MAE: 6.3187\n"
     ]
    }
   ],
   "source": [
    "print(f\"LightGBM MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost MAE: {mae_cb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97063fc",
   "metadata": {},
   "source": [
    "We are seeing some minor improvements, but not enough to justify that our features are that helpful. Let's use the post processing trick that was used by the 1st place winner, and see how those results compare:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf4c0a",
   "metadata": {},
   "source": [
    "We applied the post-processing trick used by the Kaggle first-place solution, which corrects each model’s predictions by subtracting the weighted average prediction for each time_id. The weights are based on a predefined stock_weights dictionary reflecting the relative importance or liquidity of each stock.\n",
    "\n",
    "*Explanation contributed by [Brandon]*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4963125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stock weights (Given by Kaggle) (Attila, Cooper)\n",
    "weight_df = pd.DataFrame()\n",
    "weight_df['stock_id'] = list(range(200))\n",
    "weight_df['weight'] = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,]\n",
    "stock_weights = dict(zip(weight_df['stock_id'], weight_df['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LightGBM post-processing ---\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "LightGBM Adjusted MAE: 6.2221\n",
      "\n",
      "--- XGBoost post-processing ---\n",
      "XGBoost Adjusted MAE: 6.2313\n",
      "\n",
      "--- CatBoost post-processing ---\n",
      "CatBoost Adjusted MAE: 6.3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brandon - made universal helper function for post processing\n",
    "\n",
    "def apply_postprocessing(model, X_val, y_val, stock_weights, model_name=\"Model\"):\n",
    "    preds_df = pd.DataFrame({\n",
    "        'prediction': model.predict(X_val),\n",
    "        'stock_id': X_val['stock_id'],\n",
    "        'time_id': X_val['time_id']\n",
    "    })\n",
    "    preds_df['stock_weights'] = preds_df['stock_id'].map(stock_weights)\n",
    "    preds_df['weighted_sum'] = preds_df['prediction'] * preds_df['stock_weights']\n",
    "    \n",
    "    weighted_avg = preds_df.groupby('time_id')['weighted_sum'].transform('sum') / preds_df.groupby('time_id')['stock_weights'].transform('sum')\n",
    "    preds_adjusted = preds_df['prediction'] - weighted_avg\n",
    "    \n",
    "    mae_adjusted = mean_absolute_error(y_val, preds_adjusted)\n",
    "    print(f\"{model_name} Adjusted MAE: {mae_adjusted:.4f}\\n\")\n",
    "    return preds_adjusted, mae_adjusted\n",
    "\n",
    "print(\"--- LightGBM post-processing ---\")\n",
    "y_pred_lgbm_adjusted, mae_lgbm_adjusted = apply_postprocessing(lgbm_model, X_val, y_val, stock_weights, \"LightGBM\")\n",
    "\n",
    "print(\"--- XGBoost post-processing ---\")\n",
    "y_pred_xgb_adjusted, mae_xgb_adjusted = apply_postprocessing(xgb_model, X_val, y_val, stock_weights, \"XGBoost\")\n",
    "\n",
    "print(\"--- CatBoost post-processing ---\")\n",
    "y_pred_cb_adjusted, mae_cb_adjusted = apply_postprocessing(cb_model, X_val, y_val, stock_weights, \"CatBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8681fd",
   "metadata": {},
   "source": [
    "Looks like it was helpful to post process only for LightGBM and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d34885",
   "metadata": {},
   "source": [
    "## Additional Feature Ideas Inspired by Kaggle Leaderboard\n",
    "Contributed by Cooper Richmond\n",
    "\n",
    "This is the code for some features we created that we got the ideas from on the kaggle leaderboard. \n",
    "\n",
    "- `seconds_in_bucket_group`: Splits time into three groups: 0 (0-299 seconds), 1 (300-479 seconds), 2 (480+ seconds).  \n",
    "\n",
    "- `bid_ask_spread`: Difference between ask (sell) and bid (buy) prices.\n",
    "\n",
    "- `imbalance_ratio`: Divides imbalance size (buy/sell order difference) by matched size (executed orders).\n",
    "\n",
    "- `mid_price`: Average of ask and bid prices.\n",
    "\n",
    "- `time_in_auction`: Normalizes time (seconds in bucket) by dividing by 540 to scale between 0 and 1.\n",
    "\n",
    "We also combined these with our original features into a combined function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Cooper and Attila \n",
    "\n",
    "# Helper functions\n",
    "def _apply_non_leaderboard_features(data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Volatility features\n",
    "    data_df['wap_volatility'] = data_df.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "    data_df['bid_ask_spread'] = data_df['ask_price'] - data_df['bid_price']\n",
    "    data_df['bid_ask_volatility'] = data_df.groupby('stock_id')['bid_ask_spread'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # Momentum features\n",
    "    data_df['wap_momentum'] = data_df.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "    data_df['price_momentum'] = data_df.groupby('stock_id')['reference_price'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "\n",
    "    # Log transformations\n",
    "    size_cols = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size']\n",
    "    for col in size_cols:\n",
    "        data_df[f'log_{col}'] = np.log1p(data_df[col].clip(lower=0))\n",
    "\n",
    "    # Time-based interactions\n",
    "    data_df['bucket_price_interaction'] = data_df['seconds_in_bucket'] * data_df['reference_price']\n",
    "    data_df['bucket_imbalance_interaction'] = data_df['seconds_in_bucket'] * data_df['imbalance_size']\n",
    "\n",
    "    # Relative price features\n",
    "    data_df['wap_to_ref_price'] = data_df['wap'] / (data_df['reference_price'] + 1e-6)\n",
    "    data_df['bid_to_ask_price'] = data_df['bid_price'] / (data_df['ask_price'] + 1e-6)\n",
    "\n",
    "    # Handle NaN and inf for all newly created numerical columns\n",
    "    # Exclude identifier/target columns as they are handled in the main processing functions\n",
    "    new_cols_added = [col for col in data_df.columns if col not in ['stock_id', 'date_id', 'target', 'time_id', 'row_id']]\n",
    "    # Filter to only numerical columns that might contain NaN/inf\n",
    "    numerical_cols_to_fill = data_df[new_cols_added].select_dtypes(include=np.number).columns\n",
    "    data_df[numerical_cols_to_fill] = data_df[numerical_cols_to_fill].replace([np.inf, -np.inf], np.nan)\n",
    "    data_df[numerical_cols_to_fill] = data_df[numerical_cols_to_fill].fillna(data_df[numerical_cols_to_fill].median())\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def _apply_leaderboard_features(data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Handle NaN and infinities in specific input columns first as per original\n",
    "    input_cols = ['imbalance_size', 'matched_size', 'ask_price', 'bid_price', 'wap', 'reference_price']\n",
    "    for col in input_cols:\n",
    "        data_df[col] = data_df[col].replace([np.inf, -np.inf], np.nan).fillna(data_df[col].median())\n",
    "\n",
    "    # 1st Place: Seconds in bucket group\n",
    "    data_df['seconds_in_bucket_group'] = np.where(data_df['seconds_in_bucket'] < 300, 0,\n",
    "                                              np.where(data_df['seconds_in_bucket'] < 480, 1, 2))\n",
    "\n",
    "    # 9th Place: Basic features\n",
    "    data_df['bid_ask_spread'] = data_df['ask_price'] - data_df['bid_price']\n",
    "    data_df['imbalance_ratio'] = data_df['imbalance_size'] / (data_df['matched_size'] + 1e-6)\n",
    "\n",
    "    # 14th Place: Mid price\n",
    "    data_df['mid_price'] = (data_df['ask_price'] + data_df['bid_price']) / 2\n",
    "\n",
    "    # Time in auction\n",
    "    data_df['time_in_auction'] = data_df['seconds_in_bucket'] / 540\n",
    "\n",
    "    # Handle NaN and inf for newly created columns\n",
    "    new_cols = ['seconds_in_bucket_group', 'bid_ask_spread', 'imbalance_ratio', 'mid_price', 'time_in_auction']\n",
    "    data_df[new_cols] = data_df[new_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    data_df[new_cols] = data_df[new_cols].fillna(data_df[new_cols].median())\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# Main data processing functions\n",
    "def process_data_non_leaderboard(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df_copy = data.copy()\n",
    "    # Drop rows where 'target' is null as these cannot be used for training\n",
    "    df_copy.dropna(subset=[\"target\"], inplace=True)\n",
    "    df_processed = _apply_non_leaderboard_features(df_copy)\n",
    "\n",
    "    X = df_processed.drop([\"target\", \"time_id\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n",
    "\n",
    "def process_data_leaderboard(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df_copy = data.copy()\n",
    "    # Drop rows where 'target' is null as these cannot be used for training\n",
    "    df_copy.dropna(subset=[\"target\"], inplace=True)\n",
    "    df_processed = _apply_leaderboard_features(df_copy)\n",
    "\n",
    "    X = df_processed.drop([\"target\", \"time_id\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n",
    "\n",
    "def process_data_combined(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df_copy = data.copy()\n",
    "    # Drop rows where 'target' is null as these cannot be used for training\n",
    "    df_copy.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # Apply non-leaderboard features first\n",
    "    df_processed = _apply_non_leaderboard_features(df_copy)\n",
    "\n",
    "    # Apply additional combined features from the leaderboard approach\n",
    "    # Handle NaN and infinities in specific input columns first as per original combined\n",
    "    input_cols_combined = ['imbalance_size', 'matched_size', 'ask_price', 'bid_price', 'wap', 'reference_price']\n",
    "    for col in input_cols_combined:\n",
    "        df_processed[col] = df_processed[col].replace([np.inf, -np.inf], np.nan).fillna(df_processed[col].median())\n",
    "\n",
    "    df_processed['seconds_in_bucket_group'] = np.where(df_processed['seconds_in_bucket'] < 300, 0,\n",
    "                                              np.where(df_processed['seconds_in_bucket'] < 480, 1, 2))\n",
    "    df_processed['imbalance_ratio'] = df_processed['imbalance_size'] / (df_processed['matched_size'] + 1e-6)\n",
    "    df_processed['mid_price'] = (df_processed['ask_price'] + df_processed['bid_price']) / 2\n",
    "    df_processed['time_in_auction'] = df_processed['seconds_in_bucket'] / 540\n",
    "\n",
    "    # Handle NaN and inf for newly created combined features\n",
    "    new_cols_combined = ['seconds_in_bucket_group', 'imbalance_ratio', 'mid_price', 'time_in_auction']\n",
    "    df_processed[new_cols_combined] = df_processed[new_cols_combined].replace([np.inf, -np.inf], np.nan)\n",
    "    df_processed[new_cols_combined] = df_processed[new_cols_combined].fillna(df_processed[new_cols_combined].median())\n",
    "\n",
    "    X = df_processed.drop([\"target\", \"time_id\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362fc7a0",
   "metadata": {},
   "source": [
    "Now that these functions are defined we can evaluate our models on these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c3b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3884\n",
      "[LightGBM] [Info] Number of data points in the train set: 3509387, number of used features: 19\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA TITAN RTX, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 19 dense feature groups (66.94 MB) transferred to GPU in 0.060675 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] Start training from score -0.044118\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.29163\n",
      "[300]\tvalid_0's l1: 6.28137\n",
      "[450]\tvalid_0's l1: 6.27309\n",
      "[600]\tvalid_0's l1: 6.26716\n",
      "[750]\tvalid_0's l1: 6.26234\n",
      "[900]\tvalid_0's l1: 6.25671\n",
      "[1050]\tvalid_0's l1: 6.2523\n",
      "[1200]\tvalid_0's l1: 6.24845\n",
      "[1350]\tvalid_0's l1: 6.24466\n",
      "[1500]\tvalid_0's l1: 6.24118\n",
      "[1650]\tvalid_0's l1: 6.23821\n",
      "[1800]\tvalid_0's l1: 6.23493\n",
      "[1950]\tvalid_0's l1: 6.23193\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 6.23091\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because MAE is/are not implemented for GPU\n"
     ]
    }
   ],
   "source": [
    "X, y = process_data_leaderboard(df.copy())\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "mae_lgbm, lgbm_model = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)\n",
    "mae_xgb, xgb_model = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)\n",
    "mae_cb, cb_model = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95792251",
   "metadata": {},
   "source": [
    "This gets us the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAE: 6.2309\n",
      "XGBoost MAE: 6.2416\n",
      "CatBoost MAE: 6.3264\n"
     ]
    }
   ],
   "source": [
    "print(f\"LightGBM MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost MAE: {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8cc2d",
   "metadata": {},
   "source": [
    "Now we can evaluate on combined features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810e211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361242/2755009002.py:7: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
      "/tmp/ipykernel_3361242/2755009002.py:16: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n",
      "/tmp/ipykernel_3361242/2755009002.py:19: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 6944\n",
      "[LightGBM] [Info] Number of data points in the train set: 3509387, number of used features: 31\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA TITAN RTX, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 31 dense feature groups (107.10 MB) transferred to GPU in 0.078936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Info] Start training from score -0.044118\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.28222\n",
      "[300]\tvalid_0's l1: 6.27168\n",
      "[450]\tvalid_0's l1: 6.26393\n",
      "[600]\tvalid_0's l1: 6.25758\n",
      "[750]\tvalid_0's l1: 6.25216\n",
      "[900]\tvalid_0's l1: 6.24708\n",
      "[1050]\tvalid_0's l1: 6.24156\n",
      "[1200]\tvalid_0's l1: 6.23772\n",
      "[1350]\tvalid_0's l1: 6.23467\n",
      "[1500]\tvalid_0's l1: 6.23159\n",
      "[1650]\tvalid_0's l1: 6.22885\n",
      "[1800]\tvalid_0's l1: 6.22606\n",
      "[1950]\tvalid_0's l1: 6.22257\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 6.22162\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because MAE is/are not implemented for GPU\n"
     ]
    }
   ],
   "source": [
    "X, y = process_data_combined(df.copy())\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "mae_lgbm, lgbm_model = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)\n",
    "mae_xgb, xgb_model = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)\n",
    "mae_cb, cb_model = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAE: 6.2216\n",
      "XGBoost MAE: 6.2301\n",
      "CatBoost MAE: 6.3188\n"
     ]
    }
   ],
   "source": [
    "print(f\"LightGBM MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost MAE: {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12), sharex=False)\n",
    "\n",
    "# 1. MAE Comparison Bar Plot\n",
    "results_df = pd.DataFrame(results).T.reset_index()\n",
    "results_df.columns = ['Feature Set', 'Random Forest', 'Linear Regression', 'XGBoost']\n",
    "results_df = results_df.melt(id_vars='Feature Set', var_name='Model', value_name='MAE')\n",
    "\n",
    "sns.barplot(data=results_df, x='Feature Set', y='MAE', hue='Model', ax=ax1)\n",
    "ax1.set_title('MAE Comparison Across Feature Sets and Models', fontsize=14)\n",
    "ax1.set_xlabel('Feature Set', fontsize=12)\n",
    "ax1.set_ylabel('Mean Absolute Error (MAE)', fontsize=12)\n",
    "ax1.legend(title='Model', fontsize=10)\n",
    "ax1.tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 2. Overfitting Line Plot (Training vs. Validation MAE)\n",
    "# We need to retrain the models to get both training and validation MAEs.\n",
    "overfitting_data = []\n",
    "feature_sets = [\n",
    "    ('No Features', feature_engineering_none, raw_features),\n",
    "    ('Non-Leaderboard Features', feature_engineering_non_leaderboard, None),\n",
    "    ('Leaderboard Features', feature_engineering_leaderboard, None),\n",
    "    ('Combined Features', feature_engineering_combined, None)\n",
    "]\n",
    "\n",
    "for fe_name, fe_func, specific_features in feature_sets:\n",
    "    data_fe = fe_func(data.copy())\n",
    "    \n",
    "    if specific_features:\n",
    "        features = specific_features\n",
    "    else:\n",
    "        features = data_fe.columns.difference(['stock_id', 'date_id', 'target', 'time_id', 'row_id']).tolist()\n",
    "\n",
    "    X = data_fe[features].replace([np.inf, -np.inf], np.nan).fillna(data_fe[features].median()).astype(np.float32)\n",
    "    y = data_fe['target'].replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # --- Random Forest ---\n",
    "    model_rf, _ = train_random_forest(X_train, y_train, X_val, y_val, sample_size=500000)\n",
    "    y_pred_train_rf = model_rf.predict(X_train)\n",
    "    y_pred_val_rf = model_rf.predict(X_val)\n",
    "    mae_train_rf = mean_absolute_error(y_train, y_pred_train_rf)\n",
    "    mae_val_rf = mean_absolute_error(y_val, y_pred_val_rf)\n",
    "    overfitting_data.append({'Feature Set': fe_name, 'Model': 'Random Forest', 'Set': 'Training', 'MAE': mae_train_rf})\n",
    "    overfitting_data.append({'Feature Set': fe_name, 'Model': 'Random Forest', 'Set': 'Validation', 'MAE': mae_val_rf})\n",
    "\n",
    "    # --- Linear Regression ---\n",
    "    model_lr, _ = train_lin_reg(X_train, y_train, X_val, y_val)\n",
    "    y_pred_train_lr = model_lr.predict(X_train)\n",
    "    y_pred_val_lr = model_lr.predict(X_val)\n",
    "    mae_train_lr = mean_absolute_error(y_train, y_pred_train_lr)\n",
    "    mae_val_lr = mean_absolute_error(y_val, y_pred_val_lr)\n",
    "    overfitting_data.append({'Feature Set': fe_name, 'Model': 'Linear Regression', 'Set': 'Training', 'MAE': mae_train_lr})\n",
    "    overfitting_data.append({'Feature Set': fe_name, 'Model': 'Linear Regression', 'Set': 'Validation', 'MAE': mae_val_lr})\n",
    "\n",
    "    # --- XGBoost ---\n",
    "    model_xgb, _, _ = run_xgboost_regression(X_train, y_train, X_val, y_val, features)\n",
    "    y_pred_train_xgb = model_xgb.predict(X_train[features])\n",
    "    y_pred_val_xgb = model_xgb.predict(X_val[features])\n",
    "    mae_train_xgb = mean_absolute_error(y_train, y_pred_train_xgb)\n",
    "    mae_val_xgb = mean_absolute_error(y_val, y_pred_val_xgb)\n",
    "    overfitting_data.append({'Feature Set': fe_name, 'Model': 'XGBoost', 'Set': 'Training', 'MAE': mae_train_xgb})\n",
    "    overfitting_data.append({'Feature Set': fe_name, 'Model': 'XGBoost', 'Set': 'Validation', 'MAE': mae_val_xgb})\n",
    "\n",
    "overfitting_df = pd.DataFrame(overfitting_data)\n",
    "sns.lineplot(data=overfitting_df, x='Feature Set', y='MAE', hue='Model', style='Set', markers=True, dashes=False, ax=ax2)\n",
    "ax2.set_title('Training vs. Validation MAE Across Feature Sets and Models', fontsize=14)\n",
    "ax2.set_xlabel('Feature Set', fontsize=12)\n",
    "ax2.set_ylabel('Mean Absolute Error (MAE)', fontsize=12)\n",
    "ax2.legend(title='Model / Set', fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f043b",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "*Contributed by [Brandon]*\n",
    "\n",
    "Our experiments have shown that the additional features definetely help improve the Mean Absolute Error of the models as features like seconds_in_bucket_group, imbalance_ratio, and mid_price help the models understand different auction phases, supply-demand imbalances, and price centrality which heavily impact how we model price movements. \n",
    "\n",
    "Despite having far fewer features and less development time than top leaderboard competitors as well as the top placers on the leaderboard being able to construct over 300 features for this dataset combined with the computing power to quickly test through those data sets, we were able to get relatively good enough results (the difference of MAE from number 1 to us is <1). This showcases the importance of selecting the right model combined with proper feature engineering. If we continued to develop more features and pick which subsets of features worked best we would be able to get good results. Unfortunately we were hit with a time limitation, as the Kaggle ran for over 6 months, while we only had realistically one week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade6d3b",
   "metadata": {},
   "source": [
    "## Areas of Improvement\n",
    "*Contributed by [Brandon]*\n",
    "\n",
    "While our results with our current feature engineering and models have promising results, there a clear areas that we can optimize and refine upon to enhance performance. \n",
    "\n",
    "### 1. Handling Missing Data\n",
    "We could improve upon how we handle the NaN rows of far_price and near_price. As a human, we know that this means that the orders did not fill for the auction, but the models have no way to tell. If we were to find some way to put this in a feature, it might improve the results of the model. Rather than simply dropping the values, creating an indicator to flag for missing migth help with providing better signals over masking patterns in the data.\n",
    "\n",
    "### 2. Expanded feature Engineering\n",
    "Another area of improvement would be like previously mentioned, picking the best features and forgetting about other ones that worsen or elongate training time. The winning solution had hundreds of features, and going through each feature to determine the most important and relevant ones could increase accuracy.  \n",
    "\n",
    "### 3. Model Hyperparameter Tuning\n",
    "We could also try hyper parameter tuning the other models, as fine tuning hyperparameters such as learning rate, tree depth, number of estimators, regularization terms, and subsampling ratios can lead to significant gains.\n",
    "\n",
    "### 4. Outside Kaggle Competition\n",
    "While in regards to the competition, we were supposed to only use the single dataset we were given if we were to continue to refine the model outside of it for real world applicatino, linking external data might help add additional context such as interest rates, CPI, or even news sentiment. Additionally, using other scoring methods such as time series cross validaiton could better reflect everchanging and sequential stock data. \n",
    "\n",
    "All of these would be achievable given more time, and we would definitely see some improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
