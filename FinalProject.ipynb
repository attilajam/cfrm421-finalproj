{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8908d8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Predicting closing price movements of NASDAQ stocks\n",
    "Code and text done by Attila Jamilov, Cooper Richmond, some code by Yueying Du, Brandon Leong, Josephine Welin.\n",
    "Text written and improved by Brandon\n",
    "\n",
    "## Introduction\n",
    "In the last minutes of the market being open, many stocks see heightened volatility as well as big price fluctuation. NASDAQ stock exchange uses the NASDAQ Closing Cross auction to determine the official closing prices for various assets on their exchange. We want to evaluate the performance of multiple models that we learned in class and not, on predicting this closing price movement using the dataset provided in the [Kaggle](https://www.kaggle.com/competitions/optiver-trading-at-the-close/overview), and see what models performs best, and what features we can engineer to improve on the performance of the models. \n",
    "\n",
    "### Project Goal\n",
    "We evaluate how well different machine learning algorithms can predict stock price movement near the market close. Our specific target is the change in the *market clearing price* during the NASDAQ Closing Cross.\n",
    "\n",
    "### Data Overview\n",
    "Each row represents auction-related market activity for a given stock on a given day. For our features, we will try using only the features provided in the dataset, then creating our own original features, trying features that the Kaggle competitors had success with, and finally a compilation of all features. Then, we will select only the most helpful features, and then test our best model on the test data set through the Kaggle. \n",
    "\n",
    "### Models and Algorithms\n",
    "For our models, we will begin with Linear Regression (Josephine), Random Forest (Brandon), LightGBM and CNN (Yueying), XGBoost (Cooper), and finally we will look into Catboost (Attila), a model developed by Yandex which the winner of the Kaggle used for his approach to this Kaggle. We compare all of these models using mean average error. \n",
    "\n",
    "## Data Processing\n",
    "First, we need to import the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fadd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing done by Attila and Cooper\n",
    "import pandas as pd\n",
    "\n",
    "def process_data(df: pd.DataFrame):\n",
    "    df.dropna(subset=[\"target\"], inplace=True)\n",
    "    X = df.drop([\"target\"], axis=1)\n",
    "    y = df[\"target\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0bf40",
   "metadata": {},
   "source": [
    "We also define several processing functions.\n",
    "\n",
    "The function process_data_drop_null_target removes all rows with missing target values.\n",
    "\n",
    "The function process_data_drop_prices removes rows with missing near_price or far_price, because some models (linear regression) can't handle null features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac478317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_drop_null_target(df: pd.DataFrame):\n",
    "    df_processed = df.dropna(subset=[\"target\"]).copy()\n",
    "    X = df_processed.drop([\"target\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n",
    "\n",
    "def process_data_drop_prices(df: pd.DataFrame):\n",
    "    df_processed = df.dropna(subset=[\"near_price\", \"far_price\"]).copy()\n",
    "    X = df_processed.drop([\"target\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef193c66",
   "metadata": {},
   "source": [
    "We create a general training function to handle training multiple models with the same dataset, as well as specialized cases for LightGBM due to early stopping and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b460557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "def train_model(model_class, X_train, y_train, X_val, y_val, params=None):\n",
    "    if params:\n",
    "        model = model_class(**params)\n",
    "    else:\n",
    "        model = model_class()\n",
    "\n",
    "    if isinstance(model, LGBMRegressor):\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='mae',\n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=150, verbose=True),\n",
    "                log_evaluation(period=150)\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    return mae, model\n",
    "\n",
    "df = pd.read_csv(\"./train.csv\", index_col=\"row_id\") \n",
    "\n",
    "df = df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81f2ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = process_data_drop_null_target(df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922a156",
   "metadata": {},
   "source": [
    "We 'remove' the identifying feature \"row_id\", but we set it as the `index_col`, which is necessary for submitting to the Kaggle but will not be used as a feature when training the models. \n",
    "\n",
    "Next, we need to split the data into a training and validating subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a523dd44",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b3c11",
   "metadata": {},
   "source": [
    "`train_test_split` shuffles the data on it's own, therefore there is nothing we need to do on that part. We are done with our initial data processing, but certain models cannot handle NaN values, so individual work will need to be done to get them to work. Now we can move on to Naive model training with the most basic features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690da49d",
   "metadata": {},
   "source": [
    "## Naive feature selection\n",
    "\n",
    "Before doing any feature selection, we will first evaluate our models on the given features, to see how well feature engineering will improve our results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef5540a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperrichmond/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/cooperrichmond/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/cooperrichmond/Code/VSCODE/Python/CFRM 521/conda-env/lib/python3.11/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "# Original code by Josephine, appended and corrected to work with data to Notebook by Attila, Cooper\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "X_lr, y_lr = process_data_drop_prices(df.copy())\n",
    "X_train_lr, X_val_lr, y_train_lr, y_val_lr = train_test_split(X_lr, y_lr, test_size=0.33, random_state=42)\n",
    "mae_lr, _ = train_model(LinearRegression, X_train_lr, y_train_lr, X_val_lr, y_val_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112c42f",
   "metadata": {},
   "source": [
    "Random forest is an ensemble learning method that utilizes decision trees to output average predictions. While easy to tune, training on the full data set was computationally intensive and inefficient due to it's size. Since Random Forest was a core part of our class' material, we included it by training it on a smaller (5%) random sample of the dataset to reduce training time even though it isn't a good fit for what we are trying to achieve.\n",
    "\n",
    "*Contributed by [Brandon]*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddb35a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Original code by Brandon, appended and corrected to work with data to Notebook by Attila, Cooper\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df_rf = df.sample(frac=0.05).copy()\n",
    "X_rf, y_rf = process_data_drop_null_target(df_rf)\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(X_rf, y_rf, test_size=0.33, random_state=42)\n",
    "rf_params = {\"n_estimators\": 100, \"random_state\": 42, \"n_jobs\": -1}\n",
    "mae_rf, _ = train_model(RandomForestRegressor, X_train_rf, y_train_rf, X_val_rf, y_val_rf, params=rf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301c23a",
   "metadata": {},
   "source": [
    "LightGBM (Light Gradient Boosting Machine) is a high-performance gradient boosting framework that is well-suited for large-scale datasets and tabular data with many features. It is particularly efficient for training on millions of observations, which made it a practical choice for our dataset of over 5 million instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3316\n",
      "[LightGBM] [Info] Number of data points in the train set: 350939, number of used features: 15\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Start training from score -0.026625\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.32681\n",
      "[300]\tvalid_0's l1: 6.31888\n",
      "[450]\tvalid_0's l1: 6.3149\n",
      "[600]\tvalid_0's l1: 6.31322\n",
      "[750]\tvalid_0's l1: 6.31105\n",
      "[900]\tvalid_0's l1: 6.30933\n",
      "[1050]\tvalid_0's l1: 6.30834\n",
      "[1200]\tvalid_0's l1: 6.30757\n",
      "[1350]\tvalid_0's l1: 6.30704\n",
      "[1500]\tvalid_0's l1: 6.30636\n",
      "[1650]\tvalid_0's l1: 6.30576\n",
      "[1800]\tvalid_0's l1: 6.30483\n",
      "[1950]\tvalid_0's l1: 6.30448\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 6.30436\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "# Original code by Yueying, appended to notebook by Attila, Cooper\n",
    "\n",
    "lgbm_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 31,\n",
    "    'n_estimators': 2000,\n",
    "    'min_child_samples': 50,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 0.5,\n",
    "    'device': 'gpu',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "mae_lgbm, _ = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe1216",
   "metadata": {},
   "source": [
    "This code is for training an XGBoost regression model, evaluating it with MAE, and inspecting feature importances. It accepts the training and validation features and target values, along with a list of feature names to use for training. The function configures the model with predefined hyperparameters. These include a squared error objective, mean absolute error (MAE) as the evaluation, a learning rate of 0.05, and a maximum tree depth of 6. After training, it makes predictions and calculates the MAE to determine performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:absoluteerror',\n",
    "    'eval_metric': 'mae',\n",
    "    'device': \"cuda\",\n",
    "    'random_state': 42\n",
    "}\n",
    "mae_xgb, _ = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1702a2",
   "metadata": {},
   "source": [
    "Catboost is the model that was used to win first place in the kaggle. Catboost itself is an algorithm with a novel gradient-boosting scheme on decision trees. It is developed by Yandex, and it has many benefits, such as providing great quality with the default parameters, saving time from parameter tuning, and it supports categorical features, instead of having to pre-process data to numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fede1c3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 6.4050494\ttotal: 92.5ms\tremaining: 1m 32s\n",
      "1:\tlearn: 6.4021959\ttotal: 125ms\tremaining: 1m 2s\n",
      "2:\tlearn: 6.3997248\ttotal: 156ms\tremaining: 51.8s\n",
      "3:\tlearn: 6.3973006\ttotal: 186ms\tremaining: 46.3s\n",
      "4:\tlearn: 6.3948483\ttotal: 218ms\tremaining: 43.4s\n",
      "5:\tlearn: 6.3924930\ttotal: 247ms\tremaining: 40.9s\n",
      "6:\tlearn: 6.3905573\ttotal: 274ms\tremaining: 38.8s\n",
      "7:\tlearn: 6.3886754\ttotal: 305ms\tremaining: 37.9s\n",
      "8:\tlearn: 6.3866726\ttotal: 343ms\tremaining: 37.8s\n",
      "9:\tlearn: 6.3848106\ttotal: 374ms\tremaining: 37s\n",
      "10:\tlearn: 6.3830461\ttotal: 405ms\tremaining: 36.4s\n",
      "11:\tlearn: 6.3813733\ttotal: 435ms\tremaining: 35.8s\n",
      "12:\tlearn: 6.3797903\ttotal: 466ms\tremaining: 35.3s\n",
      "13:\tlearn: 6.3783677\ttotal: 493ms\tremaining: 34.7s\n",
      "14:\tlearn: 6.3769530\ttotal: 523ms\tremaining: 34.3s\n",
      "15:\tlearn: 6.3756847\ttotal: 552ms\tremaining: 33.9s\n",
      "16:\tlearn: 6.3745334\ttotal: 581ms\tremaining: 33.6s\n",
      "17:\tlearn: 6.3733299\ttotal: 612ms\tremaining: 33.4s\n",
      "18:\tlearn: 6.3721532\ttotal: 643ms\tremaining: 33.2s\n",
      "19:\tlearn: 6.3711233\ttotal: 670ms\tremaining: 32.8s\n",
      "20:\tlearn: 6.3699772\ttotal: 701ms\tremaining: 32.7s\n",
      "21:\tlearn: 6.3689713\ttotal: 732ms\tremaining: 32.5s\n",
      "22:\tlearn: 6.3679571\ttotal: 763ms\tremaining: 32.4s\n",
      "23:\tlearn: 6.3670666\ttotal: 793ms\tremaining: 32.3s\n",
      "24:\tlearn: 6.3662227\ttotal: 825ms\tremaining: 32.2s\n",
      "25:\tlearn: 6.3652916\ttotal: 856ms\tremaining: 32.1s\n",
      "26:\tlearn: 6.3645126\ttotal: 886ms\tremaining: 31.9s\n",
      "27:\tlearn: 6.3637536\ttotal: 917ms\tremaining: 31.8s\n",
      "28:\tlearn: 6.3631484\ttotal: 944ms\tremaining: 31.6s\n",
      "29:\tlearn: 6.3624403\ttotal: 975ms\tremaining: 31.5s\n",
      "30:\tlearn: 6.3618500\ttotal: 1s\tremaining: 31.3s\n",
      "31:\tlearn: 6.3612049\ttotal: 1.03s\tremaining: 31.2s\n",
      "32:\tlearn: 6.3605488\ttotal: 1.06s\tremaining: 31.2s\n",
      "33:\tlearn: 6.3600222\ttotal: 1.09s\tremaining: 31s\n",
      "34:\tlearn: 6.3594469\ttotal: 1.12s\tremaining: 30.9s\n",
      "35:\tlearn: 6.3588540\ttotal: 1.15s\tremaining: 30.8s\n",
      "36:\tlearn: 6.3582249\ttotal: 1.18s\tremaining: 30.7s\n",
      "37:\tlearn: 6.3577331\ttotal: 1.21s\tremaining: 30.7s\n",
      "38:\tlearn: 6.3572349\ttotal: 1.24s\tremaining: 30.5s\n",
      "39:\tlearn: 6.3566625\ttotal: 1.27s\tremaining: 30.5s\n",
      "40:\tlearn: 6.3562103\ttotal: 1.3s\tremaining: 30.4s\n",
      "41:\tlearn: 6.3558029\ttotal: 1.33s\tremaining: 30.4s\n",
      "42:\tlearn: 6.3553315\ttotal: 1.36s\tremaining: 30.2s\n",
      "43:\tlearn: 6.3547161\ttotal: 1.39s\tremaining: 30.1s\n",
      "44:\tlearn: 6.3543799\ttotal: 1.42s\tremaining: 30.1s\n",
      "45:\tlearn: 6.3540233\ttotal: 1.45s\tremaining: 30s\n",
      "46:\tlearn: 6.3534929\ttotal: 1.47s\tremaining: 29.9s\n",
      "47:\tlearn: 6.3531053\ttotal: 1.5s\tremaining: 29.8s\n",
      "48:\tlearn: 6.3527264\ttotal: 1.53s\tremaining: 29.7s\n",
      "49:\tlearn: 6.3523937\ttotal: 1.56s\tremaining: 29.7s\n",
      "50:\tlearn: 6.3519226\ttotal: 1.59s\tremaining: 29.6s\n",
      "51:\tlearn: 6.3514408\ttotal: 1.62s\tremaining: 29.6s\n",
      "52:\tlearn: 6.3511629\ttotal: 1.65s\tremaining: 29.5s\n",
      "53:\tlearn: 6.3508378\ttotal: 1.68s\tremaining: 29.5s\n",
      "54:\tlearn: 6.3505789\ttotal: 1.71s\tremaining: 29.4s\n",
      "55:\tlearn: 6.3502291\ttotal: 1.74s\tremaining: 29.4s\n",
      "56:\tlearn: 6.3497432\ttotal: 1.77s\tremaining: 29.4s\n",
      "57:\tlearn: 6.3494682\ttotal: 1.8s\tremaining: 29.3s\n",
      "58:\tlearn: 6.3491386\ttotal: 1.83s\tremaining: 29.2s\n",
      "59:\tlearn: 6.3488778\ttotal: 1.86s\tremaining: 29.2s\n",
      "60:\tlearn: 6.3483776\ttotal: 1.89s\tremaining: 29.1s\n",
      "61:\tlearn: 6.3481110\ttotal: 1.92s\tremaining: 29.1s\n",
      "62:\tlearn: 6.3478526\ttotal: 1.95s\tremaining: 29s\n",
      "63:\tlearn: 6.3476186\ttotal: 1.98s\tremaining: 28.9s\n",
      "64:\tlearn: 6.3472437\ttotal: 2s\tremaining: 28.8s\n",
      "65:\tlearn: 6.3470322\ttotal: 2.03s\tremaining: 28.8s\n",
      "66:\tlearn: 6.3466367\ttotal: 2.06s\tremaining: 28.7s\n",
      "67:\tlearn: 6.3464663\ttotal: 2.1s\tremaining: 28.7s\n",
      "68:\tlearn: 6.3462173\ttotal: 2.12s\tremaining: 28.7s\n",
      "69:\tlearn: 6.3460531\ttotal: 2.15s\tremaining: 28.6s\n",
      "70:\tlearn: 6.3458646\ttotal: 2.18s\tremaining: 28.6s\n",
      "71:\tlearn: 6.3455489\ttotal: 2.21s\tremaining: 28.5s\n",
      "72:\tlearn: 6.3453719\ttotal: 2.25s\tremaining: 28.5s\n",
      "73:\tlearn: 6.3450264\ttotal: 2.27s\tremaining: 28.5s\n",
      "74:\tlearn: 6.3447396\ttotal: 2.31s\tremaining: 28.4s\n",
      "75:\tlearn: 6.3445709\ttotal: 2.33s\tremaining: 28.4s\n",
      "76:\tlearn: 6.3443918\ttotal: 2.36s\tremaining: 28.3s\n",
      "77:\tlearn: 6.3442114\ttotal: 2.39s\tremaining: 28.3s\n",
      "78:\tlearn: 6.3439269\ttotal: 2.42s\tremaining: 28.2s\n",
      "79:\tlearn: 6.3437265\ttotal: 2.45s\tremaining: 28.2s\n",
      "80:\tlearn: 6.3434763\ttotal: 2.48s\tremaining: 28.1s\n",
      "81:\tlearn: 6.3433033\ttotal: 2.5s\tremaining: 28s\n",
      "82:\tlearn: 6.3429751\ttotal: 2.53s\tremaining: 28s\n",
      "83:\tlearn: 6.3427228\ttotal: 2.56s\tremaining: 28s\n",
      "84:\tlearn: 6.3424314\ttotal: 2.59s\tremaining: 27.9s\n",
      "85:\tlearn: 6.3421721\ttotal: 2.62s\tremaining: 27.8s\n",
      "86:\tlearn: 6.3419297\ttotal: 2.65s\tremaining: 27.8s\n",
      "87:\tlearn: 6.3416320\ttotal: 2.68s\tremaining: 27.8s\n",
      "88:\tlearn: 6.3415127\ttotal: 2.71s\tremaining: 27.8s\n",
      "89:\tlearn: 6.3413886\ttotal: 2.75s\tremaining: 27.8s\n",
      "90:\tlearn: 6.3410546\ttotal: 2.77s\tremaining: 27.7s\n",
      "91:\tlearn: 6.3407205\ttotal: 2.8s\tremaining: 27.7s\n",
      "92:\tlearn: 6.3405766\ttotal: 2.84s\tremaining: 27.7s\n",
      "93:\tlearn: 6.3402771\ttotal: 2.87s\tremaining: 27.7s\n",
      "94:\tlearn: 6.3401500\ttotal: 2.9s\tremaining: 27.7s\n",
      "95:\tlearn: 6.3398515\ttotal: 2.93s\tremaining: 27.6s\n",
      "96:\tlearn: 6.3395592\ttotal: 2.96s\tremaining: 27.6s\n",
      "97:\tlearn: 6.3392542\ttotal: 2.99s\tremaining: 27.5s\n",
      "98:\tlearn: 6.3391027\ttotal: 3.02s\tremaining: 27.5s\n",
      "99:\tlearn: 6.3388666\ttotal: 3.05s\tremaining: 27.5s\n",
      "100:\tlearn: 6.3386025\ttotal: 3.08s\tremaining: 27.4s\n",
      "101:\tlearn: 6.3383994\ttotal: 3.11s\tremaining: 27.4s\n",
      "102:\tlearn: 6.3382734\ttotal: 3.14s\tremaining: 27.3s\n",
      "103:\tlearn: 6.3380723\ttotal: 3.17s\tremaining: 27.3s\n",
      "104:\tlearn: 6.3378465\ttotal: 3.2s\tremaining: 27.3s\n",
      "105:\tlearn: 6.3376239\ttotal: 3.23s\tremaining: 27.3s\n",
      "106:\tlearn: 6.3374123\ttotal: 3.27s\tremaining: 27.3s\n",
      "107:\tlearn: 6.3372698\ttotal: 3.29s\tremaining: 27.2s\n",
      "108:\tlearn: 6.3370476\ttotal: 3.32s\tremaining: 27.2s\n",
      "109:\tlearn: 6.3370096\ttotal: 3.35s\tremaining: 27.1s\n",
      "110:\tlearn: 6.3367948\ttotal: 3.38s\tremaining: 27s\n",
      "111:\tlearn: 6.3366119\ttotal: 3.41s\tremaining: 27s\n",
      "112:\tlearn: 6.3363618\ttotal: 3.44s\tremaining: 27s\n",
      "113:\tlearn: 6.3360383\ttotal: 3.47s\tremaining: 26.9s\n",
      "114:\tlearn: 6.3358491\ttotal: 3.5s\tremaining: 26.9s\n",
      "115:\tlearn: 6.3355341\ttotal: 3.53s\tremaining: 26.9s\n",
      "116:\tlearn: 6.3353847\ttotal: 3.56s\tremaining: 26.9s\n",
      "117:\tlearn: 6.3351599\ttotal: 3.59s\tremaining: 26.8s\n",
      "118:\tlearn: 6.3349249\ttotal: 3.62s\tremaining: 26.8s\n",
      "119:\tlearn: 6.3348993\ttotal: 3.64s\tremaining: 26.7s\n",
      "120:\tlearn: 6.3346901\ttotal: 3.67s\tremaining: 26.7s\n",
      "121:\tlearn: 6.3346139\ttotal: 3.7s\tremaining: 26.6s\n",
      "122:\tlearn: 6.3343515\ttotal: 3.73s\tremaining: 26.6s\n",
      "123:\tlearn: 6.3340942\ttotal: 3.76s\tremaining: 26.6s\n",
      "124:\tlearn: 6.3339878\ttotal: 3.79s\tremaining: 26.5s\n",
      "125:\tlearn: 6.3339240\ttotal: 3.82s\tremaining: 26.5s\n",
      "126:\tlearn: 6.3337597\ttotal: 3.85s\tremaining: 26.4s\n",
      "127:\tlearn: 6.3335076\ttotal: 3.87s\tremaining: 26.4s\n",
      "128:\tlearn: 6.3332939\ttotal: 3.9s\tremaining: 26.4s\n",
      "129:\tlearn: 6.3331159\ttotal: 3.93s\tremaining: 26.3s\n",
      "130:\tlearn: 6.3329183\ttotal: 3.96s\tremaining: 26.3s\n",
      "131:\tlearn: 6.3327216\ttotal: 4s\tremaining: 26.3s\n",
      "132:\tlearn: 6.3324621\ttotal: 4.03s\tremaining: 26.2s\n",
      "133:\tlearn: 6.3322350\ttotal: 4.05s\tremaining: 26.2s\n",
      "134:\tlearn: 6.3319456\ttotal: 4.08s\tremaining: 26.2s\n",
      "135:\tlearn: 6.3317495\ttotal: 4.11s\tremaining: 26.1s\n",
      "136:\tlearn: 6.3316602\ttotal: 4.14s\tremaining: 26.1s\n",
      "137:\tlearn: 6.3314627\ttotal: 4.17s\tremaining: 26.1s\n",
      "138:\tlearn: 6.3312995\ttotal: 4.2s\tremaining: 26s\n",
      "139:\tlearn: 6.3311900\ttotal: 4.23s\tremaining: 26s\n",
      "140:\tlearn: 6.3310345\ttotal: 4.26s\tremaining: 26s\n",
      "141:\tlearn: 6.3308896\ttotal: 4.29s\tremaining: 25.9s\n",
      "142:\tlearn: 6.3307207\ttotal: 4.32s\tremaining: 25.9s\n",
      "143:\tlearn: 6.3305442\ttotal: 4.35s\tremaining: 25.9s\n",
      "144:\tlearn: 6.3303765\ttotal: 4.38s\tremaining: 25.9s\n",
      "145:\tlearn: 6.3301673\ttotal: 4.42s\tremaining: 25.8s\n",
      "146:\tlearn: 6.3299901\ttotal: 4.45s\tremaining: 25.8s\n",
      "147:\tlearn: 6.3298499\ttotal: 4.48s\tremaining: 25.8s\n",
      "148:\tlearn: 6.3296828\ttotal: 4.51s\tremaining: 25.8s\n",
      "149:\tlearn: 6.3294773\ttotal: 4.54s\tremaining: 25.7s\n",
      "150:\tlearn: 6.3293691\ttotal: 4.57s\tremaining: 25.7s\n",
      "151:\tlearn: 6.3292093\ttotal: 4.6s\tremaining: 25.7s\n",
      "152:\tlearn: 6.3291483\ttotal: 4.63s\tremaining: 25.6s\n",
      "153:\tlearn: 6.3289692\ttotal: 4.66s\tremaining: 25.6s\n",
      "154:\tlearn: 6.3288648\ttotal: 4.69s\tremaining: 25.6s\n",
      "155:\tlearn: 6.3287246\ttotal: 4.72s\tremaining: 25.6s\n",
      "156:\tlearn: 6.3285502\ttotal: 4.76s\tremaining: 25.5s\n",
      "157:\tlearn: 6.3283092\ttotal: 4.79s\tremaining: 25.5s\n",
      "158:\tlearn: 6.3281505\ttotal: 4.82s\tremaining: 25.5s\n",
      "159:\tlearn: 6.3279021\ttotal: 4.85s\tremaining: 25.4s\n",
      "160:\tlearn: 6.3276697\ttotal: 4.87s\tremaining: 25.4s\n",
      "161:\tlearn: 6.3275689\ttotal: 4.9s\tremaining: 25.3s\n",
      "162:\tlearn: 6.3273621\ttotal: 4.93s\tremaining: 25.3s\n",
      "163:\tlearn: 6.3271787\ttotal: 4.96s\tremaining: 25.3s\n",
      "164:\tlearn: 6.3270290\ttotal: 4.99s\tremaining: 25.2s\n",
      "165:\tlearn: 6.3268692\ttotal: 5.02s\tremaining: 25.2s\n",
      "166:\tlearn: 6.3268237\ttotal: 5.04s\tremaining: 25.2s\n",
      "167:\tlearn: 6.3267775\ttotal: 5.08s\tremaining: 25.1s\n",
      "168:\tlearn: 6.3266259\ttotal: 5.1s\tremaining: 25.1s\n",
      "169:\tlearn: 6.3264744\ttotal: 5.13s\tremaining: 25s\n",
      "170:\tlearn: 6.3263008\ttotal: 5.16s\tremaining: 25s\n",
      "171:\tlearn: 6.3261929\ttotal: 5.19s\tremaining: 25s\n",
      "172:\tlearn: 6.3260613\ttotal: 5.22s\tremaining: 25s\n",
      "173:\tlearn: 6.3258860\ttotal: 5.25s\tremaining: 24.9s\n",
      "174:\tlearn: 6.3258588\ttotal: 5.28s\tremaining: 24.9s\n",
      "175:\tlearn: 6.3257092\ttotal: 5.31s\tremaining: 24.9s\n",
      "176:\tlearn: 6.3256258\ttotal: 5.34s\tremaining: 24.8s\n",
      "177:\tlearn: 6.3255051\ttotal: 5.37s\tremaining: 24.8s\n",
      "178:\tlearn: 6.3253981\ttotal: 5.4s\tremaining: 24.8s\n",
      "179:\tlearn: 6.3252435\ttotal: 5.43s\tremaining: 24.7s\n",
      "180:\tlearn: 6.3252089\ttotal: 5.46s\tremaining: 24.7s\n",
      "181:\tlearn: 6.3250523\ttotal: 5.49s\tremaining: 24.7s\n",
      "182:\tlearn: 6.3248787\ttotal: 5.52s\tremaining: 24.7s\n",
      "183:\tlearn: 6.3247483\ttotal: 5.55s\tremaining: 24.6s\n",
      "184:\tlearn: 6.3246489\ttotal: 5.58s\tremaining: 24.6s\n",
      "185:\tlearn: 6.3245427\ttotal: 5.61s\tremaining: 24.6s\n",
      "186:\tlearn: 6.3243562\ttotal: 5.64s\tremaining: 24.5s\n",
      "187:\tlearn: 6.3242359\ttotal: 5.67s\tremaining: 24.5s\n",
      "188:\tlearn: 6.3240889\ttotal: 5.7s\tremaining: 24.5s\n",
      "189:\tlearn: 6.3239754\ttotal: 5.73s\tremaining: 24.4s\n",
      "190:\tlearn: 6.3239100\ttotal: 5.75s\tremaining: 24.4s\n",
      "191:\tlearn: 6.3237006\ttotal: 5.78s\tremaining: 24.3s\n",
      "192:\tlearn: 6.3234507\ttotal: 5.81s\tremaining: 24.3s\n",
      "193:\tlearn: 6.3234126\ttotal: 5.84s\tremaining: 24.3s\n",
      "194:\tlearn: 6.3234094\ttotal: 5.86s\tremaining: 24.2s\n",
      "195:\tlearn: 6.3233472\ttotal: 5.89s\tremaining: 24.1s\n",
      "196:\tlearn: 6.3232376\ttotal: 5.92s\tremaining: 24.1s\n",
      "197:\tlearn: 6.3231497\ttotal: 5.94s\tremaining: 24.1s\n",
      "198:\tlearn: 6.3230907\ttotal: 5.97s\tremaining: 24s\n",
      "199:\tlearn: 6.3229926\ttotal: 6s\tremaining: 24s\n",
      "200:\tlearn: 6.3229162\ttotal: 6.03s\tremaining: 24s\n",
      "201:\tlearn: 6.3228869\ttotal: 6.06s\tremaining: 23.9s\n",
      "202:\tlearn: 6.3228569\ttotal: 6.08s\tremaining: 23.9s\n",
      "203:\tlearn: 6.3227461\ttotal: 6.12s\tremaining: 23.9s\n",
      "204:\tlearn: 6.3227166\ttotal: 6.14s\tremaining: 23.8s\n",
      "205:\tlearn: 6.3226243\ttotal: 6.17s\tremaining: 23.8s\n",
      "206:\tlearn: 6.3225603\ttotal: 6.2s\tremaining: 23.8s\n",
      "207:\tlearn: 6.3224943\ttotal: 6.23s\tremaining: 23.7s\n",
      "208:\tlearn: 6.3223703\ttotal: 6.26s\tremaining: 23.7s\n",
      "209:\tlearn: 6.3222527\ttotal: 6.29s\tremaining: 23.6s\n",
      "210:\tlearn: 6.3221430\ttotal: 6.31s\tremaining: 23.6s\n",
      "211:\tlearn: 6.3219694\ttotal: 6.34s\tremaining: 23.6s\n",
      "212:\tlearn: 6.3218837\ttotal: 6.38s\tremaining: 23.6s\n",
      "213:\tlearn: 6.3218059\ttotal: 6.41s\tremaining: 23.5s\n",
      "214:\tlearn: 6.3216119\ttotal: 6.44s\tremaining: 23.5s\n",
      "215:\tlearn: 6.3214738\ttotal: 6.47s\tremaining: 23.5s\n",
      "216:\tlearn: 6.3213993\ttotal: 6.49s\tremaining: 23.4s\n",
      "217:\tlearn: 6.3212967\ttotal: 6.53s\tremaining: 23.4s\n",
      "218:\tlearn: 6.3211503\ttotal: 6.55s\tremaining: 23.4s\n",
      "219:\tlearn: 6.3210260\ttotal: 6.58s\tremaining: 23.3s\n",
      "220:\tlearn: 6.3209409\ttotal: 6.61s\tremaining: 23.3s\n",
      "221:\tlearn: 6.3208821\ttotal: 6.64s\tremaining: 23.3s\n",
      "222:\tlearn: 6.3208139\ttotal: 6.66s\tremaining: 23.2s\n",
      "223:\tlearn: 6.3207830\ttotal: 6.69s\tremaining: 23.2s\n",
      "224:\tlearn: 6.3206265\ttotal: 6.72s\tremaining: 23.1s\n",
      "225:\tlearn: 6.3205645\ttotal: 6.75s\tremaining: 23.1s\n",
      "226:\tlearn: 6.3204612\ttotal: 6.78s\tremaining: 23.1s\n",
      "227:\tlearn: 6.3203293\ttotal: 6.81s\tremaining: 23.1s\n",
      "228:\tlearn: 6.3203214\ttotal: 6.83s\tremaining: 23s\n",
      "229:\tlearn: 6.3202121\ttotal: 6.86s\tremaining: 23s\n",
      "230:\tlearn: 6.3200885\ttotal: 6.89s\tremaining: 22.9s\n",
      "231:\tlearn: 6.3199638\ttotal: 6.92s\tremaining: 22.9s\n",
      "232:\tlearn: 6.3197889\ttotal: 6.95s\tremaining: 22.9s\n",
      "233:\tlearn: 6.3196842\ttotal: 6.98s\tremaining: 22.8s\n",
      "234:\tlearn: 6.3196571\ttotal: 7.01s\tremaining: 22.8s\n",
      "235:\tlearn: 6.3195407\ttotal: 7.04s\tremaining: 22.8s\n",
      "236:\tlearn: 6.3193521\ttotal: 7.07s\tremaining: 22.8s\n",
      "237:\tlearn: 6.3192067\ttotal: 7.1s\tremaining: 22.7s\n",
      "238:\tlearn: 6.3191270\ttotal: 7.13s\tremaining: 22.7s\n",
      "239:\tlearn: 6.3191011\ttotal: 7.16s\tremaining: 22.7s\n",
      "240:\tlearn: 6.3190050\ttotal: 7.19s\tremaining: 22.6s\n",
      "241:\tlearn: 6.3189558\ttotal: 7.22s\tremaining: 22.6s\n",
      "242:\tlearn: 6.3188653\ttotal: 7.25s\tremaining: 22.6s\n",
      "243:\tlearn: 6.3187527\ttotal: 7.28s\tremaining: 22.6s\n",
      "244:\tlearn: 6.3186659\ttotal: 7.31s\tremaining: 22.5s\n",
      "245:\tlearn: 6.3185599\ttotal: 7.34s\tremaining: 22.5s\n",
      "246:\tlearn: 6.3184150\ttotal: 7.37s\tremaining: 22.5s\n",
      "247:\tlearn: 6.3183060\ttotal: 7.39s\tremaining: 22.4s\n",
      "248:\tlearn: 6.3182282\ttotal: 7.42s\tremaining: 22.4s\n",
      "249:\tlearn: 6.3180804\ttotal: 7.45s\tremaining: 22.3s\n",
      "250:\tlearn: 6.3179022\ttotal: 7.47s\tremaining: 22.3s\n",
      "251:\tlearn: 6.3178411\ttotal: 7.5s\tremaining: 22.3s\n",
      "252:\tlearn: 6.3177212\ttotal: 7.54s\tremaining: 22.3s\n",
      "253:\tlearn: 6.3176236\ttotal: 7.57s\tremaining: 22.2s\n",
      "254:\tlearn: 6.3174924\ttotal: 7.6s\tremaining: 22.2s\n",
      "255:\tlearn: 6.3174143\ttotal: 7.63s\tremaining: 22.2s\n",
      "256:\tlearn: 6.3173749\ttotal: 7.66s\tremaining: 22.1s\n",
      "257:\tlearn: 6.3173191\ttotal: 7.68s\tremaining: 22.1s\n",
      "258:\tlearn: 6.3171721\ttotal: 7.71s\tremaining: 22.1s\n",
      "259:\tlearn: 6.3171440\ttotal: 7.74s\tremaining: 22s\n",
      "260:\tlearn: 6.3170585\ttotal: 7.77s\tremaining: 22s\n",
      "261:\tlearn: 6.3169946\ttotal: 7.79s\tremaining: 21.9s\n",
      "262:\tlearn: 6.3168188\ttotal: 7.82s\tremaining: 21.9s\n",
      "263:\tlearn: 6.3166356\ttotal: 7.85s\tremaining: 21.9s\n",
      "264:\tlearn: 6.3164884\ttotal: 7.88s\tremaining: 21.9s\n",
      "265:\tlearn: 6.3163729\ttotal: 7.91s\tremaining: 21.8s\n",
      "266:\tlearn: 6.3162330\ttotal: 7.94s\tremaining: 21.8s\n",
      "267:\tlearn: 6.3161485\ttotal: 7.97s\tremaining: 21.8s\n",
      "268:\tlearn: 6.3159929\ttotal: 8.01s\tremaining: 21.8s\n",
      "269:\tlearn: 6.3158652\ttotal: 8.04s\tremaining: 21.7s\n",
      "270:\tlearn: 6.3157448\ttotal: 8.06s\tremaining: 21.7s\n",
      "271:\tlearn: 6.3156961\ttotal: 8.09s\tremaining: 21.7s\n",
      "272:\tlearn: 6.3155291\ttotal: 8.12s\tremaining: 21.6s\n",
      "273:\tlearn: 6.3153928\ttotal: 8.15s\tremaining: 21.6s\n",
      "274:\tlearn: 6.3152986\ttotal: 8.18s\tremaining: 21.6s\n",
      "275:\tlearn: 6.3151999\ttotal: 8.21s\tremaining: 21.5s\n",
      "276:\tlearn: 6.3150168\ttotal: 8.24s\tremaining: 21.5s\n",
      "277:\tlearn: 6.3148570\ttotal: 8.27s\tremaining: 21.5s\n",
      "278:\tlearn: 6.3147538\ttotal: 8.3s\tremaining: 21.5s\n",
      "279:\tlearn: 6.3147501\ttotal: 8.32s\tremaining: 21.4s\n",
      "280:\tlearn: 6.3146482\ttotal: 8.35s\tremaining: 21.4s\n",
      "281:\tlearn: 6.3145819\ttotal: 8.38s\tremaining: 21.3s\n",
      "282:\tlearn: 6.3144525\ttotal: 8.4s\tremaining: 21.3s\n",
      "283:\tlearn: 6.3143419\ttotal: 8.43s\tremaining: 21.3s\n",
      "284:\tlearn: 6.3142839\ttotal: 8.46s\tremaining: 21.2s\n",
      "285:\tlearn: 6.3142151\ttotal: 8.49s\tremaining: 21.2s\n",
      "286:\tlearn: 6.3141515\ttotal: 8.52s\tremaining: 21.2s\n",
      "287:\tlearn: 6.3140419\ttotal: 8.55s\tremaining: 21.1s\n",
      "288:\tlearn: 6.3140003\ttotal: 8.58s\tremaining: 21.1s\n",
      "289:\tlearn: 6.3139195\ttotal: 8.61s\tremaining: 21.1s\n",
      "290:\tlearn: 6.3137484\ttotal: 8.64s\tremaining: 21.1s\n",
      "291:\tlearn: 6.3136459\ttotal: 8.67s\tremaining: 21s\n",
      "292:\tlearn: 6.3135688\ttotal: 8.7s\tremaining: 21s\n",
      "293:\tlearn: 6.3134011\ttotal: 8.73s\tremaining: 21s\n",
      "294:\tlearn: 6.3132892\ttotal: 8.76s\tremaining: 20.9s\n",
      "295:\tlearn: 6.3131943\ttotal: 8.79s\tremaining: 20.9s\n",
      "296:\tlearn: 6.3130691\ttotal: 8.81s\tremaining: 20.9s\n",
      "297:\tlearn: 6.3129875\ttotal: 8.84s\tremaining: 20.8s\n",
      "298:\tlearn: 6.3129506\ttotal: 8.87s\tremaining: 20.8s\n",
      "299:\tlearn: 6.3128609\ttotal: 8.9s\tremaining: 20.8s\n",
      "300:\tlearn: 6.3127999\ttotal: 8.93s\tremaining: 20.7s\n",
      "301:\tlearn: 6.3126690\ttotal: 8.96s\tremaining: 20.7s\n",
      "302:\tlearn: 6.3125890\ttotal: 8.99s\tremaining: 20.7s\n",
      "303:\tlearn: 6.3125376\ttotal: 9.02s\tremaining: 20.6s\n",
      "304:\tlearn: 6.3124290\ttotal: 9.05s\tremaining: 20.6s\n",
      "305:\tlearn: 6.3122800\ttotal: 9.07s\tremaining: 20.6s\n",
      "306:\tlearn: 6.3122088\ttotal: 9.1s\tremaining: 20.5s\n",
      "307:\tlearn: 6.3120107\ttotal: 9.13s\tremaining: 20.5s\n",
      "308:\tlearn: 6.3118588\ttotal: 9.16s\tremaining: 20.5s\n",
      "309:\tlearn: 6.3117025\ttotal: 9.19s\tremaining: 20.4s\n",
      "310:\tlearn: 6.3117025\ttotal: 9.2s\tremaining: 20.4s\n",
      "311:\tlearn: 6.3116721\ttotal: 9.23s\tremaining: 20.4s\n",
      "312:\tlearn: 6.3116319\ttotal: 9.26s\tremaining: 20.3s\n",
      "313:\tlearn: 6.3114914\ttotal: 9.28s\tremaining: 20.3s\n",
      "314:\tlearn: 6.3113885\ttotal: 9.32s\tremaining: 20.3s\n",
      "315:\tlearn: 6.3113302\ttotal: 9.35s\tremaining: 20.2s\n",
      "316:\tlearn: 6.3111728\ttotal: 9.38s\tremaining: 20.2s\n",
      "317:\tlearn: 6.3111453\ttotal: 9.41s\tremaining: 20.2s\n",
      "318:\tlearn: 6.3111173\ttotal: 9.43s\tremaining: 20.1s\n",
      "319:\tlearn: 6.3110286\ttotal: 9.46s\tremaining: 20.1s\n",
      "320:\tlearn: 6.3109162\ttotal: 9.48s\tremaining: 20.1s\n",
      "321:\tlearn: 6.3108566\ttotal: 9.51s\tremaining: 20s\n",
      "322:\tlearn: 6.3107350\ttotal: 9.54s\tremaining: 20s\n",
      "323:\tlearn: 6.3106865\ttotal: 9.57s\tremaining: 20s\n",
      "324:\tlearn: 6.3105659\ttotal: 9.6s\tremaining: 19.9s\n",
      "325:\tlearn: 6.3104066\ttotal: 9.63s\tremaining: 19.9s\n",
      "326:\tlearn: 6.3102855\ttotal: 9.65s\tremaining: 19.9s\n",
      "327:\tlearn: 6.3102380\ttotal: 9.68s\tremaining: 19.8s\n",
      "328:\tlearn: 6.3102023\ttotal: 9.71s\tremaining: 19.8s\n",
      "329:\tlearn: 6.3101163\ttotal: 9.74s\tremaining: 19.8s\n",
      "330:\tlearn: 6.3100496\ttotal: 9.77s\tremaining: 19.7s\n",
      "331:\tlearn: 6.3099648\ttotal: 9.8s\tremaining: 19.7s\n",
      "332:\tlearn: 6.3097990\ttotal: 9.83s\tremaining: 19.7s\n",
      "333:\tlearn: 6.3097202\ttotal: 9.86s\tremaining: 19.7s\n",
      "334:\tlearn: 6.3096693\ttotal: 9.88s\tremaining: 19.6s\n",
      "335:\tlearn: 6.3095878\ttotal: 9.92s\tremaining: 19.6s\n",
      "336:\tlearn: 6.3095297\ttotal: 9.95s\tremaining: 19.6s\n",
      "337:\tlearn: 6.3094128\ttotal: 9.98s\tremaining: 19.5s\n",
      "338:\tlearn: 6.3092939\ttotal: 10s\tremaining: 19.5s\n",
      "339:\tlearn: 6.3092009\ttotal: 10s\tremaining: 19.5s\n",
      "340:\tlearn: 6.3091990\ttotal: 10.1s\tremaining: 19.4s\n",
      "341:\tlearn: 6.3091179\ttotal: 10.1s\tremaining: 19.4s\n",
      "342:\tlearn: 6.3090224\ttotal: 10.1s\tremaining: 19.4s\n",
      "343:\tlearn: 6.3089650\ttotal: 10.1s\tremaining: 19.4s\n",
      "344:\tlearn: 6.3088877\ttotal: 10.2s\tremaining: 19.3s\n",
      "345:\tlearn: 6.3088014\ttotal: 10.2s\tremaining: 19.3s\n",
      "346:\tlearn: 6.3086898\ttotal: 10.2s\tremaining: 19.3s\n",
      "347:\tlearn: 6.3085400\ttotal: 10.3s\tremaining: 19.2s\n",
      "348:\tlearn: 6.3085003\ttotal: 10.3s\tremaining: 19.2s\n",
      "349:\tlearn: 6.3084302\ttotal: 10.3s\tremaining: 19.2s\n",
      "350:\tlearn: 6.3083595\ttotal: 10.4s\tremaining: 19.2s\n",
      "351:\tlearn: 6.3083065\ttotal: 10.4s\tremaining: 19.1s\n",
      "352:\tlearn: 6.3081850\ttotal: 10.4s\tremaining: 19.1s\n",
      "353:\tlearn: 6.3081421\ttotal: 10.4s\tremaining: 19.1s\n",
      "354:\tlearn: 6.3080270\ttotal: 10.5s\tremaining: 19s\n",
      "355:\tlearn: 6.3079835\ttotal: 10.5s\tremaining: 19s\n",
      "356:\tlearn: 6.3079826\ttotal: 10.5s\tremaining: 19s\n",
      "357:\tlearn: 6.3079166\ttotal: 10.6s\tremaining: 18.9s\n",
      "358:\tlearn: 6.3077846\ttotal: 10.6s\tremaining: 18.9s\n",
      "359:\tlearn: 6.3077351\ttotal: 10.6s\tremaining: 18.9s\n",
      "360:\tlearn: 6.3076977\ttotal: 10.6s\tremaining: 18.8s\n",
      "361:\tlearn: 6.3075961\ttotal: 10.7s\tremaining: 18.8s\n",
      "362:\tlearn: 6.3075422\ttotal: 10.7s\tremaining: 18.8s\n",
      "363:\tlearn: 6.3074652\ttotal: 10.7s\tremaining: 18.8s\n",
      "364:\tlearn: 6.3073706\ttotal: 10.8s\tremaining: 18.7s\n",
      "365:\tlearn: 6.3072694\ttotal: 10.8s\tremaining: 18.7s\n",
      "366:\tlearn: 6.3071939\ttotal: 10.8s\tremaining: 18.7s\n",
      "367:\tlearn: 6.3071212\ttotal: 10.8s\tremaining: 18.6s\n",
      "368:\tlearn: 6.3070161\ttotal: 10.9s\tremaining: 18.6s\n",
      "369:\tlearn: 6.3070103\ttotal: 10.9s\tremaining: 18.6s\n",
      "370:\tlearn: 6.3069343\ttotal: 10.9s\tremaining: 18.5s\n",
      "371:\tlearn: 6.3068424\ttotal: 11s\tremaining: 18.5s\n",
      "372:\tlearn: 6.3068043\ttotal: 11s\tremaining: 18.5s\n",
      "373:\tlearn: 6.3066919\ttotal: 11s\tremaining: 18.4s\n",
      "374:\tlearn: 6.3066166\ttotal: 11.1s\tremaining: 18.4s\n",
      "375:\tlearn: 6.3064801\ttotal: 11.1s\tremaining: 18.4s\n",
      "376:\tlearn: 6.3064120\ttotal: 11.1s\tremaining: 18.4s\n",
      "377:\tlearn: 6.3063174\ttotal: 11.1s\tremaining: 18.3s\n",
      "378:\tlearn: 6.3062740\ttotal: 11.2s\tremaining: 18.3s\n",
      "379:\tlearn: 6.3062130\ttotal: 11.2s\tremaining: 18.3s\n",
      "380:\tlearn: 6.3061148\ttotal: 11.2s\tremaining: 18.2s\n",
      "381:\tlearn: 6.3060737\ttotal: 11.3s\tremaining: 18.2s\n",
      "382:\tlearn: 6.3059745\ttotal: 11.3s\tremaining: 18.2s\n",
      "383:\tlearn: 6.3058763\ttotal: 11.3s\tremaining: 18.1s\n",
      "384:\tlearn: 6.3057561\ttotal: 11.3s\tremaining: 18.1s\n",
      "385:\tlearn: 6.3057034\ttotal: 11.4s\tremaining: 18.1s\n",
      "386:\tlearn: 6.3056736\ttotal: 11.4s\tremaining: 18.1s\n",
      "387:\tlearn: 6.3056098\ttotal: 11.4s\tremaining: 18s\n",
      "388:\tlearn: 6.3055697\ttotal: 11.5s\tremaining: 18s\n",
      "389:\tlearn: 6.3054235\ttotal: 11.5s\tremaining: 18s\n",
      "390:\tlearn: 6.3053200\ttotal: 11.5s\tremaining: 17.9s\n",
      "391:\tlearn: 6.3052664\ttotal: 11.6s\tremaining: 17.9s\n",
      "392:\tlearn: 6.3051683\ttotal: 11.6s\tremaining: 17.9s\n",
      "393:\tlearn: 6.3050910\ttotal: 11.6s\tremaining: 17.9s\n",
      "394:\tlearn: 6.3050222\ttotal: 11.6s\tremaining: 17.8s\n",
      "395:\tlearn: 6.3048779\ttotal: 11.7s\tremaining: 17.8s\n",
      "396:\tlearn: 6.3048026\ttotal: 11.7s\tremaining: 17.8s\n",
      "397:\tlearn: 6.3047786\ttotal: 11.7s\tremaining: 17.7s\n",
      "398:\tlearn: 6.3046752\ttotal: 11.8s\tremaining: 17.7s\n",
      "399:\tlearn: 6.3046363\ttotal: 11.8s\tremaining: 17.7s\n",
      "400:\tlearn: 6.3045739\ttotal: 11.8s\tremaining: 17.6s\n",
      "401:\tlearn: 6.3044195\ttotal: 11.8s\tremaining: 17.6s\n",
      "402:\tlearn: 6.3043596\ttotal: 11.9s\tremaining: 17.6s\n",
      "403:\tlearn: 6.3042869\ttotal: 11.9s\tremaining: 17.6s\n",
      "404:\tlearn: 6.3041952\ttotal: 11.9s\tremaining: 17.5s\n",
      "405:\tlearn: 6.3041414\ttotal: 12s\tremaining: 17.5s\n",
      "406:\tlearn: 6.3040483\ttotal: 12s\tremaining: 17.5s\n",
      "407:\tlearn: 6.3039566\ttotal: 12s\tremaining: 17.4s\n",
      "408:\tlearn: 6.3038406\ttotal: 12s\tremaining: 17.4s\n",
      "409:\tlearn: 6.3037738\ttotal: 12.1s\tremaining: 17.4s\n",
      "410:\tlearn: 6.3036125\ttotal: 12.1s\tremaining: 17.4s\n",
      "411:\tlearn: 6.3034816\ttotal: 12.1s\tremaining: 17.3s\n",
      "412:\tlearn: 6.3033833\ttotal: 12.2s\tremaining: 17.3s\n",
      "413:\tlearn: 6.3033529\ttotal: 12.2s\tremaining: 17.3s\n",
      "414:\tlearn: 6.3032624\ttotal: 12.2s\tremaining: 17.2s\n",
      "415:\tlearn: 6.3031070\ttotal: 12.3s\tremaining: 17.2s\n",
      "416:\tlearn: 6.3030087\ttotal: 12.3s\tremaining: 17.2s\n",
      "417:\tlearn: 6.3029171\ttotal: 12.3s\tremaining: 17.1s\n",
      "418:\tlearn: 6.3027887\ttotal: 12.3s\tremaining: 17.1s\n",
      "419:\tlearn: 6.3027456\ttotal: 12.4s\tremaining: 17.1s\n",
      "420:\tlearn: 6.3026371\ttotal: 12.4s\tremaining: 17.1s\n",
      "421:\tlearn: 6.3025541\ttotal: 12.4s\tremaining: 17s\n",
      "422:\tlearn: 6.3024695\ttotal: 12.5s\tremaining: 17s\n",
      "423:\tlearn: 6.3023341\ttotal: 12.5s\tremaining: 17s\n",
      "424:\tlearn: 6.3021885\ttotal: 12.5s\tremaining: 16.9s\n",
      "425:\tlearn: 6.3021051\ttotal: 12.6s\tremaining: 16.9s\n",
      "426:\tlearn: 6.3020615\ttotal: 12.6s\tremaining: 16.9s\n",
      "427:\tlearn: 6.3019448\ttotal: 12.6s\tremaining: 16.9s\n",
      "428:\tlearn: 6.3019084\ttotal: 12.6s\tremaining: 16.8s\n",
      "429:\tlearn: 6.3017822\ttotal: 12.7s\tremaining: 16.8s\n",
      "430:\tlearn: 6.3017360\ttotal: 12.7s\tremaining: 16.8s\n",
      "431:\tlearn: 6.3016310\ttotal: 12.7s\tremaining: 16.7s\n",
      "432:\tlearn: 6.3015497\ttotal: 12.8s\tremaining: 16.7s\n",
      "433:\tlearn: 6.3014235\ttotal: 12.8s\tremaining: 16.7s\n",
      "434:\tlearn: 6.3013222\ttotal: 12.8s\tremaining: 16.6s\n",
      "435:\tlearn: 6.3011854\ttotal: 12.8s\tremaining: 16.6s\n",
      "436:\tlearn: 6.3010867\ttotal: 12.9s\tremaining: 16.6s\n",
      "437:\tlearn: 6.3009894\ttotal: 12.9s\tremaining: 16.6s\n",
      "438:\tlearn: 6.3008955\ttotal: 12.9s\tremaining: 16.5s\n",
      "439:\tlearn: 6.3008074\ttotal: 13s\tremaining: 16.5s\n",
      "440:\tlearn: 6.3007314\ttotal: 13s\tremaining: 16.5s\n",
      "441:\tlearn: 6.3006581\ttotal: 13s\tremaining: 16.5s\n",
      "442:\tlearn: 6.3005681\ttotal: 13.1s\tremaining: 16.4s\n",
      "443:\tlearn: 6.3004794\ttotal: 13.1s\tremaining: 16.4s\n",
      "444:\tlearn: 6.3004004\ttotal: 13.1s\tremaining: 16.4s\n",
      "445:\tlearn: 6.3002675\ttotal: 13.2s\tremaining: 16.3s\n",
      "446:\tlearn: 6.3002439\ttotal: 13.2s\tremaining: 16.3s\n",
      "447:\tlearn: 6.3001584\ttotal: 13.2s\tremaining: 16.3s\n",
      "448:\tlearn: 6.3000809\ttotal: 13.2s\tremaining: 16.3s\n",
      "449:\tlearn: 6.2999795\ttotal: 13.3s\tremaining: 16.2s\n",
      "450:\tlearn: 6.2998597\ttotal: 13.3s\tremaining: 16.2s\n",
      "451:\tlearn: 6.2997803\ttotal: 13.3s\tremaining: 16.2s\n",
      "452:\tlearn: 6.2996824\ttotal: 13.4s\tremaining: 16.1s\n",
      "453:\tlearn: 6.2995805\ttotal: 13.4s\tremaining: 16.1s\n",
      "454:\tlearn: 6.2994419\ttotal: 13.4s\tremaining: 16.1s\n",
      "455:\tlearn: 6.2993039\ttotal: 13.5s\tremaining: 16.1s\n",
      "456:\tlearn: 6.2992321\ttotal: 13.5s\tremaining: 16s\n",
      "457:\tlearn: 6.2991701\ttotal: 13.5s\tremaining: 16s\n",
      "458:\tlearn: 6.2990715\ttotal: 13.6s\tremaining: 16s\n",
      "459:\tlearn: 6.2989990\ttotal: 13.6s\tremaining: 15.9s\n",
      "460:\tlearn: 6.2988765\ttotal: 13.6s\tremaining: 15.9s\n",
      "461:\tlearn: 6.2988326\ttotal: 13.6s\tremaining: 15.9s\n",
      "462:\tlearn: 6.2987431\ttotal: 13.7s\tremaining: 15.9s\n",
      "463:\tlearn: 6.2986545\ttotal: 13.7s\tremaining: 15.8s\n",
      "464:\tlearn: 6.2985633\ttotal: 13.7s\tremaining: 15.8s\n",
      "465:\tlearn: 6.2984776\ttotal: 13.8s\tremaining: 15.8s\n",
      "466:\tlearn: 6.2983870\ttotal: 13.8s\tremaining: 15.8s\n",
      "467:\tlearn: 6.2983172\ttotal: 13.8s\tremaining: 15.7s\n",
      "468:\tlearn: 6.2982089\ttotal: 13.9s\tremaining: 15.7s\n",
      "469:\tlearn: 6.2980886\ttotal: 13.9s\tremaining: 15.7s\n",
      "470:\tlearn: 6.2979684\ttotal: 13.9s\tremaining: 15.7s\n",
      "471:\tlearn: 6.2978366\ttotal: 14s\tremaining: 15.6s\n",
      "472:\tlearn: 6.2977494\ttotal: 14s\tremaining: 15.6s\n",
      "473:\tlearn: 6.2976879\ttotal: 14s\tremaining: 15.6s\n",
      "474:\tlearn: 6.2976015\ttotal: 14.1s\tremaining: 15.6s\n",
      "475:\tlearn: 6.2975419\ttotal: 14.1s\tremaining: 15.5s\n",
      "476:\tlearn: 6.2974385\ttotal: 14.1s\tremaining: 15.5s\n",
      "477:\tlearn: 6.2972815\ttotal: 14.2s\tremaining: 15.5s\n",
      "478:\tlearn: 6.2971423\ttotal: 14.2s\tremaining: 15.4s\n",
      "479:\tlearn: 6.2970507\ttotal: 14.2s\tremaining: 15.4s\n",
      "480:\tlearn: 6.2969294\ttotal: 14.3s\tremaining: 15.4s\n",
      "481:\tlearn: 6.2968325\ttotal: 14.3s\tremaining: 15.4s\n",
      "482:\tlearn: 6.2967537\ttotal: 14.3s\tremaining: 15.3s\n",
      "483:\tlearn: 6.2966570\ttotal: 14.3s\tremaining: 15.3s\n",
      "484:\tlearn: 6.2964908\ttotal: 14.4s\tremaining: 15.3s\n",
      "485:\tlearn: 6.2964204\ttotal: 14.4s\tremaining: 15.2s\n",
      "486:\tlearn: 6.2963216\ttotal: 14.4s\tremaining: 15.2s\n",
      "487:\tlearn: 6.2961897\ttotal: 14.5s\tremaining: 15.2s\n",
      "488:\tlearn: 6.2960621\ttotal: 14.5s\tremaining: 15.2s\n",
      "489:\tlearn: 6.2958944\ttotal: 14.5s\tremaining: 15.1s\n",
      "490:\tlearn: 6.2958222\ttotal: 14.6s\tremaining: 15.1s\n",
      "491:\tlearn: 6.2956766\ttotal: 14.6s\tremaining: 15.1s\n",
      "492:\tlearn: 6.2955854\ttotal: 14.6s\tremaining: 15s\n",
      "493:\tlearn: 6.2954324\ttotal: 14.7s\tremaining: 15s\n",
      "494:\tlearn: 6.2953710\ttotal: 14.7s\tremaining: 15s\n",
      "495:\tlearn: 6.2953177\ttotal: 14.7s\tremaining: 15s\n",
      "496:\tlearn: 6.2952198\ttotal: 14.8s\tremaining: 14.9s\n",
      "497:\tlearn: 6.2951609\ttotal: 14.8s\tremaining: 14.9s\n",
      "498:\tlearn: 6.2950868\ttotal: 14.8s\tremaining: 14.9s\n",
      "499:\tlearn: 6.2950367\ttotal: 14.8s\tremaining: 14.8s\n",
      "500:\tlearn: 6.2949435\ttotal: 14.9s\tremaining: 14.8s\n",
      "501:\tlearn: 6.2948266\ttotal: 14.9s\tremaining: 14.8s\n",
      "502:\tlearn: 6.2947368\ttotal: 14.9s\tremaining: 14.8s\n",
      "503:\tlearn: 6.2946248\ttotal: 15s\tremaining: 14.7s\n",
      "504:\tlearn: 6.2945699\ttotal: 15s\tremaining: 14.7s\n",
      "505:\tlearn: 6.2944285\ttotal: 15s\tremaining: 14.7s\n",
      "506:\tlearn: 6.2943193\ttotal: 15.1s\tremaining: 14.6s\n",
      "507:\tlearn: 6.2942169\ttotal: 15.1s\tremaining: 14.6s\n",
      "508:\tlearn: 6.2941010\ttotal: 15.1s\tremaining: 14.6s\n",
      "509:\tlearn: 6.2940444\ttotal: 15.2s\tremaining: 14.6s\n",
      "510:\tlearn: 6.2938726\ttotal: 15.2s\tremaining: 14.5s\n",
      "511:\tlearn: 6.2938133\ttotal: 15.2s\tremaining: 14.5s\n",
      "512:\tlearn: 6.2936981\ttotal: 15.3s\tremaining: 14.5s\n",
      "513:\tlearn: 6.2936198\ttotal: 15.3s\tremaining: 14.4s\n",
      "514:\tlearn: 6.2935536\ttotal: 15.3s\tremaining: 14.4s\n",
      "515:\tlearn: 6.2934323\ttotal: 15.3s\tremaining: 14.4s\n",
      "516:\tlearn: 6.2933306\ttotal: 15.4s\tremaining: 14.4s\n",
      "517:\tlearn: 6.2932006\ttotal: 15.4s\tremaining: 14.3s\n",
      "518:\tlearn: 6.2931288\ttotal: 15.4s\tremaining: 14.3s\n",
      "519:\tlearn: 6.2930866\ttotal: 15.5s\tremaining: 14.3s\n",
      "520:\tlearn: 6.2929500\ttotal: 15.5s\tremaining: 14.2s\n",
      "521:\tlearn: 6.2928650\ttotal: 15.5s\tremaining: 14.2s\n",
      "522:\tlearn: 6.2927530\ttotal: 15.6s\tremaining: 14.2s\n",
      "523:\tlearn: 6.2927238\ttotal: 15.6s\tremaining: 14.2s\n",
      "524:\tlearn: 6.2926164\ttotal: 15.6s\tremaining: 14.1s\n",
      "525:\tlearn: 6.2924970\ttotal: 15.6s\tremaining: 14.1s\n",
      "526:\tlearn: 6.2923959\ttotal: 15.7s\tremaining: 14.1s\n",
      "527:\tlearn: 6.2922346\ttotal: 15.7s\tremaining: 14s\n",
      "528:\tlearn: 6.2922021\ttotal: 15.7s\tremaining: 14s\n",
      "529:\tlearn: 6.2921509\ttotal: 15.8s\tremaining: 14s\n",
      "530:\tlearn: 6.2920381\ttotal: 15.8s\tremaining: 14s\n",
      "531:\tlearn: 6.2918533\ttotal: 15.8s\tremaining: 13.9s\n",
      "532:\tlearn: 6.2917563\ttotal: 15.9s\tremaining: 13.9s\n",
      "533:\tlearn: 6.2916893\ttotal: 15.9s\tremaining: 13.9s\n",
      "534:\tlearn: 6.2916286\ttotal: 15.9s\tremaining: 13.8s\n",
      "535:\tlearn: 6.2915140\ttotal: 16s\tremaining: 13.8s\n",
      "536:\tlearn: 6.2914386\ttotal: 16s\tremaining: 13.8s\n",
      "537:\tlearn: 6.2913254\ttotal: 16s\tremaining: 13.8s\n",
      "538:\tlearn: 6.2912339\ttotal: 16.1s\tremaining: 13.7s\n",
      "539:\tlearn: 6.2911199\ttotal: 16.1s\tremaining: 13.7s\n",
      "540:\tlearn: 6.2910175\ttotal: 16.1s\tremaining: 13.7s\n",
      "541:\tlearn: 6.2909551\ttotal: 16.1s\tremaining: 13.6s\n",
      "542:\tlearn: 6.2908608\ttotal: 16.2s\tremaining: 13.6s\n",
      "543:\tlearn: 6.2907055\ttotal: 16.2s\tremaining: 13.6s\n",
      "544:\tlearn: 6.2906289\ttotal: 16.2s\tremaining: 13.5s\n",
      "545:\tlearn: 6.2904979\ttotal: 16.3s\tremaining: 13.5s\n",
      "546:\tlearn: 6.2904247\ttotal: 16.3s\tremaining: 13.5s\n",
      "547:\tlearn: 6.2903591\ttotal: 16.3s\tremaining: 13.5s\n",
      "548:\tlearn: 6.2901858\ttotal: 16.3s\tremaining: 13.4s\n",
      "549:\tlearn: 6.2900807\ttotal: 16.4s\tremaining: 13.4s\n",
      "550:\tlearn: 6.2899159\ttotal: 16.4s\tremaining: 13.4s\n",
      "551:\tlearn: 6.2898606\ttotal: 16.4s\tremaining: 13.3s\n",
      "552:\tlearn: 6.2897299\ttotal: 16.5s\tremaining: 13.3s\n",
      "553:\tlearn: 6.2896339\ttotal: 16.5s\tremaining: 13.3s\n",
      "554:\tlearn: 6.2894923\ttotal: 16.5s\tremaining: 13.2s\n",
      "555:\tlearn: 6.2894305\ttotal: 16.6s\tremaining: 13.2s\n",
      "556:\tlearn: 6.2892984\ttotal: 16.6s\tremaining: 13.2s\n",
      "557:\tlearn: 6.2892062\ttotal: 16.6s\tremaining: 13.2s\n",
      "558:\tlearn: 6.2890938\ttotal: 16.6s\tremaining: 13.1s\n",
      "559:\tlearn: 6.2889291\ttotal: 16.7s\tremaining: 13.1s\n",
      "560:\tlearn: 6.2888492\ttotal: 16.7s\tremaining: 13.1s\n",
      "561:\tlearn: 6.2886628\ttotal: 16.7s\tremaining: 13s\n",
      "562:\tlearn: 6.2885480\ttotal: 16.8s\tremaining: 13s\n",
      "563:\tlearn: 6.2884345\ttotal: 16.8s\tremaining: 13s\n",
      "564:\tlearn: 6.2882943\ttotal: 16.8s\tremaining: 13s\n",
      "565:\tlearn: 6.2881552\ttotal: 16.9s\tremaining: 12.9s\n",
      "566:\tlearn: 6.2880804\ttotal: 16.9s\tremaining: 12.9s\n",
      "567:\tlearn: 6.2879247\ttotal: 16.9s\tremaining: 12.9s\n",
      "568:\tlearn: 6.2878940\ttotal: 16.9s\tremaining: 12.8s\n",
      "569:\tlearn: 6.2877812\ttotal: 17s\tremaining: 12.8s\n",
      "570:\tlearn: 6.2876964\ttotal: 17s\tremaining: 12.8s\n",
      "571:\tlearn: 6.2876510\ttotal: 17s\tremaining: 12.8s\n",
      "572:\tlearn: 6.2875341\ttotal: 17.1s\tremaining: 12.7s\n",
      "573:\tlearn: 6.2873831\ttotal: 17.1s\tremaining: 12.7s\n",
      "574:\tlearn: 6.2873015\ttotal: 17.1s\tremaining: 12.7s\n",
      "575:\tlearn: 6.2872167\ttotal: 17.2s\tremaining: 12.6s\n",
      "576:\tlearn: 6.2871345\ttotal: 17.2s\tremaining: 12.6s\n",
      "577:\tlearn: 6.2870156\ttotal: 17.2s\tremaining: 12.6s\n",
      "578:\tlearn: 6.2869418\ttotal: 17.3s\tremaining: 12.6s\n",
      "579:\tlearn: 6.2868206\ttotal: 17.3s\tremaining: 12.5s\n",
      "580:\tlearn: 6.2867366\ttotal: 17.3s\tremaining: 12.5s\n",
      "581:\tlearn: 6.2866553\ttotal: 17.4s\tremaining: 12.5s\n",
      "582:\tlearn: 6.2865445\ttotal: 17.4s\tremaining: 12.4s\n",
      "583:\tlearn: 6.2864312\ttotal: 17.4s\tremaining: 12.4s\n",
      "584:\tlearn: 6.2863117\ttotal: 17.5s\tremaining: 12.4s\n",
      "585:\tlearn: 6.2861670\ttotal: 17.5s\tremaining: 12.4s\n",
      "586:\tlearn: 6.2860305\ttotal: 17.5s\tremaining: 12.3s\n",
      "587:\tlearn: 6.2859397\ttotal: 17.5s\tremaining: 12.3s\n",
      "588:\tlearn: 6.2859019\ttotal: 17.6s\tremaining: 12.3s\n",
      "589:\tlearn: 6.2858178\ttotal: 17.6s\tremaining: 12.2s\n",
      "590:\tlearn: 6.2857036\ttotal: 17.6s\tremaining: 12.2s\n",
      "591:\tlearn: 6.2855756\ttotal: 17.7s\tremaining: 12.2s\n",
      "592:\tlearn: 6.2854237\ttotal: 17.7s\tremaining: 12.2s\n",
      "593:\tlearn: 6.2853576\ttotal: 17.7s\tremaining: 12.1s\n",
      "594:\tlearn: 6.2852294\ttotal: 17.8s\tremaining: 12.1s\n",
      "595:\tlearn: 6.2851218\ttotal: 17.8s\tremaining: 12.1s\n",
      "596:\tlearn: 6.2850471\ttotal: 17.8s\tremaining: 12s\n",
      "597:\tlearn: 6.2849666\ttotal: 17.9s\tremaining: 12s\n",
      "598:\tlearn: 6.2848198\ttotal: 17.9s\tremaining: 12s\n",
      "599:\tlearn: 6.2847548\ttotal: 17.9s\tremaining: 11.9s\n",
      "600:\tlearn: 6.2847238\ttotal: 18s\tremaining: 11.9s\n",
      "601:\tlearn: 6.2846024\ttotal: 18s\tremaining: 11.9s\n",
      "602:\tlearn: 6.2844883\ttotal: 18s\tremaining: 11.9s\n",
      "603:\tlearn: 6.2843747\ttotal: 18s\tremaining: 11.8s\n",
      "604:\tlearn: 6.2842740\ttotal: 18.1s\tremaining: 11.8s\n",
      "605:\tlearn: 6.2841625\ttotal: 18.1s\tremaining: 11.8s\n",
      "606:\tlearn: 6.2841179\ttotal: 18.1s\tremaining: 11.7s\n",
      "607:\tlearn: 6.2840481\ttotal: 18.2s\tremaining: 11.7s\n",
      "608:\tlearn: 6.2839103\ttotal: 18.2s\tremaining: 11.7s\n",
      "609:\tlearn: 6.2837844\ttotal: 18.2s\tremaining: 11.7s\n",
      "610:\tlearn: 6.2837171\ttotal: 18.3s\tremaining: 11.6s\n",
      "611:\tlearn: 6.2836592\ttotal: 18.3s\tremaining: 11.6s\n",
      "612:\tlearn: 6.2835487\ttotal: 18.3s\tremaining: 11.6s\n",
      "613:\tlearn: 6.2834375\ttotal: 18.3s\tremaining: 11.5s\n",
      "614:\tlearn: 6.2832878\ttotal: 18.4s\tremaining: 11.5s\n",
      "615:\tlearn: 6.2831603\ttotal: 18.4s\tremaining: 11.5s\n",
      "616:\tlearn: 6.2830500\ttotal: 18.4s\tremaining: 11.4s\n",
      "617:\tlearn: 6.2829304\ttotal: 18.5s\tremaining: 11.4s\n",
      "618:\tlearn: 6.2828492\ttotal: 18.5s\tremaining: 11.4s\n",
      "619:\tlearn: 6.2827880\ttotal: 18.5s\tremaining: 11.4s\n",
      "620:\tlearn: 6.2826778\ttotal: 18.6s\tremaining: 11.3s\n",
      "621:\tlearn: 6.2825742\ttotal: 18.6s\tremaining: 11.3s\n",
      "622:\tlearn: 6.2824711\ttotal: 18.6s\tremaining: 11.3s\n",
      "623:\tlearn: 6.2823732\ttotal: 18.7s\tremaining: 11.2s\n",
      "624:\tlearn: 6.2822721\ttotal: 18.7s\tremaining: 11.2s\n",
      "625:\tlearn: 6.2821733\ttotal: 18.7s\tremaining: 11.2s\n",
      "626:\tlearn: 6.2821303\ttotal: 18.7s\tremaining: 11.2s\n",
      "627:\tlearn: 6.2820180\ttotal: 18.8s\tremaining: 11.1s\n",
      "628:\tlearn: 6.2819878\ttotal: 18.8s\tremaining: 11.1s\n",
      "629:\tlearn: 6.2819282\ttotal: 18.8s\tremaining: 11.1s\n",
      "630:\tlearn: 6.2818442\ttotal: 18.9s\tremaining: 11s\n",
      "631:\tlearn: 6.2817337\ttotal: 18.9s\tremaining: 11s\n",
      "632:\tlearn: 6.2816548\ttotal: 18.9s\tremaining: 11s\n",
      "633:\tlearn: 6.2816074\ttotal: 19s\tremaining: 10.9s\n",
      "634:\tlearn: 6.2815000\ttotal: 19s\tremaining: 10.9s\n",
      "635:\tlearn: 6.2813486\ttotal: 19s\tremaining: 10.9s\n",
      "636:\tlearn: 6.2812403\ttotal: 19.1s\tremaining: 10.9s\n",
      "637:\tlearn: 6.2812032\ttotal: 19.1s\tremaining: 10.8s\n",
      "638:\tlearn: 6.2811315\ttotal: 19.1s\tremaining: 10.8s\n",
      "639:\tlearn: 6.2810242\ttotal: 19.1s\tremaining: 10.8s\n",
      "640:\tlearn: 6.2809238\ttotal: 19.2s\tremaining: 10.7s\n",
      "641:\tlearn: 6.2807816\ttotal: 19.2s\tremaining: 10.7s\n",
      "642:\tlearn: 6.2806586\ttotal: 19.2s\tremaining: 10.7s\n",
      "643:\tlearn: 6.2805396\ttotal: 19.3s\tremaining: 10.7s\n",
      "644:\tlearn: 6.2804552\ttotal: 19.3s\tremaining: 10.6s\n",
      "645:\tlearn: 6.2803519\ttotal: 19.3s\tremaining: 10.6s\n",
      "646:\tlearn: 6.2802669\ttotal: 19.4s\tremaining: 10.6s\n",
      "647:\tlearn: 6.2801690\ttotal: 19.4s\tremaining: 10.5s\n",
      "648:\tlearn: 6.2800636\ttotal: 19.4s\tremaining: 10.5s\n",
      "649:\tlearn: 6.2800018\ttotal: 19.5s\tremaining: 10.5s\n",
      "650:\tlearn: 6.2799037\ttotal: 19.5s\tremaining: 10.4s\n",
      "651:\tlearn: 6.2798302\ttotal: 19.5s\tremaining: 10.4s\n",
      "652:\tlearn: 6.2797353\ttotal: 19.6s\tremaining: 10.4s\n",
      "653:\tlearn: 6.2796468\ttotal: 19.6s\tremaining: 10.4s\n",
      "654:\tlearn: 6.2795872\ttotal: 19.6s\tremaining: 10.3s\n",
      "655:\tlearn: 6.2794765\ttotal: 19.6s\tremaining: 10.3s\n",
      "656:\tlearn: 6.2793687\ttotal: 19.7s\tremaining: 10.3s\n",
      "657:\tlearn: 6.2792795\ttotal: 19.7s\tremaining: 10.2s\n",
      "658:\tlearn: 6.2792547\ttotal: 19.7s\tremaining: 10.2s\n",
      "659:\tlearn: 6.2791893\ttotal: 19.8s\tremaining: 10.2s\n",
      "660:\tlearn: 6.2790563\ttotal: 19.8s\tremaining: 10.2s\n",
      "661:\tlearn: 6.2789815\ttotal: 19.8s\tremaining: 10.1s\n",
      "662:\tlearn: 6.2789123\ttotal: 19.9s\tremaining: 10.1s\n",
      "663:\tlearn: 6.2788092\ttotal: 19.9s\tremaining: 10.1s\n",
      "664:\tlearn: 6.2787627\ttotal: 19.9s\tremaining: 10s\n",
      "665:\tlearn: 6.2786355\ttotal: 20s\tremaining: 10s\n",
      "666:\tlearn: 6.2785886\ttotal: 20s\tremaining: 9.97s\n",
      "667:\tlearn: 6.2784746\ttotal: 20s\tremaining: 9.95s\n",
      "668:\tlearn: 6.2784289\ttotal: 20s\tremaining: 9.92s\n",
      "669:\tlearn: 6.2783750\ttotal: 20.1s\tremaining: 9.89s\n",
      "670:\tlearn: 6.2783010\ttotal: 20.1s\tremaining: 9.86s\n",
      "671:\tlearn: 6.2782628\ttotal: 20.1s\tremaining: 9.83s\n",
      "672:\tlearn: 6.2782461\ttotal: 20.2s\tremaining: 9.8s\n",
      "673:\tlearn: 6.2781270\ttotal: 20.2s\tremaining: 9.77s\n",
      "674:\tlearn: 6.2780565\ttotal: 20.2s\tremaining: 9.74s\n",
      "675:\tlearn: 6.2779670\ttotal: 20.3s\tremaining: 9.71s\n",
      "676:\tlearn: 6.2779473\ttotal: 20.3s\tremaining: 9.68s\n",
      "677:\tlearn: 6.2778316\ttotal: 20.3s\tremaining: 9.64s\n",
      "678:\tlearn: 6.2777398\ttotal: 20.3s\tremaining: 9.62s\n",
      "679:\tlearn: 6.2776031\ttotal: 20.4s\tremaining: 9.59s\n",
      "680:\tlearn: 6.2774643\ttotal: 20.4s\tremaining: 9.56s\n",
      "681:\tlearn: 6.2773398\ttotal: 20.4s\tremaining: 9.53s\n",
      "682:\tlearn: 6.2772568\ttotal: 20.5s\tremaining: 9.5s\n",
      "683:\tlearn: 6.2770969\ttotal: 20.5s\tremaining: 9.47s\n",
      "684:\tlearn: 6.2770275\ttotal: 20.5s\tremaining: 9.44s\n",
      "685:\tlearn: 6.2769800\ttotal: 20.6s\tremaining: 9.41s\n",
      "686:\tlearn: 6.2769452\ttotal: 20.6s\tremaining: 9.38s\n",
      "687:\tlearn: 6.2768622\ttotal: 20.6s\tremaining: 9.35s\n",
      "688:\tlearn: 6.2767483\ttotal: 20.6s\tremaining: 9.32s\n",
      "689:\tlearn: 6.2766778\ttotal: 20.7s\tremaining: 9.29s\n",
      "690:\tlearn: 6.2765686\ttotal: 20.7s\tremaining: 9.26s\n",
      "691:\tlearn: 6.2765042\ttotal: 20.7s\tremaining: 9.23s\n",
      "692:\tlearn: 6.2764088\ttotal: 20.8s\tremaining: 9.2s\n",
      "693:\tlearn: 6.2763351\ttotal: 20.8s\tremaining: 9.17s\n",
      "694:\tlearn: 6.2762681\ttotal: 20.8s\tremaining: 9.14s\n",
      "695:\tlearn: 6.2761564\ttotal: 20.9s\tremaining: 9.11s\n",
      "696:\tlearn: 6.2761234\ttotal: 20.9s\tremaining: 9.08s\n",
      "697:\tlearn: 6.2760597\ttotal: 20.9s\tremaining: 9.05s\n",
      "698:\tlearn: 6.2759763\ttotal: 21s\tremaining: 9.02s\n",
      "699:\tlearn: 6.2759283\ttotal: 21s\tremaining: 8.99s\n",
      "700:\tlearn: 6.2758456\ttotal: 21s\tremaining: 8.96s\n",
      "701:\tlearn: 6.2757646\ttotal: 21s\tremaining: 8.93s\n",
      "702:\tlearn: 6.2756997\ttotal: 21.1s\tremaining: 8.91s\n",
      "703:\tlearn: 6.2756275\ttotal: 21.1s\tremaining: 8.88s\n",
      "704:\tlearn: 6.2754752\ttotal: 21.1s\tremaining: 8.85s\n",
      "705:\tlearn: 6.2753859\ttotal: 21.2s\tremaining: 8.82s\n",
      "706:\tlearn: 6.2753140\ttotal: 21.2s\tremaining: 8.79s\n",
      "707:\tlearn: 6.2752188\ttotal: 21.2s\tremaining: 8.76s\n",
      "708:\tlearn: 6.2751146\ttotal: 21.3s\tremaining: 8.73s\n",
      "709:\tlearn: 6.2750530\ttotal: 21.3s\tremaining: 8.7s\n",
      "710:\tlearn: 6.2749801\ttotal: 21.3s\tremaining: 8.67s\n",
      "711:\tlearn: 6.2749173\ttotal: 21.4s\tremaining: 8.64s\n",
      "712:\tlearn: 6.2748673\ttotal: 21.4s\tremaining: 8.61s\n",
      "713:\tlearn: 6.2747948\ttotal: 21.4s\tremaining: 8.58s\n",
      "714:\tlearn: 6.2747409\ttotal: 21.4s\tremaining: 8.55s\n",
      "715:\tlearn: 6.2746943\ttotal: 21.5s\tremaining: 8.52s\n",
      "716:\tlearn: 6.2746496\ttotal: 21.5s\tremaining: 8.48s\n",
      "717:\tlearn: 6.2745682\ttotal: 21.5s\tremaining: 8.46s\n",
      "718:\tlearn: 6.2744751\ttotal: 21.6s\tremaining: 8.43s\n",
      "719:\tlearn: 6.2744475\ttotal: 21.6s\tremaining: 8.39s\n",
      "720:\tlearn: 6.2744206\ttotal: 21.6s\tremaining: 8.36s\n",
      "721:\tlearn: 6.2743367\ttotal: 21.6s\tremaining: 8.33s\n",
      "722:\tlearn: 6.2742569\ttotal: 21.7s\tremaining: 8.3s\n",
      "723:\tlearn: 6.2741583\ttotal: 21.7s\tremaining: 8.27s\n",
      "724:\tlearn: 6.2740658\ttotal: 21.7s\tremaining: 8.24s\n",
      "725:\tlearn: 6.2739608\ttotal: 21.8s\tremaining: 8.21s\n",
      "726:\tlearn: 6.2738766\ttotal: 21.8s\tremaining: 8.19s\n",
      "727:\tlearn: 6.2738134\ttotal: 21.8s\tremaining: 8.15s\n",
      "728:\tlearn: 6.2737395\ttotal: 21.9s\tremaining: 8.13s\n",
      "729:\tlearn: 6.2736267\ttotal: 21.9s\tremaining: 8.1s\n",
      "730:\tlearn: 6.2735339\ttotal: 21.9s\tremaining: 8.07s\n",
      "731:\tlearn: 6.2734053\ttotal: 22s\tremaining: 8.04s\n",
      "732:\tlearn: 6.2733415\ttotal: 22s\tremaining: 8.01s\n",
      "733:\tlearn: 6.2732581\ttotal: 22s\tremaining: 7.98s\n",
      "734:\tlearn: 6.2731624\ttotal: 22s\tremaining: 7.95s\n",
      "735:\tlearn: 6.2730867\ttotal: 22.1s\tremaining: 7.92s\n",
      "736:\tlearn: 6.2730258\ttotal: 22.1s\tremaining: 7.89s\n",
      "737:\tlearn: 6.2729779\ttotal: 22.1s\tremaining: 7.86s\n",
      "738:\tlearn: 6.2728785\ttotal: 22.2s\tremaining: 7.83s\n",
      "739:\tlearn: 6.2727658\ttotal: 22.2s\tremaining: 7.8s\n",
      "740:\tlearn: 6.2727367\ttotal: 22.2s\tremaining: 7.77s\n",
      "741:\tlearn: 6.2726581\ttotal: 22.3s\tremaining: 7.74s\n",
      "742:\tlearn: 6.2725967\ttotal: 22.3s\tremaining: 7.71s\n",
      "743:\tlearn: 6.2725367\ttotal: 22.3s\tremaining: 7.68s\n",
      "744:\tlearn: 6.2724367\ttotal: 22.3s\tremaining: 7.65s\n",
      "745:\tlearn: 6.2723830\ttotal: 22.4s\tremaining: 7.62s\n",
      "746:\tlearn: 6.2722683\ttotal: 22.4s\tremaining: 7.59s\n",
      "747:\tlearn: 6.2722387\ttotal: 22.4s\tremaining: 7.56s\n",
      "748:\tlearn: 6.2721539\ttotal: 22.5s\tremaining: 7.53s\n",
      "749:\tlearn: 6.2719901\ttotal: 22.5s\tremaining: 7.5s\n",
      "750:\tlearn: 6.2718881\ttotal: 22.5s\tremaining: 7.47s\n",
      "751:\tlearn: 6.2717972\ttotal: 22.6s\tremaining: 7.44s\n",
      "752:\tlearn: 6.2717635\ttotal: 22.6s\tremaining: 7.41s\n",
      "753:\tlearn: 6.2717139\ttotal: 22.6s\tremaining: 7.38s\n",
      "754:\tlearn: 6.2716387\ttotal: 22.7s\tremaining: 7.35s\n",
      "755:\tlearn: 6.2715678\ttotal: 22.7s\tremaining: 7.32s\n",
      "756:\tlearn: 6.2714818\ttotal: 22.7s\tremaining: 7.29s\n",
      "757:\tlearn: 6.2713187\ttotal: 22.8s\tremaining: 7.26s\n",
      "758:\tlearn: 6.2712642\ttotal: 22.8s\tremaining: 7.24s\n",
      "759:\tlearn: 6.2712075\ttotal: 22.8s\tremaining: 7.21s\n",
      "760:\tlearn: 6.2711449\ttotal: 22.8s\tremaining: 7.17s\n",
      "761:\tlearn: 6.2710852\ttotal: 22.9s\tremaining: 7.14s\n",
      "762:\tlearn: 6.2710542\ttotal: 22.9s\tremaining: 7.12s\n",
      "763:\tlearn: 6.2709934\ttotal: 22.9s\tremaining: 7.08s\n",
      "764:\tlearn: 6.2709118\ttotal: 23s\tremaining: 7.05s\n",
      "765:\tlearn: 6.2708281\ttotal: 23s\tremaining: 7.02s\n",
      "766:\tlearn: 6.2707186\ttotal: 23s\tremaining: 6.99s\n",
      "767:\tlearn: 6.2706130\ttotal: 23.1s\tremaining: 6.96s\n",
      "768:\tlearn: 6.2705543\ttotal: 23.1s\tremaining: 6.93s\n",
      "769:\tlearn: 6.2704965\ttotal: 23.1s\tremaining: 6.9s\n",
      "770:\tlearn: 6.2704401\ttotal: 23.1s\tremaining: 6.87s\n",
      "771:\tlearn: 6.2704081\ttotal: 23.2s\tremaining: 6.84s\n",
      "772:\tlearn: 6.2703431\ttotal: 23.2s\tremaining: 6.81s\n",
      "773:\tlearn: 6.2702459\ttotal: 23.2s\tremaining: 6.78s\n",
      "774:\tlearn: 6.2701146\ttotal: 23.3s\tremaining: 6.75s\n",
      "775:\tlearn: 6.2700717\ttotal: 23.3s\tremaining: 6.72s\n",
      "776:\tlearn: 6.2699988\ttotal: 23.3s\tremaining: 6.7s\n",
      "777:\tlearn: 6.2699112\ttotal: 23.4s\tremaining: 6.67s\n",
      "778:\tlearn: 6.2698364\ttotal: 23.4s\tremaining: 6.64s\n",
      "779:\tlearn: 6.2697548\ttotal: 23.4s\tremaining: 6.61s\n",
      "780:\tlearn: 6.2697131\ttotal: 23.5s\tremaining: 6.58s\n",
      "781:\tlearn: 6.2696448\ttotal: 23.5s\tremaining: 6.55s\n",
      "782:\tlearn: 6.2695884\ttotal: 23.5s\tremaining: 6.52s\n",
      "783:\tlearn: 6.2695153\ttotal: 23.6s\tremaining: 6.49s\n",
      "784:\tlearn: 6.2694649\ttotal: 23.6s\tremaining: 6.46s\n",
      "785:\tlearn: 6.2693830\ttotal: 23.6s\tremaining: 6.43s\n",
      "786:\tlearn: 6.2692923\ttotal: 23.6s\tremaining: 6.4s\n",
      "787:\tlearn: 6.2692241\ttotal: 23.7s\tremaining: 6.37s\n",
      "788:\tlearn: 6.2691867\ttotal: 23.7s\tremaining: 6.34s\n",
      "789:\tlearn: 6.2691190\ttotal: 23.7s\tremaining: 6.31s\n",
      "790:\tlearn: 6.2690449\ttotal: 23.8s\tremaining: 6.28s\n",
      "791:\tlearn: 6.2689399\ttotal: 23.8s\tremaining: 6.25s\n",
      "792:\tlearn: 6.2688912\ttotal: 23.8s\tremaining: 6.22s\n",
      "793:\tlearn: 6.2688252\ttotal: 23.9s\tremaining: 6.19s\n",
      "794:\tlearn: 6.2687469\ttotal: 23.9s\tremaining: 6.16s\n",
      "795:\tlearn: 6.2686980\ttotal: 23.9s\tremaining: 6.13s\n",
      "796:\tlearn: 6.2686351\ttotal: 23.9s\tremaining: 6.1s\n",
      "797:\tlearn: 6.2685475\ttotal: 24s\tremaining: 6.07s\n",
      "798:\tlearn: 6.2684344\ttotal: 24s\tremaining: 6.04s\n",
      "799:\tlearn: 6.2684124\ttotal: 24s\tremaining: 6.01s\n",
      "800:\tlearn: 6.2683021\ttotal: 24.1s\tremaining: 5.98s\n",
      "801:\tlearn: 6.2681917\ttotal: 24.1s\tremaining: 5.95s\n",
      "802:\tlearn: 6.2681144\ttotal: 24.1s\tremaining: 5.92s\n",
      "803:\tlearn: 6.2680721\ttotal: 24.2s\tremaining: 5.89s\n",
      "804:\tlearn: 6.2680158\ttotal: 24.2s\tremaining: 5.86s\n",
      "805:\tlearn: 6.2679124\ttotal: 24.2s\tremaining: 5.83s\n",
      "806:\tlearn: 6.2678605\ttotal: 24.3s\tremaining: 5.8s\n",
      "807:\tlearn: 6.2678071\ttotal: 24.3s\tremaining: 5.77s\n",
      "808:\tlearn: 6.2677119\ttotal: 24.3s\tremaining: 5.74s\n",
      "809:\tlearn: 6.2676132\ttotal: 24.4s\tremaining: 5.71s\n",
      "810:\tlearn: 6.2675537\ttotal: 24.4s\tremaining: 5.68s\n",
      "811:\tlearn: 6.2674688\ttotal: 24.4s\tremaining: 5.65s\n",
      "812:\tlearn: 6.2673497\ttotal: 24.4s\tremaining: 5.62s\n",
      "813:\tlearn: 6.2673033\ttotal: 24.5s\tremaining: 5.59s\n",
      "814:\tlearn: 6.2672236\ttotal: 24.5s\tremaining: 5.56s\n",
      "815:\tlearn: 6.2671882\ttotal: 24.5s\tremaining: 5.53s\n",
      "816:\tlearn: 6.2671055\ttotal: 24.6s\tremaining: 5.51s\n",
      "817:\tlearn: 6.2670511\ttotal: 24.6s\tremaining: 5.48s\n",
      "818:\tlearn: 6.2669369\ttotal: 24.6s\tremaining: 5.45s\n",
      "819:\tlearn: 6.2668781\ttotal: 24.7s\tremaining: 5.42s\n",
      "820:\tlearn: 6.2667817\ttotal: 24.7s\tremaining: 5.38s\n",
      "821:\tlearn: 6.2666998\ttotal: 24.7s\tremaining: 5.36s\n",
      "822:\tlearn: 6.2665548\ttotal: 24.8s\tremaining: 5.33s\n",
      "823:\tlearn: 6.2664469\ttotal: 24.8s\tremaining: 5.29s\n",
      "824:\tlearn: 6.2663999\ttotal: 24.8s\tremaining: 5.26s\n",
      "825:\tlearn: 6.2663143\ttotal: 24.9s\tremaining: 5.24s\n",
      "826:\tlearn: 6.2662099\ttotal: 24.9s\tremaining: 5.21s\n",
      "827:\tlearn: 6.2661388\ttotal: 24.9s\tremaining: 5.18s\n",
      "828:\tlearn: 6.2660897\ttotal: 24.9s\tremaining: 5.15s\n",
      "829:\tlearn: 6.2659959\ttotal: 25s\tremaining: 5.12s\n",
      "830:\tlearn: 6.2659432\ttotal: 25s\tremaining: 5.09s\n",
      "831:\tlearn: 6.2658746\ttotal: 25s\tremaining: 5.06s\n",
      "832:\tlearn: 6.2657948\ttotal: 25.1s\tremaining: 5.03s\n",
      "833:\tlearn: 6.2657583\ttotal: 25.1s\tremaining: 5s\n",
      "834:\tlearn: 6.2656671\ttotal: 25.1s\tremaining: 4.97s\n",
      "835:\tlearn: 6.2655980\ttotal: 25.2s\tremaining: 4.93s\n",
      "836:\tlearn: 6.2655531\ttotal: 25.2s\tremaining: 4.91s\n",
      "837:\tlearn: 6.2654975\ttotal: 25.2s\tremaining: 4.88s\n",
      "838:\tlearn: 6.2654373\ttotal: 25.3s\tremaining: 4.84s\n",
      "839:\tlearn: 6.2653339\ttotal: 25.3s\tremaining: 4.82s\n",
      "840:\tlearn: 6.2652339\ttotal: 25.3s\tremaining: 4.79s\n",
      "841:\tlearn: 6.2651312\ttotal: 25.3s\tremaining: 4.76s\n",
      "842:\tlearn: 6.2650621\ttotal: 25.4s\tremaining: 4.73s\n",
      "843:\tlearn: 6.2649858\ttotal: 25.4s\tremaining: 4.7s\n",
      "844:\tlearn: 6.2649553\ttotal: 25.4s\tremaining: 4.67s\n",
      "845:\tlearn: 6.2648879\ttotal: 25.5s\tremaining: 4.64s\n",
      "846:\tlearn: 6.2648335\ttotal: 25.5s\tremaining: 4.61s\n",
      "847:\tlearn: 6.2647588\ttotal: 25.5s\tremaining: 4.58s\n",
      "848:\tlearn: 6.2646973\ttotal: 25.6s\tremaining: 4.55s\n",
      "849:\tlearn: 6.2646349\ttotal: 25.6s\tremaining: 4.52s\n",
      "850:\tlearn: 6.2645069\ttotal: 25.6s\tremaining: 4.49s\n",
      "851:\tlearn: 6.2644436\ttotal: 25.7s\tremaining: 4.46s\n",
      "852:\tlearn: 6.2643495\ttotal: 25.7s\tremaining: 4.42s\n",
      "853:\tlearn: 6.2643051\ttotal: 25.7s\tremaining: 4.4s\n",
      "854:\tlearn: 6.2642490\ttotal: 25.7s\tremaining: 4.37s\n",
      "855:\tlearn: 6.2641853\ttotal: 25.8s\tremaining: 4.34s\n",
      "856:\tlearn: 6.2641170\ttotal: 25.8s\tremaining: 4.31s\n",
      "857:\tlearn: 6.2640635\ttotal: 25.8s\tremaining: 4.28s\n",
      "858:\tlearn: 6.2639842\ttotal: 25.9s\tremaining: 4.25s\n",
      "859:\tlearn: 6.2638668\ttotal: 25.9s\tremaining: 4.22s\n",
      "860:\tlearn: 6.2638238\ttotal: 25.9s\tremaining: 4.19s\n",
      "861:\tlearn: 6.2637827\ttotal: 26s\tremaining: 4.16s\n",
      "862:\tlearn: 6.2636951\ttotal: 26s\tremaining: 4.13s\n",
      "863:\tlearn: 6.2636156\ttotal: 26s\tremaining: 4.1s\n",
      "864:\tlearn: 6.2635732\ttotal: 26.1s\tremaining: 4.07s\n",
      "865:\tlearn: 6.2635407\ttotal: 26.1s\tremaining: 4.04s\n",
      "866:\tlearn: 6.2634802\ttotal: 26.1s\tremaining: 4s\n",
      "867:\tlearn: 6.2633936\ttotal: 26.1s\tremaining: 3.98s\n",
      "868:\tlearn: 6.2632826\ttotal: 26.2s\tremaining: 3.95s\n",
      "869:\tlearn: 6.2631763\ttotal: 26.2s\tremaining: 3.92s\n",
      "870:\tlearn: 6.2631345\ttotal: 26.2s\tremaining: 3.88s\n",
      "871:\tlearn: 6.2630256\ttotal: 26.3s\tremaining: 3.85s\n",
      "872:\tlearn: 6.2629750\ttotal: 26.3s\tremaining: 3.82s\n",
      "873:\tlearn: 6.2628734\ttotal: 26.3s\tremaining: 3.79s\n",
      "874:\tlearn: 6.2628130\ttotal: 26.4s\tremaining: 3.76s\n",
      "875:\tlearn: 6.2626868\ttotal: 26.4s\tremaining: 3.73s\n",
      "876:\tlearn: 6.2625986\ttotal: 26.4s\tremaining: 3.71s\n",
      "877:\tlearn: 6.2625295\ttotal: 26.4s\tremaining: 3.67s\n",
      "878:\tlearn: 6.2624712\ttotal: 26.5s\tremaining: 3.64s\n",
      "879:\tlearn: 6.2623816\ttotal: 26.5s\tremaining: 3.61s\n",
      "880:\tlearn: 6.2622935\ttotal: 26.5s\tremaining: 3.58s\n",
      "881:\tlearn: 6.2622443\ttotal: 26.6s\tremaining: 3.55s\n",
      "882:\tlearn: 6.2621329\ttotal: 26.6s\tremaining: 3.52s\n",
      "883:\tlearn: 6.2620503\ttotal: 26.6s\tremaining: 3.49s\n",
      "884:\tlearn: 6.2619819\ttotal: 26.7s\tremaining: 3.46s\n",
      "885:\tlearn: 6.2618565\ttotal: 26.7s\tremaining: 3.43s\n",
      "886:\tlearn: 6.2618013\ttotal: 26.7s\tremaining: 3.4s\n",
      "887:\tlearn: 6.2617448\ttotal: 26.8s\tremaining: 3.37s\n",
      "888:\tlearn: 6.2616640\ttotal: 26.8s\tremaining: 3.34s\n",
      "889:\tlearn: 6.2616003\ttotal: 26.8s\tremaining: 3.31s\n",
      "890:\tlearn: 6.2615570\ttotal: 26.8s\tremaining: 3.28s\n",
      "891:\tlearn: 6.2614458\ttotal: 26.9s\tremaining: 3.25s\n",
      "892:\tlearn: 6.2613780\ttotal: 26.9s\tremaining: 3.22s\n",
      "893:\tlearn: 6.2613061\ttotal: 26.9s\tremaining: 3.19s\n",
      "894:\tlearn: 6.2612552\ttotal: 27s\tremaining: 3.16s\n",
      "895:\tlearn: 6.2611690\ttotal: 27s\tremaining: 3.13s\n",
      "896:\tlearn: 6.2611002\ttotal: 27s\tremaining: 3.1s\n",
      "897:\tlearn: 6.2610779\ttotal: 27.1s\tremaining: 3.07s\n",
      "898:\tlearn: 6.2610134\ttotal: 27.1s\tremaining: 3.04s\n",
      "899:\tlearn: 6.2609390\ttotal: 27.1s\tremaining: 3.01s\n",
      "900:\tlearn: 6.2608732\ttotal: 27.2s\tremaining: 2.98s\n",
      "901:\tlearn: 6.2607366\ttotal: 27.2s\tremaining: 2.95s\n",
      "902:\tlearn: 6.2607011\ttotal: 27.2s\tremaining: 2.92s\n",
      "903:\tlearn: 6.2605955\ttotal: 27.3s\tremaining: 2.89s\n",
      "904:\tlearn: 6.2605208\ttotal: 27.3s\tremaining: 2.86s\n",
      "905:\tlearn: 6.2604481\ttotal: 27.3s\tremaining: 2.83s\n",
      "906:\tlearn: 6.2603561\ttotal: 27.3s\tremaining: 2.8s\n",
      "907:\tlearn: 6.2602995\ttotal: 27.4s\tremaining: 2.77s\n",
      "908:\tlearn: 6.2602613\ttotal: 27.4s\tremaining: 2.74s\n",
      "909:\tlearn: 6.2601577\ttotal: 27.4s\tremaining: 2.71s\n",
      "910:\tlearn: 6.2600930\ttotal: 27.5s\tremaining: 2.68s\n",
      "911:\tlearn: 6.2600316\ttotal: 27.5s\tremaining: 2.65s\n",
      "912:\tlearn: 6.2599882\ttotal: 27.5s\tremaining: 2.62s\n",
      "913:\tlearn: 6.2599386\ttotal: 27.6s\tremaining: 2.59s\n",
      "914:\tlearn: 6.2598584\ttotal: 27.6s\tremaining: 2.56s\n",
      "915:\tlearn: 6.2598062\ttotal: 27.6s\tremaining: 2.53s\n",
      "916:\tlearn: 6.2597629\ttotal: 27.6s\tremaining: 2.5s\n",
      "917:\tlearn: 6.2596838\ttotal: 27.7s\tremaining: 2.47s\n",
      "918:\tlearn: 6.2595977\ttotal: 27.7s\tremaining: 2.44s\n",
      "919:\tlearn: 6.2595068\ttotal: 27.7s\tremaining: 2.41s\n",
      "920:\tlearn: 6.2594545\ttotal: 27.8s\tremaining: 2.38s\n",
      "921:\tlearn: 6.2593790\ttotal: 27.8s\tremaining: 2.35s\n",
      "922:\tlearn: 6.2593127\ttotal: 27.8s\tremaining: 2.32s\n",
      "923:\tlearn: 6.2592741\ttotal: 27.9s\tremaining: 2.29s\n",
      "924:\tlearn: 6.2591733\ttotal: 27.9s\tremaining: 2.26s\n",
      "925:\tlearn: 6.2591247\ttotal: 27.9s\tremaining: 2.23s\n",
      "926:\tlearn: 6.2590465\ttotal: 28s\tremaining: 2.2s\n",
      "927:\tlearn: 6.2589984\ttotal: 28s\tremaining: 2.17s\n",
      "928:\tlearn: 6.2589265\ttotal: 28s\tremaining: 2.14s\n",
      "929:\tlearn: 6.2588385\ttotal: 28.1s\tremaining: 2.11s\n",
      "930:\tlearn: 6.2587558\ttotal: 28.1s\tremaining: 2.08s\n",
      "931:\tlearn: 6.2586930\ttotal: 28.1s\tremaining: 2.05s\n",
      "932:\tlearn: 6.2586461\ttotal: 28.1s\tremaining: 2.02s\n",
      "933:\tlearn: 6.2585620\ttotal: 28.2s\tremaining: 1.99s\n",
      "934:\tlearn: 6.2585010\ttotal: 28.2s\tremaining: 1.96s\n",
      "935:\tlearn: 6.2584555\ttotal: 28.3s\tremaining: 1.93s\n",
      "936:\tlearn: 6.2583471\ttotal: 28.3s\tremaining: 1.9s\n",
      "937:\tlearn: 6.2582729\ttotal: 28.3s\tremaining: 1.87s\n",
      "938:\tlearn: 6.2582353\ttotal: 28.3s\tremaining: 1.84s\n",
      "939:\tlearn: 6.2582054\ttotal: 28.4s\tremaining: 1.81s\n",
      "940:\tlearn: 6.2581446\ttotal: 28.4s\tremaining: 1.78s\n",
      "941:\tlearn: 6.2581044\ttotal: 28.4s\tremaining: 1.75s\n",
      "942:\tlearn: 6.2580524\ttotal: 28.5s\tremaining: 1.72s\n",
      "943:\tlearn: 6.2580018\ttotal: 28.5s\tremaining: 1.69s\n",
      "944:\tlearn: 6.2579036\ttotal: 28.5s\tremaining: 1.66s\n",
      "945:\tlearn: 6.2578707\ttotal: 28.6s\tremaining: 1.63s\n",
      "946:\tlearn: 6.2578339\ttotal: 28.6s\tremaining: 1.6s\n",
      "947:\tlearn: 6.2577852\ttotal: 28.6s\tremaining: 1.57s\n",
      "948:\tlearn: 6.2577165\ttotal: 28.7s\tremaining: 1.54s\n",
      "949:\tlearn: 6.2576267\ttotal: 28.7s\tremaining: 1.51s\n",
      "950:\tlearn: 6.2575399\ttotal: 28.7s\tremaining: 1.48s\n",
      "951:\tlearn: 6.2574678\ttotal: 28.7s\tremaining: 1.45s\n",
      "952:\tlearn: 6.2573816\ttotal: 28.8s\tremaining: 1.42s\n",
      "953:\tlearn: 6.2572976\ttotal: 28.8s\tremaining: 1.39s\n",
      "954:\tlearn: 6.2572097\ttotal: 28.8s\tremaining: 1.36s\n",
      "955:\tlearn: 6.2571474\ttotal: 28.9s\tremaining: 1.33s\n",
      "956:\tlearn: 6.2570785\ttotal: 28.9s\tremaining: 1.3s\n",
      "957:\tlearn: 6.2569999\ttotal: 28.9s\tremaining: 1.27s\n",
      "958:\tlearn: 6.2569534\ttotal: 29s\tremaining: 1.24s\n",
      "959:\tlearn: 6.2568831\ttotal: 29s\tremaining: 1.21s\n",
      "960:\tlearn: 6.2567896\ttotal: 29s\tremaining: 1.18s\n",
      "961:\tlearn: 6.2567220\ttotal: 29.1s\tremaining: 1.15s\n",
      "962:\tlearn: 6.2566531\ttotal: 29.1s\tremaining: 1.12s\n",
      "963:\tlearn: 6.2566270\ttotal: 29.1s\tremaining: 1.09s\n",
      "964:\tlearn: 6.2565986\ttotal: 29.1s\tremaining: 1.06s\n",
      "965:\tlearn: 6.2565346\ttotal: 29.2s\tremaining: 1.03s\n",
      "966:\tlearn: 6.2564797\ttotal: 29.2s\tremaining: 997ms\n",
      "967:\tlearn: 6.2564296\ttotal: 29.2s\tremaining: 967ms\n",
      "968:\tlearn: 6.2563775\ttotal: 29.3s\tremaining: 936ms\n",
      "969:\tlearn: 6.2563493\ttotal: 29.3s\tremaining: 906ms\n",
      "970:\tlearn: 6.2562632\ttotal: 29.3s\tremaining: 876ms\n",
      "971:\tlearn: 6.2561836\ttotal: 29.4s\tremaining: 846ms\n",
      "972:\tlearn: 6.2561336\ttotal: 29.4s\tremaining: 815ms\n",
      "973:\tlearn: 6.2560660\ttotal: 29.4s\tremaining: 785ms\n",
      "974:\tlearn: 6.2560070\ttotal: 29.4s\tremaining: 755ms\n",
      "975:\tlearn: 6.2559118\ttotal: 29.5s\tremaining: 725ms\n",
      "976:\tlearn: 6.2558404\ttotal: 29.5s\tremaining: 695ms\n",
      "977:\tlearn: 6.2557852\ttotal: 29.5s\tremaining: 665ms\n",
      "978:\tlearn: 6.2557265\ttotal: 29.6s\tremaining: 634ms\n",
      "979:\tlearn: 6.2556506\ttotal: 29.6s\tremaining: 604ms\n",
      "980:\tlearn: 6.2555795\ttotal: 29.6s\tremaining: 574ms\n",
      "981:\tlearn: 6.2555293\ttotal: 29.7s\tremaining: 544ms\n",
      "982:\tlearn: 6.2554443\ttotal: 29.7s\tremaining: 514ms\n",
      "983:\tlearn: 6.2554070\ttotal: 29.7s\tremaining: 483ms\n",
      "984:\tlearn: 6.2553307\ttotal: 29.8s\tremaining: 453ms\n",
      "985:\tlearn: 6.2552943\ttotal: 29.8s\tremaining: 423ms\n",
      "986:\tlearn: 6.2552607\ttotal: 29.8s\tremaining: 393ms\n",
      "987:\tlearn: 6.2552233\ttotal: 29.8s\tremaining: 362ms\n",
      "988:\tlearn: 6.2551626\ttotal: 29.9s\tremaining: 332ms\n",
      "989:\tlearn: 6.2550531\ttotal: 29.9s\tremaining: 302ms\n",
      "990:\tlearn: 6.2550109\ttotal: 29.9s\tremaining: 272ms\n",
      "991:\tlearn: 6.2548917\ttotal: 30s\tremaining: 242ms\n",
      "992:\tlearn: 6.2548314\ttotal: 30s\tremaining: 211ms\n",
      "993:\tlearn: 6.2547819\ttotal: 30s\tremaining: 181ms\n",
      "994:\tlearn: 6.2546973\ttotal: 30.1s\tremaining: 151ms\n",
      "995:\tlearn: 6.2546487\ttotal: 30.1s\tremaining: 121ms\n",
      "996:\tlearn: 6.2545496\ttotal: 30.1s\tremaining: 90.7ms\n",
      "997:\tlearn: 6.2544657\ttotal: 30.2s\tremaining: 60.4ms\n",
      "998:\tlearn: 6.2543768\ttotal: 30.2s\tremaining: 30.2ms\n",
      "999:\tlearn: 6.2543110\ttotal: 30.2s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cb_params = {\"loss_function\": \"MAE\", \"random_state\": 42, \"verbose\": 1, \"task_type\": \"GPU\", \"thread_count\": -1, \"cat_features\": ['stock_id', 'imbalance_buy_sell_flag'] }\n",
    "mae_cb, _ = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc4012",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Naive Feature selection model performance\n",
    "Now that we have trained and predicted each algorithm, we can finally evaluate their performance to give us a baseline to test on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b60adb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: 5.7078\n",
      "Random Forest: 6.4020\n",
      "LightGBM: 6.3044\n",
      "XGBoost: 6.3004\n",
      "CatBoost: 6.3020\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Regression: {mae_lr:.4f}\")\n",
    "print(f\"Random Forest: {mae_rf:.4f}\")\n",
    "print(f\"LightGBM: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost: {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877ae30",
   "metadata": {},
   "source": [
    "Interestingly, we found dropping near price and far price (features that are half null), gave Linear Regression the best score. Among the models that trained on the whole training set, LightGBM had the best MAE. This is a good starting point to see how well we can improve the performance of the last three models. To save time and avoid redundancy, we will not pursue Linear Regression or Random Forest further.\n",
    "\n",
    "## Original Feature Engineering\n",
    "Contributed by Cooper Richmond\n",
    "\n",
    "In this section, we will think of and create our own features to use for training the finalist models (LightGBM, XGBoost, and CatBoost). \n",
    "\n",
    "These our are originally made features: \n",
    "\n",
    "- `wap_volatility`: Measures how much the weighted average price (WAP) swings over 5 time periods for each stock. \n",
    "\n",
    "- `bid_ask_spread`: Difference between the selling price (ask) and buying price (bid).\n",
    "\n",
    "- `bid_ask_volatility`: Tracks how much the bid-ask spread varies over 5 time periods for each stock.\n",
    "\n",
    "- `wap_momentum`: Shows the percentage change in WAP over 3 time periods for each stock.\n",
    "\n",
    "- `price_momentum`: Shows the percentage change in reference price over 3 time periods for each stock.\n",
    "\n",
    "- `log_imbalance_size`: Shrinks large imbalance size values (buy/sell order differences) using a log function.\n",
    "\n",
    "- `log_matched_size`: Shrinks large matched order size values using a log function.\n",
    "\n",
    "- `log_bid_size`: Shrinks large bid order size values using a log function.\n",
    "\n",
    "- `log_ask_size`: Shrinks large ask order size values using a log function.\n",
    "\n",
    "- `bucket_price_interaction`: Multiplies time (seconds in bucket) by reference price to capture their combined effect.\n",
    "\n",
    "- `bucket_imbalance_interaction`: Multiplies time (seconds in bucket) by imbalance size to capture their combined effect.\n",
    "\n",
    "- `wap_to_ref_price`: Divides WAP by reference price to show their relative difference.\n",
    "\n",
    "- `bid_to_ask_price`: Divides bid price by ask price to show their relative difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7602801",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Written by Cooper and Attila \n",
    "def process_data_with_feature_engineering(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    # Drop rows where 'target' is null, as these cannot be used for training\n",
    "    data.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # Volatility features\n",
    "    data['wap_volatility'] = data.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "    data['bid_ask_spread'] = data['ask_price'] - data['bid_price']\n",
    "    data['bid_ask_volatility'] = data.groupby('stock_id')['bid_ask_spread'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # Momentum features\n",
    "    data['wap_momentum'] = data.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "    data['price_momentum'] = data.groupby('stock_id')['reference_price'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "\n",
    "    # Log transformations\n",
    "    size_cols = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size']\n",
    "    for col in size_cols:\n",
    "        data[f'log_{col}'] = np.log1p(data[col].clip(lower=0))\n",
    "\n",
    "    # Time-based interactions\n",
    "    data['bucket_price_interaction'] = data['seconds_in_bucket'] * data['reference_price']\n",
    "    data['bucket_imbalance_interaction'] = data['seconds_in_bucket'] * data['imbalance_size']\n",
    "\n",
    "    # Relative price features\n",
    "    data['wap_to_ref_price'] = data['wap'] / (data['reference_price'] + 1e-6)\n",
    "    data['bid_to_ask_price'] = data['bid_price'] / (data['ask_price'] + 1e-6)\n",
    "\n",
    "    # Handle NaN and inf after feature creation\n",
    "    # Identify columns that are not identifiers or the target\n",
    "    cols_to_process = [col for col in data.columns if col not in ['stock_id', 'date_id', 'target', 'time_id', 'row_id']]\n",
    "    data[cols_to_process] = data[cols_to_process].replace([np.inf, -np.inf], np.nan) # Replace inf with NaN first\n",
    "    data[cols_to_process] = data[cols_to_process].fillna(data[cols_to_process].median()) # Fill remaining NaNs with median\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = data.drop(\"target\", axis=1)\n",
    "    y = data[\"target\"]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109832e3",
   "metadata": {},
   "source": [
    "Now we can redo the naive training but with our original features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee718f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/z2rw3p755gzgwr5bsbt6r5x00000gn/T/ipykernel_47502/2978197467.py:8: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
      "/var/folders/qr/z2rw3p755gzgwr5bsbt6r5x00000gn/T/ipykernel_47502/2978197467.py:17: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n",
      "/var/folders/qr/z2rw3p755gzgwr5bsbt6r5x00000gn/T/ipykernel_47502/2978197467.py:20: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6631\n",
      "[LightGBM] [Info] Number of data points in the train set: 350939, number of used features: 28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Start training from score -0.026625\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.29863\n",
      "[300]\tvalid_0's l1: 6.29406\n",
      "[450]\tvalid_0's l1: 6.29231\n",
      "[600]\tvalid_0's l1: 6.29213\n",
      "Early stopping, best iteration is:\n",
      "[496]\tvalid_0's l1: 6.29144\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X, y = process_data_with_feature_engineering(df.copy())\n",
    "\n",
    "# Split the data into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# LightGBM\n",
    "lgbm_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 31,\n",
    "    'n_estimators': 2000,\n",
    "    'min_child_samples': 50,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 0.5,\n",
    "    'device': 'gpu',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "mae_lgbm, lgbm_model = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)\n",
    "\n",
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'objective': 'reg:absoluteerror',\n",
    "    'eval_metric': 'mae',\n",
    "    'device': \"cuda\",\n",
    "    'random_state': 42\n",
    "}\n",
    "mae_xgb, xgb_model = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)\n",
    "\n",
    "# CatBoost\n",
    "cb_params = {\"loss_function\": \"MAE\", \"random_state\": 42, \"verbose\": 0, \"task_type\": \"GPU\", \"thread_count\": -1, \"cat_features\": ['stock_id', 'imbalance_buy_sell_flag']} \n",
    "mae_cb, cb_model = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "814e8ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAE: 6.2914\n",
      "XGBoost MAE: 6.2923\n",
      "CatBoost MAE: 6.2825\n"
     ]
    }
   ],
   "source": [
    "print(f\"LightGBM MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost MAE: {mae_cb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97063fc",
   "metadata": {},
   "source": [
    "We are seeing some minor improvements, but not enough to justify that our features are that helpful. Let's use the post processing trick that was used by the 1st place winner, and see how those results compare:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf4c0a",
   "metadata": {},
   "source": [
    "We applied the post-processing trick used by the Kaggle first-place solution, which corrects each model’s predictions by subtracting the weighted average prediction for each time_id. The weights are based on a predefined stock_weights dictionary reflecting the relative importance or liquidity of each stock.\n",
    "\n",
    "*Explanation contributed by [Brandon]*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4963125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stock weights (Given by Kaggle) (Attila, Cooper)\n",
    "weight_df = pd.DataFrame()\n",
    "weight_df['stock_id'] = list(range(200))\n",
    "weight_df['weight'] = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,]\n",
    "stock_weights = dict(zip(weight_df['stock_id'], weight_df['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LightGBM post-processing ---\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "LightGBM Adjusted MAE: 6.3130\n",
      "\n",
      "--- XGBoost post-processing ---\n",
      "XGBoost Adjusted MAE: 6.3160\n",
      "\n",
      "--- CatBoost post-processing ---\n",
      "CatBoost Adjusted MAE: 6.3033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brandon Leong - imrpoved and cleaned code to universal helper function for post processing\n",
    "\n",
    "def apply_postprocessing(model, X_val, y_val, stock_weights, model_name=\"Model\"):\n",
    "    preds_df = pd.DataFrame({\n",
    "        'prediction': model.predict(X_val),\n",
    "        'stock_id': X_val['stock_id'],\n",
    "        'time_id': X_val['time_id']\n",
    "    })\n",
    "    preds_df['stock_weights'] = preds_df['stock_id'].map(stock_weights)\n",
    "    preds_df['weighted_sum'] = preds_df['prediction'] * preds_df['stock_weights']\n",
    "    \n",
    "    weighted_avg = preds_df.groupby('time_id')['weighted_sum'].transform('sum') / preds_df.groupby('time_id')['stock_weights'].transform('sum')\n",
    "    preds_adjusted = preds_df['prediction'] - weighted_avg\n",
    "    \n",
    "    mae_adjusted = mean_absolute_error(y_val, preds_adjusted)\n",
    "    print(f\"{model_name} Adjusted MAE: {mae_adjusted:.4f}\\n\")\n",
    "    return preds_adjusted, mae_adjusted\n",
    "\n",
    "print(\"--- LightGBM post-processing ---\")\n",
    "y_pred_lgbm_adjusted, mae_lgbm_adjusted = apply_postprocessing(lgbm_model, X_val, y_val, stock_weights, \"LightGBM\")\n",
    "\n",
    "print(\"--- XGBoost post-processing ---\")\n",
    "y_pred_xgb_adjusted, mae_xgb_adjusted = apply_postprocessing(xgb_model, X_val, y_val, stock_weights, \"XGBoost\")\n",
    "\n",
    "print(\"--- CatBoost post-processing ---\")\n",
    "y_pred_cb_adjusted, mae_cb_adjusted = apply_postprocessing(cb_model, X_val, y_val, stock_weights, \"CatBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8681fd",
   "metadata": {},
   "source": [
    "Looks like it was helpful to post process only for LightGBM and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d34885",
   "metadata": {},
   "source": [
    "## Additional Feature Ideas Inspired by Kaggle Leaderboard\n",
    "Contributed by Cooper Richmond\n",
    "\n",
    "This is the code for some features we created that we got the ideas from on the kaggle leaderboard. \n",
    "\n",
    "- `seconds_in_bucket_group`: Splits time into three groups: 0 (0-299 seconds), 1 (300-479 seconds), 2 (480+ seconds).  \n",
    "\n",
    "- `bid_ask_spread`: Difference between ask (sell) and bid (buy) prices.\n",
    "\n",
    "- `imbalance_ratio`: Divides imbalance size (buy/sell order difference) by matched size (executed orders).\n",
    "\n",
    "- `mid_price`: Average of ask and bid prices.\n",
    "\n",
    "- `time_in_auction`: Normalizes time (seconds in bucket) by dividing by 540 to scale between 0 and 1.\n",
    "\n",
    "We also combined these with our original features into a combined function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dddf4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Cooper and Attila \n",
    "\n",
    "# Helper functions\n",
    "def _apply_non_leaderboard_features(data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Volatility features\n",
    "    data_df['wap_volatility'] = data_df.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "    data_df['bid_ask_spread'] = data_df['ask_price'] - data_df['bid_price']\n",
    "    data_df['bid_ask_volatility'] = data_df.groupby('stock_id')['bid_ask_spread'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # Momentum features\n",
    "    data_df['wap_momentum'] = data_df.groupby('stock_id')['wap'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "    data_df['price_momentum'] = data_df.groupby('stock_id')['reference_price'].transform(\n",
    "        lambda x: x.pct_change(periods=3)\n",
    "    )\n",
    "\n",
    "    # Log transformations\n",
    "    size_cols = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size']\n",
    "    for col in size_cols:\n",
    "        data_df[f'log_{col}'] = np.log1p(data_df[col].clip(lower=0))\n",
    "\n",
    "    # Time-based interactions\n",
    "    data_df['bucket_price_interaction'] = data_df['seconds_in_bucket'] * data_df['reference_price']\n",
    "    data_df['bucket_imbalance_interaction'] = data_df['seconds_in_bucket'] * data_df['imbalance_size']\n",
    "\n",
    "    # Relative price features\n",
    "    data_df['wap_to_ref_price'] = data_df['wap'] / (data_df['reference_price'] + 1e-6)\n",
    "    data_df['bid_to_ask_price'] = data_df['bid_price'] / (data_df['ask_price'] + 1e-6)\n",
    "\n",
    "    # Handle NaN and inf for all newly created numerical columns\n",
    "    # Exclude identifier/target columns as they are handled in the main processing functions\n",
    "    new_cols_added = [col for col in data_df.columns if col not in ['stock_id', 'date_id', 'target', 'time_id', 'row_id']]\n",
    "    # Filter to only numerical columns that might contain NaN/inf\n",
    "    numerical_cols_to_fill = data_df[new_cols_added].select_dtypes(include=np.number).columns\n",
    "    data_df[numerical_cols_to_fill] = data_df[numerical_cols_to_fill].replace([np.inf, -np.inf], np.nan)\n",
    "    data_df[numerical_cols_to_fill] = data_df[numerical_cols_to_fill].fillna(data_df[numerical_cols_to_fill].median())\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def _apply_leaderboard_features(data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Handle NaN and infinities in specific input columns first as per original\n",
    "    input_cols = ['imbalance_size', 'matched_size', 'ask_price', 'bid_price', 'wap', 'reference_price']\n",
    "    for col in input_cols:\n",
    "        data_df[col] = data_df[col].replace([np.inf, -np.inf], np.nan).fillna(data_df[col].median())\n",
    "\n",
    "    # 1st Place: Seconds in bucket group\n",
    "    data_df['seconds_in_bucket_group'] = np.where(data_df['seconds_in_bucket'] < 300, 0,\n",
    "                                              np.where(data_df['seconds_in_bucket'] < 480, 1, 2))\n",
    "\n",
    "    # 9th Place: Basic features\n",
    "    data_df['bid_ask_spread'] = data_df['ask_price'] - data_df['bid_price']\n",
    "    data_df['imbalance_ratio'] = data_df['imbalance_size'] / (data_df['matched_size'] + 1e-6)\n",
    "\n",
    "    # 14th Place: Mid price\n",
    "    data_df['mid_price'] = (data_df['ask_price'] + data_df['bid_price']) / 2\n",
    "\n",
    "    # Time in auction\n",
    "    data_df['time_in_auction'] = data_df['seconds_in_bucket'] / 540\n",
    "\n",
    "    # Handle NaN and inf for newly created columns\n",
    "    new_cols = ['seconds_in_bucket_group', 'bid_ask_spread', 'imbalance_ratio', 'mid_price', 'time_in_auction']\n",
    "    data_df[new_cols] = data_df[new_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    data_df[new_cols] = data_df[new_cols].fillna(data_df[new_cols].median())\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# Main data processing functions\n",
    "def process_data_non_leaderboard(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df_copy = data.copy()\n",
    "    # Drop rows where 'target' is null as these cannot be used for training\n",
    "    df_copy.dropna(subset=[\"target\"], inplace=True)\n",
    "    df_processed = _apply_non_leaderboard_features(df_copy)\n",
    "\n",
    "    X = df_processed.drop([\"target\", \"time_id\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n",
    "\n",
    "def process_data_leaderboard(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df_copy = data.copy()\n",
    "    # Drop rows where 'target' is null as these cannot be used for training\n",
    "    df_copy.dropna(subset=[\"target\"], inplace=True)\n",
    "    df_processed = _apply_leaderboard_features(df_copy)\n",
    "\n",
    "    X = df_processed.drop([\"target\", \"time_id\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n",
    "\n",
    "def process_data_combined(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df_copy = data.copy()\n",
    "    # Drop rows where 'target' is null as these cannot be used for training\n",
    "    df_copy.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # Apply non-leaderboard features first\n",
    "    df_processed = _apply_non_leaderboard_features(df_copy)\n",
    "\n",
    "    # Apply additional combined features from the leaderboard approach\n",
    "    # Handle NaN and infinities in specific input columns first as per original combined\n",
    "    input_cols_combined = ['imbalance_size', 'matched_size', 'ask_price', 'bid_price', 'wap', 'reference_price']\n",
    "    for col in input_cols_combined:\n",
    "        df_processed[col] = df_processed[col].replace([np.inf, -np.inf], np.nan).fillna(df_processed[col].median())\n",
    "\n",
    "    df_processed['seconds_in_bucket_group'] = np.where(df_processed['seconds_in_bucket'] < 300, 0,\n",
    "                                              np.where(df_processed['seconds_in_bucket'] < 480, 1, 2))\n",
    "    df_processed['imbalance_ratio'] = df_processed['imbalance_size'] / (df_processed['matched_size'] + 1e-6)\n",
    "    df_processed['mid_price'] = (df_processed['ask_price'] + df_processed['bid_price']) / 2\n",
    "    df_processed['time_in_auction'] = df_processed['seconds_in_bucket'] / 540\n",
    "\n",
    "    # Handle NaN and inf for newly created combined features\n",
    "    new_cols_combined = ['seconds_in_bucket_group', 'imbalance_ratio', 'mid_price', 'time_in_auction']\n",
    "    df_processed[new_cols_combined] = df_processed[new_cols_combined].replace([np.inf, -np.inf], np.nan)\n",
    "    df_processed[new_cols_combined] = df_processed[new_cols_combined].fillna(df_processed[new_cols_combined].median())\n",
    "\n",
    "    X = df_processed.drop([\"target\", \"time_id\"], axis=1)\n",
    "    y = df_processed[\"target\"]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362fc7a0",
   "metadata": {},
   "source": [
    "Now that these functions are defined we can evaluate our models on these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "541c3b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3884\n",
      "[LightGBM] [Info] Number of data points in the train set: 350939, number of used features: 19\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Start training from score -0.026625\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.30237\n",
      "[300]\tvalid_0's l1: 6.29669\n",
      "[450]\tvalid_0's l1: 6.29445\n",
      "[600]\tvalid_0's l1: 6.29393\n",
      "[750]\tvalid_0's l1: 6.29247\n",
      "[900]\tvalid_0's l1: 6.29081\n",
      "[1050]\tvalid_0's l1: 6.28935\n",
      "[1200]\tvalid_0's l1: 6.28805\n",
      "[1350]\tvalid_0's l1: 6.28789\n",
      "Early stopping, best iteration is:\n",
      "[1265]\tvalid_0's l1: 6.28771\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n"
     ]
    }
   ],
   "source": [
    "X, y = process_data_leaderboard(df.copy())\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "mae_lgbm, lgbm_model = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)\n",
    "mae_xgb, xgb_model = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)\n",
    "mae_cb, cb_model = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95792251",
   "metadata": {},
   "source": [
    "This gets us the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98c0e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAE: 6.2877\n",
      "XGBoost MAE: 6.2883\n",
      "CatBoost MAE: 6.2881\n"
     ]
    }
   ],
   "source": [
    "print(f\"LightGBM MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost MAE: {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8cc2d",
   "metadata": {},
   "source": [
    "Now we can evaluate on combined features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4810e211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/z2rw3p755gzgwr5bsbt6r5x00000gn/T/ipykernel_47502/2755009002.py:7: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change().rolling(window=5, min_periods=1).std()\n",
      "/var/folders/qr/z2rw3p755gzgwr5bsbt6r5x00000gn/T/ipykernel_47502/2755009002.py:16: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n",
      "/var/folders/qr/z2rw3p755gzgwr5bsbt6r5x00000gn/T/ipykernel_47502/2755009002.py:19: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lambda x: x.pct_change(periods=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6944\n",
      "[LightGBM] [Info] Number of data points in the train set: 350939, number of used features: 31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n",
      "[LightGBM] [Info] Start training from score -0.026625\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[150]\tvalid_0's l1: 6.29863\n",
      "[300]\tvalid_0's l1: 6.29358\n",
      "[450]\tvalid_0's l1: 6.29138\n",
      "[600]\tvalid_0's l1: 6.2911\n",
      "[750]\tvalid_0's l1: 6.28888\n",
      "[900]\tvalid_0's l1: 6.28726\n",
      "[1050]\tvalid_0's l1: 6.28689\n",
      "Early stopping, best iteration is:\n",
      "[1030]\tvalid_0's l1: 6.28671\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5\n"
     ]
    }
   ],
   "source": [
    "X, y = process_data_combined(df.copy())\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "mae_lgbm, lgbm_model = train_model(LGBMRegressor, X_train, y_train, X_val, y_val, params=lgbm_params)\n",
    "mae_xgb, xgb_model = train_model(xgboost.XGBRegressor, X_train, y_train, X_val, y_val, params=xgb_params)\n",
    "mae_cb, cb_model = train_model(CatBoostRegressor, X_train, y_train, X_val, y_val, params=cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c32a718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAE: 6.2867\n",
      "XGBoost MAE: 6.2874\n",
      "CatBoost MAE: 6.2806\n"
     ]
    }
   ],
   "source": [
    "print(f\"LightGBM MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"CatBoost MAE: {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e19f91",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "*Contributed by Brandon Leong*\n",
    "\n",
    "With this, we can see that CatBoost performs the best out of our finalist models when paired with all of the features. Now to further improve our model's performance, we can begin hypertuning the parameters of the model using RandomizedSearchCV. This allows us to explore a wide range of parameter combinations compared to GridSearch, which is exhaustive but computationally heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code by brandon\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# our initial catboost parameters\n",
    "cb_params = {\n",
    "    \"loss_function\": \"MAE\", \n",
    "    \"random_state\": 42, \n",
    "    \"verbose\": 0, \n",
    "    \"task_type\": \"CUDA\", \n",
    "    \"thread_count\": -1, \n",
    "    \"cat_features\": ['stock_id', 'imbalance_buy_sell_flag']\n",
    "} \n",
    "\n",
    "# distribution of parameters to try different values for, our what we are tuning\n",
    "parameter_distributions = {\n",
    "    'iterations': randint(1000, 3000), # number of trees\n",
    "    'learning_rate': uniform(0.01, 0.09), # reduces the gradient step, smaller value = more iterations needed\n",
    "    'depth': randint(4, 10), # tree depth\n",
    "    'l2_leaf_reg': uniform(1, 10), # regularizer values\n",
    "    'bagging_temperature': uniform(0, 1), # settings of the Bayesian bootstrap\n",
    "    'random_strength': uniform(0.1, 1), # amount of randomness to use for scoring splits\n",
    "    'border_count': randint(32, 255), # number of splits for numerical features\n",
    "    'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide'] # how trees are grown\n",
    "}\n",
    "\n",
    "# catboost model\n",
    "cat = CatBoostRegressor(\n",
    "    loss_function=\"MAE\",\n",
    "    task_type=\"CUDA\",\n",
    "    thread_count=-1,\n",
    "    random_state=42,\n",
    "    cat_features=['stock_id', 'imbalance_buy_sell_flag'],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# use randomized search to find best parameters\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=cat,\n",
    "    param_distributions=parameter_distributions,\n",
    "    n_iter=40,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters:{best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2baa397",
   "metadata": {},
   "source": [
    "Compared to RandomizedSearchCV, we also have Optuna, a hyperpameter optimization framework suggested to use on the Catboost documentation page. Optuna is generally more efficient and finds better parameters faster. Optuna tries hyperparameters based on past results to focus on promising areas, and supports early stopping allowing us to stop bad trials early. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7106ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code by brandon leong\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'iterations': 1000,\n",
    "        'loss_function': 'MAE',\n",
    "        'task_type': 'CUDA',\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0,\n",
    "        'cat_features': ['stock_id', 'imbalance_buy_sell_flag']\n",
    "    }\n",
    "\n",
    "    # train / test splits\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    # model\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "    # predict\n",
    "    preds = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    return mae\n",
    "\n",
    "# we want to minimize the mae\n",
    "study = optuna.create_study(direction='minimize')\n",
    "# 50 trials, can be more for better results\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print('Best trial:')\n",
    "best_params = study.best_trial.params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc47666",
   "metadata": {},
   "source": [
    "Now that we have our optimal parameters, we can retrain the model with the best parameters to get out final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params.update({\n",
    "    'loss_function': 'MAE',\n",
    "    'task_type': 'CUDA',\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'cat_features': ['stock_id', 'imbalance_buy_sell_flag']\n",
    "})\n",
    "\n",
    "final_model = CatBoostRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# predict on validation set\n",
    "final_preds = final_model.predict(X_val)\n",
    "\n",
    "# get MAE\n",
    "final_mae = mean_absolute_error(y_val, final_preds)\n",
    "\n",
    "# results\n",
    "print(f\"Final MAE with best trial params: {final_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f08b3",
   "metadata": {},
   "source": [
    "## Overfitting Exploration\n",
    "Contributed by Cooper Richmond\n",
    "\n",
    "This code calculates the training error for your final LightGBM, XGBoost, and CatBoost models by using the already-trained model objects to make predictions on the training data.It makes a line plot that visually compares each model's training error against its validation error, allowing you to easily check for signs of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddcf564f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "overfitting_data = []\n",
    "final_models_for_plot = {\n",
    "    \"LightGBM\": (lgbm_model, mae_lgbm),\n",
    "    \"XGBoost\": (xgb_model, mae_xgb),\n",
    "    \"CatBoost\": (cb_model, mae_cb)\n",
    "}\n",
    "\n",
    "for model_name, (model_obj, validation_mae) in final_models_for_plot.items():\n",
    "    y_pred_train = model_obj.predict(X_train)\n",
    "    training_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    overfitting_data.append({'Model': model_name, 'Data Set': 'Training', 'MAE': training_mae})\n",
    "    overfitting_data.append({'Model': model_name, 'Data Set': 'Validation', 'MAE': validation_mae})\n",
    "\n",
    "overfitting_df = pd.DataFrame(overfitting_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.lineplot(data=overfitting_df, x='Model', y='MAE', hue='Data Set', style='Data Set', markers=True, dashes=False, markersize=8)\n",
    "\n",
    "ax.set_title('Training vs. Validation MAE (Overfitting Check)', fontsize=16)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Mean Absolute Error (MAE)', fontsize=12)\n",
    "ax.legend(title='Data Set')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc891c",
   "metadata": {},
   "source": [
    "Overfitting is what happens when a machine learning model learns the training data too well, including its noise and outliers, which causes the model to perform poorly on new, unseen data.\n",
    "\n",
    "This chart plots the Mean Absolute Error (MAE) for both the training and validation datasets across three different models: LightGBM, XGBoost, and CatBoost.\n",
    "\n",
    "Here’s a breakdown of what the chart shows:\n",
    "\n",
    "X-axis: The machine learning models being compared.\n",
    "\n",
    "Y-axis: The Mean Absolute Error (MAE). A lower MAE indicates a better performance.\n",
    "\n",
    "Blue Line (Training MAE): This shows the error of each model on the data it was trained on.\n",
    "\n",
    "Orange Line (Validation MAE): This shows the error of each model on a separate set of data it has not seen before. This is a better representation of how the model would perform in the real world.\n",
    "\n",
    "Analysis of the Chart:\n",
    "Gap between training and validation error: There is a noticeable gap between the training and validation MAE for all three models. The validation error is consistently higher than the training error, which is expected. However, the size of this gap can indicate overfitting.\n",
    "Performance of the models:\n",
    "LightGBM and XGBoost: These two models show a larger gap between training and validation error compared to CatBoost. This suggests they generalize worse to new data and are potentially more overfit.\n",
    "\n",
    "CatBoost: This model has the smallest gap between its training and validation performance. This indicates it is less prone to overfitting on this dataset compared to the other two models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f043b",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "*Improved by Brandon Leong*\n",
    "\n",
    "Our experiments have shown that the additional features definetely help improve the Mean Absolute Error of the models as features like seconds_in_bucket_group, imbalance_ratio, and mid_price help the models understand different auction phases, supply-demand imbalances, and price centrality which heavily impact how we model price movements. \n",
    "\n",
    "Despite having far fewer features and less development time than top leaderboard competitors as well as the top placers on the leaderboard being able to construct over 300 features for this dataset combined with the computing power to quickly test through those data sets, we were able to get relatively good enough results (the difference of MAE from number 1 to us is <1). This showcases the importance of selecting the right model combined with proper feature engineering. If we continued to develop more features and pick which subsets of features worked best we would be able to get good results. Unfortunately we were hit with a time limitation, as the Kaggle ran for over 6 months, while we only had realistically one week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade6d3b",
   "metadata": {},
   "source": [
    "## Areas of Improvement\n",
    "*Improved by Brandon Leong*\n",
    "\n",
    "While our results with our current feature engineering and models have promising results, there a clear areas that we can optimize and refine upon to enhance performance. \n",
    "\n",
    "### 1. Handling Missing Data\n",
    "We could improve upon how we handle the NaN rows of far_price and near_price. As a human, we know that this means that the orders did not fill for the auction, but the models have no way to tell. If we were to find some way to put this in a feature, it might improve the results of the model. Rather than simply dropping the values, creating an indicator to flag for missing migth help with providing better signals over masking patterns in the data.\n",
    "\n",
    "### 2. Expanded feature Engineering\n",
    "Another area of improvement would be like previously mentioned, picking the best features and forgetting about other ones that worsen or elongate training time. The winning solution had hundreds of features, and going through each feature to determine the most important and relevant ones could increase accuracy.  \n",
    "\n",
    "### 3. Model Hyperparameter Tuning\n",
    "We could also try hyper parameter tuning the other models, as fine tuning hyperparameters such as learning rate, tree depth, number of estimators, regularization terms, and subsampling ratios can lead to significant gains and maybe a different model would rise to have the best results.\n",
    "\n",
    "### 4. Outside Kaggle Competition\n",
    "While in regards to the competition, we were supposed to only use the single dataset we were given if we were to continue to refine the model outside of it for real world applicatino, linking external data might help add additional context such as interest rates, CPI, or even news sentiment. Additionally, using other scoring methods such as time series cross validaiton could better reflect everchanging and sequential stock data. \n",
    "\n",
    "All of these would be achievable given more time, and we would definitely see some improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
